"use strict";(self.webpackChunkproject_website=self.webpackChunkproject_website||[]).push([[8962],{5345:(e,r,a)=>{a.r(r),a.d(r,{assets:()=>i,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>c,toc:()=>d});var t=a(62540),n=a(43023);const o={},s="Reading and Writing Arrow Data",c={id:"arrowjs/developer-guide/reading-and-writing",title:"Reading and Writing Arrow Data",description:"About RecordBatches",source:"@site/../docs/arrowjs/developer-guide/reading-and-writing.md",sourceDirName:"arrowjs/developer-guide",slug:"/arrowjs/developer-guide/reading-and-writing",permalink:"/docs/arrowjs/developer-guide/reading-and-writing",draft:!1,unlisted:!1,editUrl:"https://github.com/visgl/loaders.gl/tree/master/website/../docs/arrowjs/developer-guide/reading-and-writing.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Data Sources and Sinks",permalink:"/docs/arrowjs/developer-guide/data-sources"},next:{title:"Apache Arrow JavaScript API Reference",permalink:"/docs/arrowjs/api-reference/"}},i={},d=[{value:"About RecordBatches",id:"about-recordbatches",level:2},{value:"Reading Arrow Data",id:"reading-arrow-data",level:2},{value:"Using RecordBatchReader to read from a Data Source",id:"using-recordbatchreader-to-read-from-a-data-source",level:3},{value:"Reading Multiple Tables from a Data Source",id:"reading-multiple-tables-from-a-data-source",level:3},{value:"Using Transform Streams",id:"using-transform-streams",level:2},{value:"Connecting to Python Processes",id:"connecting-to-python-processes",level:3}];function l(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,n.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"reading-and-writing-arrow-data",children:"Reading and Writing Arrow Data"})}),"\n",(0,t.jsx)(r.h2,{id:"about-recordbatches",children:"About RecordBatches"}),"\n",(0,t.jsx)(r.p,{children:"Arrow tables are typically split into record batches, allowing them to be incrementally loaded or written, and naturally the Arrow API provides classes to facilite this reading."}),"\n",(0,t.jsx)(r.h2,{id:"reading-arrow-data",children:"Reading Arrow Data"}),"\n",(0,t.jsxs)(r.p,{children:["The ",(0,t.jsx)(r.code,{children:"Table"})," class provides a simple ",(0,t.jsx)(r.code,{children:"Table.from"})," convenience method for reading an Arrow formatted data file into Arrow data structures:"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{children:"import { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\nconst arrow = readFileSync('simple.arrow');\nconst table = Table.from([arrow]);\nconsole.log(table.toString());\n"})}),"\n",(0,t.jsx)(r.h3,{id:"using-recordbatchreader-to-read-from-a-data-source",children:"Using RecordBatchReader to read from a Data Source"}),"\n",(0,t.jsxs)(r.p,{children:["To read Arrow tables incrementally, you use the ",(0,t.jsx)(r.code,{children:"RecordBatchReader"})," class."]}),"\n",(0,t.jsxs)(r.p,{children:["If you only have one table in your file (the normal case), then you'll only need one ",(0,t.jsx)(r.code,{children:"RecordBatchReader"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-typescript",children:"const reader = await RecordBatchReader.from(fetch(path, {credentials: 'omit'}));\nfor await (const batch of reader) {\n  console.log(batch.length);\n}\n"})}),"\n",(0,t.jsx)(r.h3,{id:"reading-multiple-tables-from-a-data-source",children:"Reading Multiple Tables from a Data Source"}),"\n",(0,t.jsx)(r.p,{children:'The JavaScript Arrow API supports arrow data streams that contain multiple tables (this is an "extension" to the arrow spec). Naturally, each Table comes with its own set of record batches, so to read all batches from all tables in the data source you will need a double loop:'}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-typescript",children:"const readers = RecordBatchReader.readAll(fetch(path, {credentials: 'omit'}));\nfor await (const reader of readers) {\n  for await (const batch of reader) {\n    console.log(batch.length);\n  }\n}\n"})}),"\n",(0,t.jsx)(r.p,{children:"Note: this code also works if there is only one table in the data source, in which case the outer loop will only execute once."}),"\n",(0,t.jsx)(r.h1,{id:"writing-arrow-data",children:"Writing Arrow Data"}),"\n",(0,t.jsxs)(r.p,{children:["The ",(0,t.jsx)(r.code,{children:"RecordStreamWriter"})," class allows you to write Arrow ",(0,t.jsx)(r.code,{children:"Table"})," and ",(0,t.jsx)(r.code,{children:"RecordBatch"})," instances to a data source."]}),"\n",(0,t.jsx)(r.h2,{id:"using-transform-streams",children:"Using Transform Streams"}),"\n",(0,t.jsx)(r.h3,{id:"connecting-to-python-processes",children:"Connecting to Python Processes"}),"\n",(0,t.jsx)(r.p,{children:"A more complicated example of using Arrow to go from node -> python -> node:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-typescript",children:"const {AsyncIterable} = require('ix');\nconst {child} = require('event-stream');\nconst {fork} = require('child_process');\nconst {RecordBatchStreamWriter} = require('apache-arrow');\n\nconst compute_degrees_via_gpu_accelerated_sql = (\n  (scriptPath) => (edgeListColumnName) =>\n    spawn('python3', [scriptPath, edgeListColumnName], {\n      env: process.env,\n      stdio: ['pipe', 'pipe', 'inherit']\n    })\n)(require('path').resolve(__dirname, 'compute_degrees.py'));\n\nfunction compute_degrees(colName, recordBatchReaders) {\n  return AsyncIterable.as(recordBatchReaders)\n    .mergeAll()\n    .pipe(RecordBatchStreamWriter.throughNode())\n    .pipe(compute_degrees_via_gpu_accelerated_sql(colName));\n}\n\nmodule.exports = compute_degrees;\n"})}),"\n",(0,t.jsx)(r.p,{children:"This example construct pipes of streams of events and that python process just reads from stdin, does a GPU-dataframe operation, and writes the results to stdout. (This example uses Rx/IxJS style functional streaming pipelines)."}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.code,{children:"compute_degrees_via_gpu_accelerated_sql"})," returns a node ",(0,t.jsx)(r.code,{children:"child_process"})," that is also a duplex stream, similar to the ",(0,t.jsxs)(r.a,{href:"https://www.npmjs.com/package/event-stream#child-child_process",children:[(0,t.jsx)(r.code,{children:"event-stream#child()"})," method"]})]})]})}function h(e={}){const{wrapper:r}={...(0,n.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},43023:(e,r,a)=>{a.d(r,{R:()=>s,x:()=>c});var t=a(63696);const n={},o=t.createContext(n);function s(e){const r=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function c(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),t.createElement(o.Provider,{value:r},e.children)}}}]);
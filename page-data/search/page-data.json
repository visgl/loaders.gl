{"componentChunkName":"component---node-modules-gatsby-theme-ocular-src-templates-search-jsx","path":"/search","result":{"pageContext":{"data":[{"excerpt":"Contributing Contributions are welcome, assuming that they align with the general design goals and philosophy of the repo. Unless you just…","rawMarkdownBody":"# Contributing\n\nContributions are welcome, assuming that they align with the general design goals and philosophy of the repo.\n\nUnless you just want to contribute a small bug fix, it is a good idea to start by opening an issue and discuss your idea with the maintainers. This maximizes the chances that your contribution will be accepted once you open a pull request.\n\n## Configuring Your Development Environment\n\nTo contribute, you will likely want to clone the loaders.gl repository and make sure you can install, build and run tests.\n\nOur primary development environment is MacOS, but it is possible to build loaders.gl on Linux and Windows (using a Linux environment).\n\n### Setting up Linux Environment on Windows 10\n\nIt is possible to build under Windows, but not directly in the Windows command prompt. You will need to install a Linux command line environment.\n\nInstall [WSL (Windows Subsystem for Linux)](https://docs.microsoft.com/en-us/windows/wsl/install-win10) on Windows 10.\n\n### Install Node and NPM\n\n```bash\nsudo apt update\nsudo apt install nodejs\n```\n\n### Option: Install NVM\n\n- `https://www.liquidweb.com/kb/how-to-install-nvm-node-version-manager-for-node-js-on-ubuntu-12-04-lts/`\n- `https://github.com/nvm-sh/nvm/releases`\n\n### Install yarn\n\nhttps://www.hostinger.com/tutorials/how-to-install-yarn-on-ubuntu/\n\n```bash\nsudo apt update\nsudo apt install yarn nodejs\nyarn –version\n```\n\n### Install jq\n\n```bash\nsudo apt-get install jq\n```\n\n### Configuring your System\n\nOn Linux Systems Install packages\n\n- mesa-utils\n- xvfb\n- libgl1-mesa-dri\n- libglapi-mesa\n- libosmesa6\n- libxi-dev\n\nTo get the headless tests working: export DISPLAY=:99.0; sh -e /etc/init.d/xvfb start\n\n## Running Tests\n\n- `yarn lint`: Check coding standards and formatting\n- `yarn lint fix`: Fix errors with formatting\n- `yarn test node`: Quick test run under Node.js\n- `yarn test browser`: Test run under browser, good for interactive debugging\n- `yarn test`: Run lint, node test, browser tests (in headless mode)\n","slug":"docs/contributing","title":"Contributing"},{"excerpt":"Upgrade Guide Upgrading to v2.0 Version 2.0 is a major release that consolidates functionality and APIs, and a number of deprecated…","rawMarkdownBody":"# Upgrade Guide\n\n## Upgrading to v2.0\n\nVersion 2.0 is a major release that consolidates functionality and APIs, and a number of deprecated functions have been removed.\n\n### `@loaders.gl/core`\n\n| Removal            | Replacement                                                            |\n| ------------------ | ---------------------------------------------------------------------- |\n| `TextEncoder`      | Use global `TextEncoder` instead and `@loaders.gl/polyfills` if needed |\n| `TextDecoder`      | Use global `TextDecoder` instead and `@loaders.gl/polyfills` if needed |\n| `createReadStream` | `fetch().then(resp => resp.body)`                                      |\n| `parseFile`        | `parse`                                                                |\n| `parseFileSync`    | `parseSync`                                                            |\n| `loadFile`         | `load`                                                                 |\n\n### `@loaders.gl/images`\n\n| Removal             | Replacement                                               |\n| ------------------- | --------------------------------------------------------- |\n| `ImageHTMLLoader`   | `ImageLoader` with `options.images.format: 'html'`        |\n| `ImageBitmapLoader` | `ImageLoader` with `options.images.format: 'imagebitmap'` |\n| `decodeImage`       | `parse(arrayBuffer, ImageLoader)`                         |\n| `isImage`           | `isBinaryImage`                                           |\n| `getImageMIMEType`  | `getBinaryImageMIMEType`                                  |\n| `getImageSize`      | `getBinaryImageSize`                                      |\n| `getImageMetadata`  | `getBinaryImageMIMEType` + `getBinaryImageSize`           |\n\n### Loader Objects\n\n- Loaders can no longer have a `loadAndParse` method. Remove it, and just make sure you define `parse` on your loaders instead.\n\n### `@loaders.gl/gltf`\n\nThe `GLTFLoader` now always uses the new v2 parser, and the original `GLTFParser` has been removed.\n\n| Removal            | Replacement  |\n| ------------------ | ------------ |\n| `GLBParser`        | `GLBLoader`  |\n| `GLBBuilder`       | `GLBWriter`  |\n| `GLTFParser`       | `GLTFLoader` |\n| `GLTFBuilder`      | `GLTFWriter` |\n| `packBinaryJson`   | N/A          |\n| `unpackBinaryJson` | N/A          |\n\nNote that automatic packing of binary data (aka \"packed JSON\" support) was only implemented in the v1 `GLTFLoader` and has thus also been removed. Experience showed that packing of binary data for `.glb` files is best handled by applications.\n\n**GLTFLoader option changes**\n\nThe foillowing top-level options are deprecated and will be removed in v2.0\n\n| Removed Option         | Replacement                             | Descriptions                                                              |\n| ---------------------- | --------------------------------------- | ------------------------------------------------------------------------- |\n| `gltf.parserVersion`   | N/A                                     | No longer needs to be specied, only the new gltf parser is available.     |\n| `fetchLinkedResources` | `gltf.fetchBuffers`, `gltf.fetchImages` |                                                                           |\n| `fetchImages`          | `gltf.fetchImages`                      |                                                                           |\n| `createImages`         | N/A                                     | Images are now always created when fetched                                |\n| `decompress`           | `gltf.decompressMeshes`                 | Decompress Draco compressed meshes (if DracoLoader available).            |\n| `DracoLoader`          | N/A                                     | Supply `DracoLoader` to `parse`, or call `registerLoaders(pDracoLoader])` |\n| `postProcess`          | `gltf.postProcess`                      | Perform additional post processing before returning data.                 |\n| `uri`                  | `baseUri`                               | Auto-populated when loading from a url-equipped source                    |\n| `fetch`                | N/A                                     | fetch is automatically available to sub-loaders.                          |\n\n### `@loaders.gl/draco`\n\n| Removal        | Replacement   |\n| -------------- | ------------- |\n| `DracoParser`  | `DracoLoader` |\n| `DracoBuilder` | `DracoWriter` |\n\n### Loader Objects\n\n- Loaders no longer have a `loadAndParse` removed. Just define `parse` on your loaders.\n\n## Upgrading from v1.2 to v1.3\n\n- As with v1.1, `GLTFLoader` will no longer return a `GLTFParser` object in v2.0. A new option `options.gltf.parserVersion: 2` is provided to opt in to the new behavior now.\n\n## Upgrading from v1.0 to v1.1\n\nA couple of functions have been deprecated and will be removed in v2.0. They now emit console warnings. Start replacing your use of these functions now to remove the console warnings and ensure a smooth future upgrade to v2.0.\n\nAlso, Node support now requires installing `@loaders.gl/polyfills` before use.\n\n### @loaders.gl/core\n\n- Removal: Node support for `fetchFile` now requires importing `@loaders.gl/polyfills` before use.\n- Removal: Node support for `TextEncoder`, and `TextDecoder` now requires importing `@loaders.gl/polyfills` before use.\n- Deprecation: `TextEncoder` and `TextDecoder` will not be exported from `loaders.gl/core` in v2.0.\n\n### @loaders.gl/images\n\n- Removal: Node support for images now requires importing `@loaders.gl/polyfills` before use.\n\n### @loaders.gl/gltf\n\n- Deprecation: `GLBParser`/`GLBBuilder` - These will be merged into GLTF classes..\n- Deprecation: `GLTFParser`/`GLTFBuilder` - The new `GLTF` class can hold GLTF data and lets application access/modify it.\n- Deprecation: `GLTFLoader` will no longer return a `GLTFParser` object in v2.0. Instead it will return a pure javascript object containing the parse json and any binary chunks. This object can be accessed through the `GLTF` class. Set `options.GLTFParser` to `false` to opt in to the new behavior now.\n\n## v1.0\n\nFirst official release of loaders.gl.\n","slug":"docs/upgrade-guide","title":"Upgrade Guide"},{"excerpt":"Roadmap We are trying to make the loaders.gl roadmap as public as possible. We share information about the direction of the framework in the…","rawMarkdownBody":"# Roadmap\n\nWe are trying to make the loaders.gl roadmap as public as possible. We share information about the direction of the framework in the following ways:\n\n- **[RFCs](https://github.com/uber-web/loaders.gl/tree/master/dev-docs/RFCs)** - RFCs are technical writeups that describe proposed features in upcoming releases.\n- **[Roadmap Document](https://github.com/uber-web/loaders.gl/tree/master/docs/overview/roadmap)** - (this document) A high-level summary of our current direction for future releases.\n- **[Blog](https://medium.com/@vis.gl)** - We use the vis.gl blog to share information about what we are doing.\n- **[Github Issues](https://github.com/uber-web/loaders.gl/issues)** - The traditional way to start or join a discussion.\n\n## Feature Roadmap\n\nMany ideas are in tracker tasks in github, but here are some ideas:\n\n**Worker Thread Pool Priming** - Worker Pools should have an option to pre-load workers so that loader thread pool is primed and ready to start off-thread parsing as soon as data arrives on the wire. This can avoid 1-2 second lag when loading starts.\n\n**Progress Tracking** - loaders can provide progress callbacks and a `ProgressTracker` class to track the progress of a set of parallel loads.\n\n**Automatic Timing** - objects returned from loaders could contain a `stats` object with timing stats.\n\n**Stats and Default Settings** - Set `setDefaultOptions({stats: true})` to enable stats collection, etc.\n\n**MIME types** - Allow MIME types (e.g. from response headers) to be provided to assist in loader auto-selection. Enable Writers to return recommended MIMEtypes.\n\n## Loader Roadmap\n\n### Data loaders\n\nStreaming tabular loaders\n\n- Streaming JSON loader\n\n### Geospatial loaders\n\nFocus on loading of complex geospatial data.\n\nFocus on loading of large, complex geospatial data.\n\n- KML\n- Shapefile\n  > > > > > > > Add tests\n- Streaming GeoJSON loader\n\n### Images\n\n- Basis image decoder\n- Better image loaders\n\n### Meshes\n\n- MTL - we should have full OBJ/MTL support.\n- Given industry convergence on glTF, we do not envision supporting any otherr mesh formats beyond OBJ/MTL.\n\n### Point Clouds\n\nFocus on loading formats for large point clouds.\n\n### Massive Point Clouds/Data Sets\n\nComplement 3D Tiles with:\n\n- potree\n- i3s\n\n### Scenegraph Formats\n\n- Focus on glTF/GLB - loaders.gl should to have a very solid implementation.\n- The glTF loaders should handle (e.g. preprocess) any glTF extensions that can be handled during the load phase (such as Draco, Basis - but many can only be handled during rendering).\n- Limited alternatives: Given the emergence of glTF as a major Khronos standard, and availability of good glTF conversion tools and exporters, loaders will most likely not implement any other scene/mesh description formats such as COLLADA.\n\n### Other loaders\n\nFinally, some \"unusual\" loaders may be included just for fun, e.g. SVG tesselation.\n","slug":"docs/roadmap","title":"Roadmap"},{"excerpt":"What's New v2.1 (In Development) Release Date: Target Feb-Mar, 2019. v2.0 Release Date: Target Dec, 2019, currently available as  releases…","rawMarkdownBody":"# What's New\n\n## v2.1 (In Development)\n\nRelease Date: Target Feb-Mar, 2019.\n\n## v2.0\n\nRelease Date: Target Dec, 2019, currently available as `2.0.0-beta` releases\n\nThe 2.0 release brings stronger loader composition, image loading improvements, significant overhauls to several loaders and removes deprecated functions across the board.\n\n- `@loaders.gl/images`: Redefined as a new Image Category, see below\n- `@loaders.gl/core`: `loader.loadAndParse` removed.\n\n### @loaders.gl/core\n\n- **Loader Specification Updates**\n\n  - All (non-worker) loaders are now required to expose a `parse` function (in addition to any more specialized `parseSync/parseText/parseInBatches` functions). This simplifies using loaders without `@loaders.gl/core`, which can reduce footprint in small applications.\n  - All exported loader and writer objects now expose a `mimeType` field. This field is not yet used by `@loaders.gl/core` but is available for applications (e.g. see `selectLoader`).\n\n- **Composite Loaders**\n\n  - Loaders can call other loaders\n\n### @loaders.gl/images\n\n- **Image Category** now defined\n\n  - Category ensures interchangability\n\n- **New ImageLoader options**\n- `options.image` contain common options that apply across the category\n\n  - `options.image.type`, Ability to control loaded image type, default `auto`\n  - TBA `options.image.useWorkers: true` - Worker Image Loaders on Chrome and Firefox\n  - TBA `options.image.decodeHTML: true` - Support for `Image.decode()` to ensure HTML images are ready to be used when loader promise resolves.\n\n- **Parsed Image API** To help working with loaded images across platform\n\n  - `isImage`, `getImageType`, `getImageSize`, `getImageData`, ...\n\n- **Binary Image API** Separate API to work with unparsed images in binary data form\n\n  - `isBinaryImage`, `getBinaryImageType`, `getBinaryImageSize`, ...\n\n- **Separate Loaders** (Experimental) Now exports a separate micro loader for each format: `_JPEGLoader`, `_PNGLoader`, `_GIFLoader`, `_BMPLoader`, `_SVGLoader`\n\n- **Separate Loaders**\n\n  - Composite loader (Array) `ImageLoaders` for easy registration.\n\n- **Improved Node.js image support**\n  - More test cases are now run in both browser and Node.js and a few important issues were uncovered and fixed.\n\n### @loaders.gl/json\n\nA new streaming `JSONLoader`(Experimental) that supports batched (i.e. streaming) parsing from standard JSON files, e.g. geojson. No need to reformat your files as line delimited JSON.\n\n## v1.3\n\nRelease Date: Sep 13, 2019\n\nThe 1.3 release is focused on production quality 3D tiles support, maturing the v2 glTF parser, and provides some improvements to the core API.\n\n<table style=\"border: 0;\" align=\"center\">\n  <tbody>\n    <tr>\n      <td style=\"text-align: center;\">\n        <img style=\"max-height:200px\" src=\"https://raw.github.com/uber-web/loaders.gl/master/website/static/images/example-3d-tiles.png\" />\n        <p><strong>Tile3DLoader</strong></p>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n### @loaders.gl/3d-tiles\n\n- **Tile3DLayer moved to deck.gl**\n\n  - The `Tile3DLayer` can now be imported from `@deck.gl/geo-layers`, and no longer needs to be copied from the loaders.gl `3d-tiles` example\n\n- **Batched 3D Model Tile Support**\n\n  - `b3dm` tiles can now be loaded and displayed by the `Tile3DLayer` (in addition to `pnts` tiles).\n\n- **Performance Tracking**\n\n  - `Tileset3D` now contain a `stats` object which tracks the loading process to help profile big tilesets.\n  - Easily displayed in your UI via the `@probe.gl/stats-widget` module (see 3d-tiles example).\n\n- **Request Scheduling**\n  - The `Tileset3D` class now cancels loads for not-yet loaded tiles that are no longer in view).\n  - Scheduling dramatically improves loading performance when panning/zooming through large tilesets.\n\n### @loaders.gl/gltf\n\n- **Version 2 Improvements**\n  - Select the new glTF parser by passing `options.gltf.parserVersion: 2` to the `GLTFLoader`.\n  - Many improvements to the v2 glTF parser.\n\n### @loaders.gl/core\n\n- **Loader Selection Improvements**\n\n  - The loader selection mechanism is now exposed to apps through the new `selectLoader` API.\n  - Loaders can now examine the first bytes of a file\n  - This complements the existing URL extension based auto detection mechanisms.\n\n- **Worker Thread Pool**\n  - Now reuses worker threads. Performance gains by avoiding worker startup overhead.\n  - Worker threads are named, easy to track in debugger\n  - Worker based loaders can now call `parse` recursively to delegate parsing of embedded data (e.g. glTF, Draco) to other loaders\n\n## v1.2\n\nThe 1.2 release is a smaller release that resolves various issues encountered while using 1.1.\n\nRelease Date: Aug 8, 2019\n\n- `@loaders.gl/core`: File Type Auto Detection now supports binary files\n- `@loaders.gl/polyfills`: Fixed `TextEncoder` warnings\n- `@loaders.gl/arrow`: Improved Node 8 support\n- `@loaders.gl/images`: Image file extensions now added to loader object\n- `@loaders.gl/gltf`: Generate default sampler parameters if none provided in gltf file\n\n### @loaders.gl/3d-tiles (EXPERIMENTAL)\n\n- Support for dynamic traversal of 3D tilesets (automatically loads and unloads tiles based on viewer position and view frustum).\n- Support for loading tilesets from Cesium ION servers.\n- Asynchronous tileset loading\n- Auto centering of view based on tileset bounding volumes\n- deck.gl `Tile3DLayer` class provided in examples.\n\n## v1.1\n\nThe 1.1 release addresses a number of gaps in original loaders.gl release, introduces the `GLTFLoader`, and initiates work on 3DTiles support.\n\nRelease Date: May 30, 2019\n\n<table style=\"border: 0;\" align=\"center\">\n  <tbody>\n    <tr>\n      <td style=\"text-align: center;\">\n        <img style=\"max-height:200px\" src=\"https://raw.github.com/uber-web/loaders.gl/master/website/static/images/example-gltf.jpg\" />\n        <p><strong>GLTFLoader</strong></p>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n### @loaders.gl/core\n\n- `fetchFile` function - Can now read browser `File` objects (from drag and drop or file selection dialogs).\n- `isImage(arrayBuffer [, mimeType])` function - can now accept a MIME type as second argument.\n\n### @loaders.gl/images\n\n- `getImageMIMEType(arrayBuffer)` function ( EW) - returns the MIME type of the image in the supplied `ArrayBuffer`.\n- `isImage(arrayBuffer [, mimeType])` function - can now accept a MIME type as second argument.\n\n### @loaders.gl/gltf\n\n- The glTF module has been refactored with the aim of simplifying the loaded data and orthogonalizing the API.\n- \"Embedded' GLB data (GLBs inside other binary formats) can now be parsed (e.g. the glTF parser can now extract embedded glTF inside 3D tile files).\n\n- New classes/functions:\n  - [`GLTFScenegraph`](/docs/api-reference/gltf/gltf-scenegraph) class (NEW) - A helper class that provides methods for structured access to and modification/creation of glTF data.\n  - [`postProcessGLTF`](/docs/api-reference/gltf/post-process-gltf) function ( EW) - Function that performs a set of transformations on loaded glTF data that simplify application processing.\n  - [`GLBLoader`](/docs/api-reference/gltf/glb-loader)/[`GLBWriter`](NEW) - loader/writer pair that enables loading/saving custom (non-glTF) data in the binary GLB format.\n  - [`GLTFLoader`](/docs/api-reference/gltf/gltf-loader), letting application separately handle post-processing.\n\n### @loaders.gl/3d-tiles (NEW MODULE)\n\n- Support for the 3D tiles format is being developed in the new `@loaders.gl/3d-tiles` module.\n- Loading of individual point cloud tiles, including support for Draco compression and compact color formats such as RGB565 is supported.\n\n### @loaders.gl/polyfills (NEW MODULE)\n\nNode support now requires importing `@loaders.gl/polyfills` before use. This reduces the number of dependencies, bundle size and potential build complications when using other loaders.gl modules when not using Node.js support.\n\n### @loaders.gl/loader-utils (NEW MODULE)\n\nHelper functions for loaders have been broken out from `@loaders.gl/core`. Individual loaders no longer depend on`@loaders.gl/core` but only on `@loaders.gl/loader-utils`.\n\n## v1.0\n\nRelease Date: April 2019\n\nFirst Official Release\n","slug":"docs/whats-new","title":"What's New"},{"excerpt":"Introduction loaders.gl is a suite of loaders for file formats for big data visualization, including point clouds, 3D geometries, images…","rawMarkdownBody":"# Introduction\n\nloaders.gl is a suite of loaders for file formats for big data visualization, including point clouds, 3D geometries, images, geospatial formats as well as tabular data.\n\nloaders.gl is part of the [vis.gl](https://vis.gl) ecosystem, and frameworks like [deck.gl](https://deck.gl) and [luma.gl](https://luma.gl) come pre-integrated with loaders.gl. However all all the provided loaders and writers are framework independent, and can be used by any application.\n\n## Quick Code Example\n\nloaders.gl provides a small core API module with common functions to load and save data, and a number of additional modules that provide loaders and writers for specific file formats.\n\nA minimal example using the `load` function and the `CSVLoader` to load a CSV formatted table into a JavaScript array:\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nconst data = await load('data.csv', CSVLoader);\n\nfor (const row of data) {\n  console.log(JSON.stringify(row)); // => '{header1: value1, header2: value2}'\n}\n```\n\nTo quickly get up to speed on how the loaders.gl API works, please see [Get Started](docs/developer-guide/get-started).\n\n## Why loaders.gl?\n\nMany open source projects already contain excellent loaders for key 3D and geospatial formats under permissive licenses. However, due to design limitations (e.g. dependencies on a certain WebGL framework, not packaged for easy re-use, lack of Node.js support, inability to run in worker threads, lack of streaming support, differing interface conventions etc) these can be hard to use in new applications.\n\nloaders.gl collects some of the best existing and a handful of newly written open source loaders, and package them all in a unified, portable, framework-independent way.\n\n## Main Design Goals\n\n**Framework Agnostic** - Files are parsed into clearly documented data structures (objects + typed arrays) that can be used with any JavaScript framework.\n\n**Worker Support** - Many loaders can run in web workers, keeping the main thread free for other tasks while parsing completes.\n\n**Node Support** - All loaders are work under Node.js, useful for running your unit tests under Node.\n\n**Loader Categories** - loaders.gl groups similar data formats into \"categories\". loaders in the same category return parsed data in \"standardized\" form, simplifying handling of multiple related formats.\n\n**Format Autodection** - Applications can specify multiple loaders when parsing a file, and loaders.gl will automatically pick the right loader for a given file.\n\n**Bundle Size Optimized** - Each format is published as an independent npm module to allow cherry-picking, and additionally, modules are setup to let tree-shaking remove any symbols not imported by user.\n\n**Modern JavaScript** - loaders.gl is written in standard ES2018 and uses fresh JavaScript constructs, e.g. async iterators instead of streams, `ArrayBuffer` instead of `Buffer`, etc.\n\n**WebGL Optimized** - loaders.gl is optimized for use with WebGL frameworks (e.g. by returning typed arrays whenever possible). However, there are no any actual WebGL dependencies and loaders can be used without restrictions in non-WebGL applications.\n\n## Supported Platforms\n\nOur intention is for loaders.gl to work well on recent versions of the major evergreen browsers (Chrome, Firefox, Safari, both desktop and mobile). We also support all Node.js LTS versions. (Long-Term Support includes Node 12, Node 10, and Node 8 through December 2019. Node.js support assumes `@loaders.gl/polyfills` is installed).\n\nWe also have an ambition that loaders.gl should run on Edge, IE11 and Node.js v8, however this assumes that both `@loaders.gl/polyfills` and additional appropriate polyfills (e.g. babel polyfills) are installed. Testing on these older platforms is limited, so if an issue on these platforms is detected please report it, if there is a clear solution we will try to fix it.\n\n## Licenses, Credits and Attributions\n\nLicense-wise, loaders.gl currently contains a collection of MIT, BSD and Apache licensed loaders. Each loader comes with its own license, so if the distinction matters to you, please check and decide accordingly. Additional licenses might be included in the future, however loaders.gl will never include code with non-permissive, commercial or copy-left licenses.\n\nRegading attributions, loaders.gl is partly a repackaging of the superb work done by many others in the open source community. We try to be as explicit as we can about the origins and attributions of each loader, both in the documentation page for each loader and in the preservation of comments relating to authorship and contributions inside forked source code.\n\nEven so, we can make mistakes, and we may note have the full history of the code we are reusing. If you think that we have missed something, or that we could do better in regard to attribution, please let us know.\n","slug":"docs","title":"Introduction"},{"excerpt":"Category: GIS The GIS category is highly experimental and may be removed in a future release Several geospatial formats return data in the…","rawMarkdownBody":"# Category: GIS\n\n> The GIS category is highly experimental and may be removed in a future release\n\nSeveral geospatial formats return data in the form of lists of lng/lat encoded geometric objects.\n\n## GeoJSON Conversion\n\nGIS category data can be converted to GeoJSON (sometimes with a loss of information). Most geospatial applications can consume geojson.\n\n## Data Structure\n\nA JavaScript object with a number of top-level array-valued fields:\n\n| Field           | Description                                          |\n| --------------- | ---------------------------------------------------- |\n| `points`        | A [GeoJson](https://geojson.org/) FeatureCollection. |\n| `lines`         | A [GeoJson](https://geojson.org/) FeatureCollection. |\n| `polygons`      | A [GeoJson](https://geojson.org/) FeatureCollection. |\n| `imageoverlays` | Urls and bounds of bitmap overlays                   |\n| `documents`     |                                                      |\n| `folders`       |                                                      |\n| `links`         |                                                      |\n\n## Loaders\n\n- [KMLLoader](/docs/api-reference/kml/kml-loader)\n","slug":"docs/specifications/category-gis","title":"Category: GIS"},{"excerpt":"Category: 3D Tiles The 3D tiles category is still under development. The 3D Tiles category defines a generalized, slightly abstracted…","rawMarkdownBody":"# Category: 3D Tiles\n\n> The 3D tiles category is still under development.\n\nThe 3D Tiles category defines a generalized, slightly abstracted representation of hierarchical geospatial data structures.\n\nIt is being defined to be able to represent the [OGC 3D Tiles](https://www.opengeospatial.org/standards/3DTiles) standard but is intended to be generalized and extended to handle the similar formats, potentially such as [OGC i3s](https://www.opengeospatial.org/standards/i3s) standard and the `potree` format as well.\n\n## Concepts\n\n- **Tile Header Hierarchy** - An initial, \"minimal\" set of data listing the _hierarchy of available tiles_, with minimal information to allow an application to determine which tiles need to be loaded based on a certain viewing position in 3d space.\n- **Tile Header** - A minimal header describing a tiles bounding volume and a screen space error tolerance (allowing the tile to be culled if it is distant), as well as the URL to load the tile's actual content from.\n- **Tile Cache** - Since the number of tiles in big tilesets often exceed what can be loaded into available memory, it is important to have a system that releases no-longer visible tiles from memory.\n- **Tileset Traversal** - Dynamically loading and rendering 3D tiles based on current viewing position, possibly triggering loads of new tiles and unloading of older, no-longer visible tiles.\n\n## Tileset Traversal Support\n\nTo start loading tiles once a top-level tileset file is loaded, the application can instantiate the `Tileset3D` class and start calling `tileset3D.traverse(camera_parameters)`.\n\nSince 3D tiled data sets tend to be very big, the key idea is to only load the tiles actually needed to show a view from the current camera position.\n\nThe `Tileset3D` allows callbacks (`onTileLoad`, `onTileUnload`) to be registered that notify the app when the set of tiles available for rendering has changed. This is important because tile loads complete asynchronously, after the `tileset3D.traverse(...)` call has returned.\n\n## Coordinate Systems\n\nTo help applications process the `position` data in the tiles, 3D Tiles category loaders are expected to provide matrices are provided to enable tiles to be used in both fixed frame or cartographic (long/lat-relative, east-north-up / ENU) coordinate systems:\n\n- _cartesian_ WGS84 fixed frame coordinates\n- _cartographic_ tile geometry positions to ENU meter offsets from `cartographicOrigin`.\n\nPosition units in both cases are in meters.\n\nFor cartographic coordinates, tiles come with a prechosen cartographic origin and precalculated model matrix. This cartographic origin is \"arbitrary\" (chosen based on the tiles bounding volume center). A different origin can be chosen and a transform can be calculated, e.g. using the math.gl `Ellipsoid` class.\n\n## Tileset Fields\n\n| Field                | Type                | Contents                                                                    |\n| -------------------- | ------------------- | --------------------------------------------------------------------------- |\n| `asset`              | `Object` (Optional) |                                                                             |\n| `root`               | `Object`            | The root tile header                                                        |\n| `cartesianCenter`    | `Number[3]`         | Center of tileset in fixed frame coordinates                                |\n| `cartographicCenter` | `Number[3]`         | Center of tileset in cartographic coordinates `[long, lat, elevation]`      |\n| `webMercatorZoom`    | `Number[3]`         | A web mercator zoom level that displays the entire tile set bounding volume |\n\n## TileHeader Fields\n\n| Field            | Type     | Contents |\n| ---------------- | -------- | -------- |\n| `boundingVolume` | `Object` |          |\n\n## Tile Fields\n\n### Common Fields\n\n| Field                     | Type                | Contents                                                                           |\n| ------------------------- | ------------------- | ---------------------------------------------------------------------------------- |\n| `loaderData`              | `Object` (Optional) | Format specific data                                                               |\n| `version`                 | `Number`            | See [Header](#header)                                                              |\n| `type`                    | `String`            | See [Mode](#mode)                                                                  |\n| `cartesianOrigin`         | `Number[3]`         | \"Center\" of tile geometry in WGS84 fixed frame coordinates                         |\n| `cartographicOrigin`      | `Number[3]`         | \"Origin\" in lng/lat (center of tile's bounding volume)                             |\n| `cartesianModelMatrix`    | `Number[16]`        | Transforms tile geometry positions to fixed frame coordinates                      |\n| `cartographicModelMatrix` | `Number[16]`        | Transforms tile geometry positions to ENU meter offsets from `cartographicOrigin`. |\n\n### PointCloudTile Fields\n\n| Field                        | Type           | Contents                                  |\n| ---------------------------- | -------------- | ----------------------------------------- |\n| `attributes`                 | `Object`       | Values are [accessor](#accessor) objects. |\n| `attributes.positions.value` | `Float32Array` |                                           |\n| `attributes.normals.value`   | `Float32Array` |                                           |\n| `attributes.colors.value`    | `Uint8Array`   |                                           |\n\nTBA - batch ids?\n\n### Instanced3DModelTile Fields\n\n| Field         | Type | Contents |\n| ------------- | ---- | -------- |\n| `modelMatrix` |      |          |\n\n### PointCloudTile Fields\n\n| Field | Type | Contents |\n| ----- | ---- | -------- |\n\n\n### CompositeTile Fields\n\n| Field   | Type       | Contents                     |\n| ------- | ---------- | ---------------------------- |\n| `tiles` | `Object[]` | Array of parsed tile objects |\n\n### Accessors\n\nFollowing vis.gl conventions, `attributes` are represented by \"glTF-style\" accessor objects with the `value` field containing the binary data for that attribute stored in a typed array of the proper type.\n\n| Accessors Fields | glTF? | Type         | Contents                                                                                                                     |\n| ---------------- | ----- | ------------ | ---------------------------------------------------------------------------------------------------------------------------- |\n| `value`          | No    | `TypedArray` | Contains the typed array (corresponds to `bufferView`). The type of the array will match the GL constant in `componentType`. |\n| `size`           | No    | `Number`     | Number of components, `1`-`4`.                                                                                               |\n| `byteOffset`     | Yes   | `Number`     | Starting offset into the bufferView.                                                                                         |\n| `count`          | Yes   | `Number`     | The number of elements/vertices in the attribute data.                                                                       |\n","slug":"docs/specifications/category-3d-tiles","title":"Category: 3D Tiles"},{"excerpt":"Category: Scenegraph The Scenegraph category is intended to represent glTF scenegraphs. The data format is fairly raw, close to the unpacked…","rawMarkdownBody":"# Category: Scenegraph\n\nThe Scenegraph category is intended to represent glTF scenegraphs.\n\nThe data format is fairly raw, close to the unpacked glTF/GLB data structure, it is described by:\n\n- a parsed JSON object (with top level arrays for `scenes`, `nodes` etc)\n- a list of `ArrayBuffer`s representing binary blocks (into which `bufferViews` and `images` in the JSON point).\n\n## Helper Classes\n\nTo simplify higher-level processing of the loaded, raw glTF data, several helper classes are provided in the `@loaders.gl/gltf` module, these can:\n\n- unpack and remove certain glTF extensions\n- extract typed array views from the JSON objects into the binary buffers\n- create HTML images from image buffers\n- etc\n\n## Non-glTF Scenegraphs\n\nThe scenegraph \"category\" is quite specific glTF, and there are no plans to support other scenegraph formats in loaders.gl. Therefore, the current recommendation is to convert scenegraph files to glTF with external tools before loading them using loaders.gl.\n\nThat said, hypothetical new loaders for other scenegraph formats (e.g. a COLLADA loader) could potentially choose to belong to the Scenegraph category by \"converting\" the loaded data to this glTF format (and thus enable interoperability with applications that are already designed to use the `GLTFLoader`).\n\n## Data Structure\n\nA JSON object with the following top-level fields:\n\n| Field     | Type            | Default | Description                                              |\n| --------- | --------------- | ------- | -------------------------------------------------------- |\n| `magic`   | `Number`        | glTF    | The first four bytes of the file                         |\n| `version` | `Number`        | `2`     | The version number                                       |\n| `json`    | `Object`        | `{}`    | The JSON chunk                                           |\n| `buffers` | `ArrayBuffer[]` | `[]`    | (glTF) The BIN chunk plus any base64 or BIN file buffers |\n\nBuffers can be objects in the shape of `{buffer, byteOffset, byteLength}`.\n\n## Loaders\n\n- [GLTFLoader](/docs/api-reference/gltf/gltf-loader)\n- [GLBLoader](/docs/api-reference/gltf/glb-loader)\n\n## Notes\n\n- [Tile3DLoader](/docs/api-reference/3d-tiles/tile-3d-loader) some tiles contain embedded glTF.\n","slug":"docs/specifications/category-scenegraph","title":"Category: Scenegraph"},{"excerpt":"Category: Image The Image Category is being defined for loaders.gl v2.0 and is currently experimental and unstable. The image category…","rawMarkdownBody":"# Category: Image\n\n> The Image Category is being defined for loaders.gl v2.0 and is currently experimental and unstable.\n\nThe image category documents a common data format, options, conventions and utilities for loader and writers for images that follow loaders.gl conventions.\n\nImage category loaders includes: `JPEGLoader`, `PNGLoader`, `GIFLoader`, `BMPLoader`, `SVGLoader` and of course all the loaders in the `ImageLoaders` composite loader.\n\n## Features and Capabilities\n\nApart from providing a set of image loaders that integrate with loaders.gl, there are a number of capabilities that are provided for\n\n- Worka under both node and browser.\n- Transparently uses ImageBitmap on supporting browsers\n- Loads images on workers\n- Handles SVG images\n- Image type detection (without loading images)\n\n## Installation and Usage\n\nImage category support is bundled in the `@loaders.gl/images` module:\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/images\n```\n\nIndividual loaders for specific image formats can be imported for `@loaders.gl/images`:\n\n```js\nimport {JEPGLoader, PNGLoader} from '@loaders.gl/images';\nimport {registerLoaders, load} from '@loaders.gl/core';\nregisterLoaders([JEPGLoader, PNGLoader]);\nconst image = await load('image.jpeg');\n```\n\nHowever since each image loader is quite small (in terms of code size and bundle size impact), most applications will just install all image loaders in one go:\n\n```js\nimport {ImageLoaders} from '@loaders.gl/images';\nimport {registerLoaders, load} from '@loaders.gl/core';\nregisterLoaders(ImageLoaders);\nconst image = await load('image.jpeg');\n```\n\n## Data Formats\n\nThe loaded image representation can vary somewhat based on your environment. For performance, image loaders use native image loading functionality in browsers. Browsers can load into two types of image classes (`ImageBitmap` and `HTMLImageElement`) and on Node.js images are represented using `ndarray`. The following table summarizes the situation:\n\n| Format Name   | Format                           | Availability                           | Workers                | Description                                                                      |\n| ------------- | -------------------------------- | -------------------------------------- | ---------------------- | -------------------------------------------------------------------------------- |\n| `imagebitmap` | `ImageBitmap`                    | Chrome/Firefox                         | Yes: **transferrable** | A newer class designed for efficient loading of images for use with WebGL        |\n| `html`        | `Image` (aka `HTMLImageElement`) | All browsers                           | No                     | The original HTML class used for image loading into DOM trees. WebGL compatible. |\n| `ndarray`     | ndarray                          | Node only, via `@loaders.gl/polyfills` | No                     | Used to load images under node. Compatible with headless gl.                     |\n\n## Options\n\nThe image category support some generic options (specified using `options.image.<option-name>`), that are applicable to all (or most) image loaders.\n\n| Option                           | Default       | Type    | Availability    | Description                                          |\n| -------------------------------- | ------------- | ------- | --------------- | ---------------------------------------------------- |\n| `options.image.format`           | `'auto'`      | string  | See table       | One of `auto`, `imagebitmap`, `html`, `ndarray`      |\n| `options.image.decodeHTML`       | `true`        | boolean | No: Edge, IE11  | Wait for HTMLImages to be fully decoded.             |\n| `options.image.crossOrigin`      | `'anonymous'` | boolean | All Browsers    | Sets `crossOrigin` field for HTMLImage loads         |\n| `options.image.useWorkers` (TBA) | `true`        | boolean | Chrome, Firefox | If true, uses worker loaders on supported platforms. |\n\n## Notes\n\n### About worker support\n\n- Worker loading is only supported for the `imagebitmap` format (on Chrome and Firefox).\n- `ImageBitmap` is **transferrable** and can be moved back to main thread without copying.ß\n- There should be no technical limitations to loading images on workers in node, however node workers are not supported yet.\n\nIn contrast to other modules, where worker loaders have to be separately installed, since image workers are small and worker loading is only available on some browsers, the image loaders dynamically determines if worker loading is available.\nUse `options.image.useWorkers: false` to disable worker loading of images on all platforms.\n\n## Utilities\n\nThe image category also provides a few utilities:\n\n- Detecting (\"sniffing\") mime type and size of image files before parsing them\n- Getting image data (arrays of pixels) from an image without knowing which type was loaded (TBA)\n","slug":"docs/specifications/category-image","title":"Category: Image"},{"excerpt":"Category: Table This category provides a set of conventions for working with tables in row-based, columnar or chunked/batched columnar…","rawMarkdownBody":"# Category: Table\n\nThis category provides a set of conventions for working with tables in row-based, columnar or chunked/batched columnar formats.\n\n## Data Structure\n\n| Field    | Type                | Contents                                                     |\n| -------- | ------------------- | ------------------------------------------------------------ |\n| `schema` | `Object`            | Metadata of the table, maps name of each column to its type. |\n| `data`   | `Object` or `Array` | Data of the table, see [table types](#table-types)           |\n| `length` | `Number`            | Number of rows                                               |\n\n## Table Types\n\nloaders.gl deals with (and offers utilities to convert between) three different types of tables:\n\n### Classic Tables (Row-Major)\n\nThis is the classic JavaScript table. `data` consists of an `Array` of `Object` instances, each representing a row.\n\n### Columnar Tables (Column-Major)\n\nColumnar tables are stored as one array per column. Columns that are numeric can be loaded as typed arrays which are stored in contigous memory. `data` is an `Object` that maps column names to an array or typed array.\n\nContiguous memory has tremendous benefits:\n\n- Values are adjacent in memory, the resulting cache locality can result in big performance gains\n- Typed arrays can of course be efficiently transferred from worker threads to main thread\n- Can be directly uploaded to the GPU for further processing.\n\n### Chunked Columnar Tables (DataFrames)\n\nA problem with columnar tables is that column arrays they can get very long, causing issues with streaming, memory allication etc. A powerful solution is to worked with chunked columnar tables, where columns is are broken into matching sequences of typed arrays.\n\nThe down-side is that complexity can increase quickly. Data Frames are optimized to minimize the amount of copying/moving/reallocation of data during common operations such e.g. loading and transformations, and support zero-cost filtering through smart iterators etc.\n\nUsing the Arrow API it is possible to work extremely efficiently with very large (multi-gigabyte) datasets.\n\n## Loaders\n\n- [ArrowLoader](/docs/api-reference/arrow/arrow-loader)\n- [CSVLoader](/docs/api-reference/csv/csv-loader)\n","slug":"docs/specifications/category-table","title":"Category: Table"},{"excerpt":"Loader Object To be compatible with the parsing/loading functions in  such as  and , a parser needs to be described by a \"loader object…","rawMarkdownBody":"# Loader Object\n\nTo be compatible with the parsing/loading functions in `@loaders.gl/core` such as `parse` and `load`, a parser needs to be described by a \"loader object\" conforming to the following specification.\n\n## Loader Object Format v1.0\n\n### Common Fields\n\n| Field               | Type       | Default  | Description                                                     |\n| ------------------- | ---------- | -------- | --------------------------------------------------------------- |\n| `name`              | `String`   | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension`         | `String`   | Required | Three letter (typically) extension used by files of this format |\n| `extensions`        | `String[]` | Required | Array of file extension strings supported by this loader        |\n| `category`          | `String`   | Optional | Indicates the type/shape of data                                |\n| `parse` \\| `worker` | `Function` | `null`   | Every non-worker loader should expose a `parse` function.       |\n\nNote: Only one of `extension` or `extensions` is required. If both are supplied, `extensions` will be used.\n\n### Test Function\n\n| Field      | Type       | Default  | Description                                                                                   |\n| ---------- | ---------- | -------- | --------------------------------------------------------------------------------------------- |\n| `test`     | `Function` | `String` | `String[]`                                                                                    | `null` | Guesses if a binary format file is of this format by examining the first bytes in the file. If the test is specified as a string or array of strings, the initial bytes are expected to be \"magic bytes\" matching one of the provided strings. |\n| `testText` | `Function` | `null`   | Guesses if a text format file is of this format by examining the first characters in the file |\n\n### Parser Functions\n\nEach (non-worker) loader should define a `parse` function. Additional parsing functions can be exposed depending on the loaders capabilities, to optimize for text parsing, synchronous parsing, streaming parsing, etc:\n\n| Parser function field               | Type       | Default | Description                                                                            |\n| ----------------------------------- | ---------- | ------- | -------------------------------------------------------------------------------------- |\n| `parse`                             | `Function` | `null`  | Asynchronously parses binary data (e.g. file contents) asynchronously (`ArrayBuffer`). |\n| `parseInBatches` (Experimental)     | `Function` | `null`  | Parses binary data chunks (`ArrayBuffer`) to output data \"batches\"                     |\n| `parseInBatchesSync` (Experimental) | `Function` | `null`  | Synchronously parses binary data chunks (`ArrayBuffer`) to output data \"batches\"       |\n| `parseSync`                         | `Function` | `null`  | Atomically and synchronously parses binary data (e.g. file contents) (`ArrayBuffer`)   |\n| `parseTextSync`                     | `Function` | `null`  | Atomically and synchronously parses a text file (`String`)                             |\n\nSynchronous parsers are more flexible as they can support synchronous parsing which can simplify application logic and debugging, and iterator-based parsers are more flexible as they can support batched loading of large data sets in addition to atomic loading.\n\nYou are encouraged to provide the most capable parser function you can (e.g. `parseSync` or `parseToIterator` if possible). Unless you are writing a completely new loader from scratch, the appropriate choice often depends on the capabilities of an existing external \"loader\" that you are working with.\n\n### Parser Function Signatures\n\n- `async parse(data : ArrayBuffer, options : Object, context : Object) : Object`\n- `parseSync(data : ArrayBuffer, options : Object, context : Object) : Object`\n- `parseInBatches(data : AsyncIterator, options : Object, context : Object) : AsyncIterator`\n\nThe `context` parameter will contain the foolowing fields\n\n- `parse` or `parseSync`\n- `url` if available\n","slug":"docs/specifications/loader-object-format","title":"Loader Object"},{"excerpt":"Creating New Loaders and Writers See the a detailed specification of the loader object format API reference. Overview Applications can also…","rawMarkdownBody":"# Creating New Loaders and Writers\n\n> See the a detailed specification of the [loader object format API reference](docs/specifications/loader-object-format).\n\n## Overview\n\nApplications can also create new loader objects. E.g. if you have existing JavaScript parsing functionality that you would like to use with the loaders.gl core utility functions.\n\n## Creating a Loader Object\n\nYou would give a name to the loader object, define what file extension(s) it uses, and define a parser function.\n\n```js\nexport default {\n  name: 'JSON',\n  extensions: ['json'],\n  testText: null,\n  parse: async (arrayBuffer) => await JSON.parse(new TextDecoder().decode(arrayBuffer),\n  parseTextSync: JSON.parse\n};\n```\n\n| Field       | Type       | Default  | Description                                                                       |\n| ----------- | ---------- | -------- | --------------------------------------------------------------------------------- |\n| `name`      | `String`   | Required | Short name of the loader ('OBJ', 'PLY' etc)                                       |\n| `extension` | `String`   | Required | Three letter (typically) extension used by files of this format                   |\n| `testText`  | `Function` | `null`   | Guesses if a file is of this format by examining the first characters in the file |\n\nA loader must define a parser function for the format, a function that takes the loaded data and converts it into a parsed object.\n\nDepending on how the underlying loader works (whether it is synchronous or asynchronous and whether it expects text or binary data), the loader object can expose the parser in a couple of different ways, specified by provided one of the parser function fields.\n\n## Dependency Management\n\nIn general, it is recommended that loaders are \"standalone\" and avoid importing `@loaders.gl/core`. `@loaders.gl/loader-utils` provides a small set of shared loader utilities.\n\n## Creating Composite Loaders\n\nloaders.gl enables loaders to call other loaders (referred to as \"sub-loaders\" in this section). This enables loaders for \"composite formats\" to be \"composed\" out of loaders for the primitive parts.\n\nGood examples of sub-loaders are the `GLTFLoader` which can delegate Draco mesh decoding to the `DracoLoader` and image decoding to the various `ImageLoaders` and the `BasisLoader`.\n\nNaturally, Composite loaders can call other composite loaders, which is for instance used by the `Tile3DLoader` which uses the `GLTFLoader` to parse embedded glTF data in certain tiles.\n\n## Calling loaders inside loaders\n\nTo call another loader, a loader should use the appropriate `parse` function provided in the `context` parameter.\n\nA conceptual example of a 3D Tiles loader calling the `GLTFLoader` with some additional options.\n\n```js\nexport async function parse3DTile(arrayBuffer, options, context) {\n  const tile = {};\n  // Extract embedded GLB (if present) into `tile.gltfArrayBuffer`\n  ...\n  if (tile.gltfArrayBuffer) {\n    const {parse} = context;\n    tile.gltf = await parse(tile.gltfArrayBuffer, GLTFLoader, {\n      gltf: {...}\n    });\n  }\n}\n```\n\nRemarks:\n\n- While a loader could potentially import `parse` from `@loaders.gl/core` to invoke a sub-loader, it is discouraged, not only from a dependency management reasons, but it prevents loaders.gl from properly handling parameters and allow worker-loaders to call other loaders.\n","slug":"docs/developer-guide/creating-loaders-and-writers","title":"Creating New Loaders and Writers"},{"excerpt":"Managing Dependencies This section is work in progress, not all options are implemented/finalized Parsers and encoders for some formats are…","rawMarkdownBody":"# Managing Dependencies\n\n> This section is work in progress, not all options are implemented/finalized\n\nParsers and encoders for some formats are quite complex and can be quite big in terms of code size.\n\n### Loading Dependencies from Alternate CDN\n\nBy default, loaders.gl loads pre-built workers and a number of bigger external libraries from the [https://unpkg.com/](https://unpkg.com/) CDN.\n\nIt is possible to specify other CDNs using `options.cdn`.\n\nKeep in mind that it is typically not sufficient to point to a server that just serves the data of the files in question. Browsers do a number of security checks on cross-origin content and requires certain response headers to be properly set, and unfortunately, error messages are not always helpful.\n\nTo determine your candidate CDN service is doing what is needed, check with `curl -u <url>` and look for headers like:\n\n```\ncontent-type: application/javascript; charset=utf-8\naccess-control-allow-origin: *\n```\n\n### Loading Dependencies from Your Own Server\n\nBy setting `options.cdn: false` and doing some extra setup, you can load dependencies from your own server. This removes the impact of a potentially flaky CDN.\n\nOptions:\n\n- Load from `node_modules/@loaders.gl/<module>/dist/libs/...`\n- Load from a modules directory `libs/...`\n- Load from unique locations - `options.modules[<dependency name>]` can be set to url strings.\n\n### Bundling Dependencies\n\nIt is also possible to include dependencies in your application bundle\n\n- PRO: Doesn't require copying/configuring/serving supporting modules.\n- CON: Increases the size of your application bundle\n\n`options.modules` will let your application `import` or `require` dependencies (thus bundling them) and supply them to loaders.gl.\n\nSee each loader module for information on its dependencies.\n\nExample: bundling the entire `draco3d` library:\n\n```js\nimport draco from 'draco3d';\nimport {setLoaderOptions} from '@loaders.gl/core';\nsetLoaderOptions({\n  modules: {\n    draco3d\n  }\n});\n```\n","slug":"docs/developer-guide/dependencies","title":"Managing Dependencies"},{"excerpt":"Writer Object To be compatible with  functions such as , writer objects need to conform to the following specification: Common Fields Field…","rawMarkdownBody":"# Writer Object\n\nTo be compatible with `@loaders.gl/core` functions such as `encode`, writer objects need to conform to the following specification:\n\n### Common Fields\n\n| Field       | Type     | Default  | Description                                                     |\n| ----------- | -------- | -------- | --------------------------------------------------------------- |\n| `name`      | `String` | Required | Short name of the loader ('OBJ', 'PLY' etc)                     |\n| `extension` | `String` | Required | Three letter (typically) extension used by files of this format |\n| `category`  | `String` | Optional | Indicates the type/shape of data                                |\n\n### Encoder Function\n\n| Field                            | Type       | Default | Description                                            |\n| -------------------------------- | ---------- | ------- | ------------------------------------------------------ |\n| `encodeSync`                     | `Function` | `null`  | Encodes synchronously                                  |\n| `encode`                         | `Function` | `null`  | Encodes asynchronously                                 |\n| `encodeInBatches` (Experimental) | `Function` | `null`  | Encodes and releases batches through an async iterator |\n\nNote: The format of the input data to the encoders depends on the loader. Several loader categories are defined to provided standardized data formats for similar loaders.\n","slug":"docs/specifications/writer-object-format","title":"Writer Object"},{"excerpt":"Get Started Installing Install loaders.gl core and loader for any modules you would like to use. Each format is published as a separate npm…","rawMarkdownBody":"# Get Started\n\n## Installing\n\nInstall loaders.gl core and loader for any modules you would like to use.\n\nEach format is published as a separate npm module.\n\n```bash\nyarn add @loaders.gl/core\nyarn add @loaders.gl/gltf\n...\n```\n\n## Usage\n\nYou can import a loader and use it directly with `parse`. Note that `parse` can accept a `fetch` response object as the source of data to be parsed:\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\nconst data = await parse(fetch('data.csv'), CSVLoader);\n```\n\nYou can register loaders after importing them\n\n```js\nimport {registerLoaders} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\nregisterLoaders(CSVLoader);\n```\n\nThen, in the same file (or some other file in the same app) that needs to load CSV, you no longer need to supply the loader to `parse`. It will autodetect the pre-registered loader:\n\n```js\nimport {parse} from '@loaders.gl/core';\n\n// The pre-registered CSVLoader gets auto selected based on file extension...\nconst data = await parse(fetch('data.csv'));\n```\n\n## Building\n\nYou can use your bundler of choice such as webpack or rollup. See the [`get-started-...`](https://github.com/uber-web/loaders.gl/tree/master/examples) examples for minimal working examples of how to bundle loaders.gl.\n\n## Supporting Older Browsers\n\nloaders.gl is designed to leverage modern JavaScript (ES2018) and to optimize functionality and performance on evergreen browsers.\n\nHowever, the default distribution is completely transpiled to ES5 so using loaders.gl with older or \"slower moving\" browsers such as IE11 and Edge is possible, assuming that the appropriate polyfills are installed.\n\nTo build on Edge and IE11, `TextEncoder` and `TextDecoder` must be polyfilled. There are several polyfills available on `npm`, but you can also use the polyfills provided by loaders.gl:\n\n```bash\nyarn install @loaders.gl/polyfills\n```\n\n```js\nimport '@loaders.gl/polyfills';\n```\n\n## Supporting Node.js\n\nA number of polyfills for `fetch`, `TextEncoder` etc are available to make loaders.gl work under Node.js, just install the `@loaders.gl/polyfills module` as described above.\n","slug":"docs/developer-guide/get-started","title":"Get Started"},{"excerpt":"Polyfills Older browsers (mainly Edge and IE11) as well as Node.js do not provide certain APIs (,  etc) that loaders.gl depends on. The good…","rawMarkdownBody":"# Polyfills\n\nOlder browsers (mainly Edge and IE11) as well as Node.js do not provide certain APIs (`TextEncoder`, `fetch` etc) that loaders.gl depends on.\n\nThe good news is that these APIs can be provided by the application using the [polyfill](<https://en.wikipedia.org/wiki/Polyfill_(programming)>) technique.\n\nWhile there are many good polyfill modules for these classes available on `npm`, to make the search for a version that is guaranteed to work with loaders.gl a little easier, the `@loaders.gl/polyfills` module is provided.\n\nTo install these polyfills, just `import` the polyfills module before start using loaders.gl.\n\n```js\nimport '@loaders.gl/polyfills';\nimport {parse} from '@loaders.gl/core';\n```\n\n## Combining with other Polyfills\n\nloaders.gl only installs polyfills if the corresponding global symbol is `undefined`. This means that if another polyfill is already installed when `@loaders.gl/polyfills` is imported, the other polyfill will remain in effect. Since most polyfill libraries work this way, applications can mix and match polyfills by ordering the polyfill import statements appropriately (but see the remarks below for a possible caveat).\n\n## Provided Polyfills\n\nSee [API Reference](/docs/api-reference/polyfills).\n\n## Remarks\n\nApplications should typically only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, they will still be included in your application bundle, i.e. importing the polyfill module will increase your application's bundle size.\n\nWhen importing polyfills for the same symbol from different libraries, the import can depend on how the other polyfill is written. to control the order of installation, you may want to use `require` rather than `import` when importing `@loaders.gl/polyfills`. As a general rule, `import` statements execute before `require` statments.\n","slug":"docs/developer-guide/polyfills","title":"Polyfills"},{"excerpt":"Loader Categories To simplify working with multiple similar formats, loaders and writers in loaders.gl are grouped into categories. The idea…","rawMarkdownBody":"# Loader Categories\n\nTo simplify working with multiple similar formats, loaders and writers in loaders.gl are grouped into _categories_.\n\nThe idea is that many loaders return very similar data (e.g. point clouds loaders), which makes it possible to represent the loaded data in the same data structure, letting applications handle the output from multiple loaders without\n\nWhen a loader is documented as belonging to a specifc category, it converts the parsed data into the common format for that category. This allows an application to support multiple formats with a single code path, since all the loaders will return similar data structures.\n\n## Categories and Loader Registration\n\nThe fact that loaders belong to categories enable applications to flexibly register new loaders in the same category.\n\nFor instance, once an application has added support for one loader in a category, other loaders in the same category can be registered during application startup.\n\nOriginal code\n\n```js\nimport {parse, registerLoaders} from '@loaders.gl/core';\nimport {PCDLoader} from `@loaders.gl/pcd';\nregisterLoaders([PCDLoader]);\nasync function loadPointCloud(url) {\n  const pointCloud = await parse(fetch(url));\n  // Use some WebGL framework to render the parsed cloud\n}\n```\n\nNow support for additional point cloud formats can be added to the application without touching the original code:\n\n```js\nimport {LASLoader} from `@loaders.gl/las';\nimport {DracoLoader} from `@loaders.gl/draco';\nregisterLoaders([LASLoader, DracoLoader]);\n```\n\n## Data Format\n\nEach category documents the returned data format. loaders and writers reference the category documentation.\n\n## Writers and Categories\n\nWriters for a format that belongs to a category accept data objects with fields described by the documentation for that category.\n\n## Accessing Format-Specific Data\n\nSometimes, not all the properties provided by a certain file format can be mapped to common properties defined by the corresponding loader category.\n\nTo access format-specific properties, use the `loaderData` field in data object returned by the loader.\n\n## Available Categories\n\nCategories are described in the specifications section. Some currently defined categories are:\n\n- [Table](/docs/specifications/category-table)\n- [PointCloud/Mesh](/docs/specifications/category-mesh)\n- [Scenegraph](/docs/specifications/category-scenegraph)\n- [GIS](/docs/specifications/category-gis)\n","slug":"docs/developer-guide/loader-categories","title":"Loader Categories"},{"excerpt":"Error Handling Applications typically want to provide solid error handling when loading and saving data. Ideally the applications wants to…","rawMarkdownBody":"# Error Handling\n\nApplications typically want to provide solid error handling when loading and saving data. Ideally the applications wants to use a simple clean API for the loading, and yet have the confidence that errors are caught and meaningful messages are presented to the user.\n\n## Types of Errors\n\nThere are tree main types of errors that arise when attempting to load a data resource:\n\n1. There is some kind of network/resource access error, preventing the request for data from being issued\n2. A request is sent to a server, but the server is unable to service the request due to some error condition (often illegal access tokens or request parameters) and sends an error response.\n3. The server returns data, but the parser is unable to parse it (perhaps due to the data being malformatted, or formatted according to an unsupported version of that format).\n\nloaders.gl can detect all of these error conditions and report the resulting errors in a unified way (the errors will be available as exceptions or rejected promises depending on your async programming style, see below).\n\n### Error Messages\n\nloaders.gl aims to prodice concise, easy-to-understand error messages that can be presented directly to the end user.\n\nWhen the fetch call fails, the genereted exception is passed to the user, and the same is true when a loader fails. For server error responses, some basic information about the error is compiled into an error message (using e.g. `response.status`, `response.url` and occasionally `response.text`).\n\nNote that while servers often send some information about errors in `response.text()` when setting HTTP error codes, there are no universally adhered-to conventions for how servers format those error messages. The data is often a set of key-value pairs that are JSON or XML encoded, but even then the exact key names are usually server-specific.\n\nAt the moment loaders.gl does not provide any error formatting plugins, so if you know how your specific service formats errors and want to extract these in a way that you can present to the user, you may want to take control of the fetch `Response` status checking, see below.\n\n## parse Error Handling\n\n`parse` accepts fetch `Response` objects, and `parse` will check the status of the `Response` before attempting to parse, and generate an exception if appropriate.\n\n## Handling Errors from Async Functions\n\nNote that `parse` is an async function, and in JavaScript, errors generated by async functions will be reported either as an exception or as a rejected promise, depending on how the async funtion was called (using promises or the `await` keyword):\n\nWhen using `await`, errors are reported as exceptions\n\n```js\ntry {\n  const response = await fetch(url);\n  const data = await parse(response);\n} catch (error) {\n  console.log(error);\n}\n```\n\nA rejected promise is generated when using `Promise.then`.\n\n```js\nfetch(url)\n  .then(response => parse(response))\n  .catch(error => console.log(error));\n```\n\nAlso note that the Javascript runtime seamlessly converts errors between exceptions and promises in mixed code.\n\n## fetch Error Handling\n\nloaders.gl is designed around the use of the modern JavaScript `fetch` API, so for additional context, it may help to review of how the JavaSctipt `fetch` function handles errors.\n\n`fetch` separates between \"network errors\" that can be detected directly (these cause the `fetch` to throw an exception) and server side errors that are reported asynchronously with HTTP status codes (in this case the `Response` object offers accessors that must be called to check if the operation was successful before accessing data).\n\nExample: \"manually\" checking separately for fetch network errors and server errors:\n\n```js\n// Check for network error\nlet response;\ntry {\n  response = await fetch(url);\n} catch (error) {\n  console.log('Network error');\n}\n\n// Check for server error\nif (!response.ok) {\n  console.log(`fetch failed with status ${response.status}`);\n}\n```\n\nNote that servers often sends a message providing some detail about what went wrong, and that message can be accessed using the standard (asynchronous) `response.text()` or `response.json()` methods.\n\n```js\nif (!response.ok) {\n  const errorMessage = await response.text();\n  // Custom parsing can be done here, if you know how your particular service formats errors\n  console.log(`fetch failed with status ${errorMessage}`);\n}\n```\n","slug":"docs/developer-guide/error-handling","title":"Error Handling"},{"excerpt":"Using Writers Writers and the  functions are available for use, however they are considere experimental. They rae still in development, and…","rawMarkdownBody":"# Using Writers\n\n> Writers and the `encode` functions are available for use, however they are considere experimental. They rae still in development, and may still have issues.\n\nWriters allow applications to generate properly formatted data for a number of the formats supported by loaders.gl.\n\n> Not all formats have writers.\n\nFor a detailed specification of the writer object format see the [API reference](docs/specifications/writer-object-format.md).\n\n## Usage\n\nAs an example, to Draco-compress a mesh using the `DracoWriter`:\n\n```js\nimport {DracoWriter} from '@loaders.gl/draco';\nimport {encode} from '@loaders.gl/core';\n\nconst mesh = {\n  attributes: {\n    POSITION: {...}\n  }\n};\n\nconst data = await encode(mesh, DracoWriter, options);\n```\n\n## Input Data\n\n_Writers_ accept the same format of data that is produced by the corresponding loaders. This format is documented either in each loader or usually as part of the documentation for that loader category.\n\nIf applications have data in a different format, they will need to first transform the data to the format expected by the _writer_.\n","slug":"docs/developer-guide/using-writers","title":"Using Writers"},{"excerpt":"Using Worker Loaders By default, many loaders.gl loader modules do their parsing on JavaScript worker threads. This means that the main…","rawMarkdownBody":"## Using Worker Loaders\n\nBy default, many loaders.gl loader modules do their parsing on JavaScript worker threads. This means that the main thread will not block during parsing. Worker threads can also run in parallel, increasing your application's performance.\n\nFor more details on the advantages and complications with worker thread based loading the [Worker Threads](./using-worker-loaders.md) article in the concepts secion.\n\n## Loading Files in Parallel using Worker Loaders\n\nThe `DracoLoader` is worker enabled and uses worker threads by default. To load two Draco encoded meshes _in parallel_ on worker threads, just use the `DracoLoader` as follows:\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {DracoLoader} from '@loaders.gl/draco';\n\nasync function loadInParallel(url1, url2) {\n  const [data1, data2] = await Promise.all([load(url1, DracoLoader), load(url2, DracoLoader)]);\n}\n```\n\n## Disabling Worker Loaders\n\nApplications can use the `worker: false` option to disable worker loaders, for instance to simplify debugging of parsing issues:\n\n```js\nasync function loadwWithoutWorker(url1) {\n  const data = await load(url1, DracoLoader, {worker: false});\n}\n```\n\n## Concurrency Level\n\nConcurrency - The `options.maxConcurrency` parameter can be adjusted to define how many workers should be created for each format. Note that setting this higher than roughly the number CPU cores on your current machine will not provide much benefit and may create extra overhead.\n\n## ArrayBuffer Neutering\n\nBe aware that when calling worker loaders, binary data is transferred from the calling thread to the worker thread. This means that if you are using `parse`, any `ArrayBuffer` parameter you pass in to the will be \"neutered\" and no longer be accessible in the calling thread.\n\nMost applications will not need to do further processing on the raw binary data after it has been parsed so this is rarely an issue, but if you do, you may need to copy the data before parsing, or disable worker loading (see above).\n\n## Specifying Worker Script URLs (Advanced)\n\nIn JavaScript, worker threads are loaded from separate scripts files and are typically not part of the main application bundle. For ease-of-use, loaders.gl provides a default set of pre-built worker threads which are published on loaders.gl npm distribution from `unpck.com` CDN (Content Delivery Network).\n\nAs an advanced option, it is possible to for application to specify alternate URLs for loading a pre-built worker loader instance.\n\nThis can be useful e.g. when building applications that cannot access CDNs or when creating highly customized application builds, or doing in-depth debugging.\n\n## Composite Loaders and Workers (Advanced)\n\nloaders.gl supports sub-loader invocation from worker loaders. This is somewhat experimental\n\nA worker loader starts a seperate thread with a javascript bundle that only contains the code for that loader, so a worker loader needs to call the main thread (and indirectly, potentially another worker thread with another worrker loader) to parse using a sub-loader, properly transferring data into and back from the other thread.\n\n## Debugging Worker Loaders (Advanced)\n\nDebugging worker loaders is tricky. While it is always possible to specify `options.worker: false` which helps in many situations, there are cases where the worker loader itself must be debugged.\n\nTBA - There is an ambition to provide better support for debugging worker loaders:\n\n- Pre-build non-minified versions of workers, and provide option to easily select those.\n- Let loaders.gl developers easily switch between CDN and locally built workers.\n- ...\n","slug":"docs/developer-guide/using-worker-loaders","title":" Using Worker Loaders"},{"excerpt":"Using Loaders loaders.gl has parser functions that use so called \"loaders\" to convert the raw data loaded from files into parsed objects…","rawMarkdownBody":"# Using Loaders\n\nloaders.gl has parser functions that use so called \"loaders\" to convert the raw data loaded from files into parsed objects. Each loader encapsulates a parsing function for one file format (or a group of related file formats) together with some metadata (like the loader name, common file extensions for the format etc).\n\n## Installing loaders\n\nloaders.gl provides a suite of pre-built loader objects packaged as scoped npm modules. The intention is that applications will install and import loaders only for the formats they need.\n\n## Using Loaders\n\nLoaders are passed into utility functions in the loaders.gl core API to enable parsing of the chosen format.\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\n\ndata = await load(url, CSVLoader);\n// Application code here\n...\n```\n\n## Specifying and Registering Loaders\n\nAs seen above can be specified directly in a call to `load` or any of the `parse` functions:\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {PCDLoader} from '@loaders.gl/pcd';\nimport {LASLoader} from '@loaders.gl/las';\n\nconst pointCloud = await load(url, [PCDLoader, LASLoader]);\n\n// Application code here\n...\n```\n\nLoaders can also be registered globally. To register a loader, use `registerLoaders`:\n\n```js\nimport {registerLoaders, load} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nregisterLoaders([CSVLoader]);\n\ndata = await load('url.csv'); // => CSVLoader selected from pre-registered loaders\n```\n\n## Selecting Loadera\n\nThe loader selection algorithm is exposed to applications via `selectLoader`:\n\n```js\nimport {selectLoader} from '@loaders.gl/core';\nimport {ArrowLoader} from '@loaders.gl/arrow';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nselectLoader([ArrowLoader, CSVLoader], 'filename.csv'); // => CSVLoader\n```\n\nNote: Selection works on urls and/or data\n\n## Loader Options\n\n`load`, `parse` and other core functions accept loader options in the form of an options object.\n\n```js\nparse(data, Loader, {...options});\n```\n\nSuch loader options objects are organized into nested sub objects, with one sub-object per loader or loader category. This provides a structured way to pass options to multiple loaders.\n\n```js\nload(url, {\n  json: {...},\n  csv: {...},\n  '3d-tiles': {...},\n  gltf: {...}\n});\n```\n\nAn advantage of this design is that since the core functions can select a loader from a list of multiple candidate loaders, or invoke sub-loaders, the nested options system allows separate specification of options to each loader in a single options object.\n\nLoader options are merged with default options using a deep, two-level merge. Any object-valued key on the top level will be merged with the corresponding key value in the default options object.\n\n## Using Composite Loaders\n\nloaders.gl enables the creation of _composite loaders_ that call other loaders (referred to as \"sub-loaders\" in this section). This enables loaders for \"composite formats\" to be quickly composed out of loaders for the primitive parts.\n\nComposite Loader usage is designed to be conceptually simple for applications (loaders.gl handles a number of subtleties under the hood).\n\nA composite loader is called just like any other loader, however there are some additional\n\n### Parameter Passing between Loaders\n\nLoaders and parameters are passed through to sub loaders and are merged so that applications can override them:\n\n```js\n  parse(data, [Tile3DLoader, GLTFLoader, DracoLoader], {\n    '3d-tiles': {\n      ...\n    },\n    gltf: {\n      ...\n    }\n  });\n```\n\nIn this example:\n\n- the passed in loaders would override any loaders specified inside the sub-loaders as well as any globally registered loaders.\n- The options will be passed through to the sub-loaders, so that the `GLTFLoader` will receive the `gltf` options, merged with any `gltf` options set by the `Tile3DLoader`.\n\nThis override system makes it easy for applications to test alternate sub-loaders or parameter options without having to modify any existing loader code.\n","slug":"docs/developer-guide/using-loaders","title":"Using Loaders"},{"excerpt":"AsyncIterators Streaming functionality in loaders.gl is built on the ES2018  concept. This page gives some background on AsyncIterator since…","rawMarkdownBody":"# AsyncIterators\n\nStreaming functionality in loaders.gl is built on the ES2018 `AsyncIterator` concept. This page gives some background on AsyncIterator since it is a recently introduced concept (at least as part of the JavaScript standard).\n\n## Availability\n\n`AsyncIterator` is a standard JavaScript ES2018 feature and is well supported by recent evergreen browsers and Node.js versions.\n\nThe `for await of` iteration syntax is supported as well as the babel transpiler.\n\n## Batched Parsing and Endcoding using AsyncIterators\n\nThe input and output from streaming loaders and writers can both be expressed in terms of async iterators.\n\n## Using AsyncIterator\n\nRemember tyhat an async iterator can be consumed (iterated over) via the for-await construct:\n\n```js\nfor await (const x of asyncIterable) {}\n```\n\n## Using Streams as AsyncIterators\n\nWith a little effort, streams in JavaScript can be treated as AsyncIterators. As the section about [Javascript Streams](docs/developer-guide/streams.md) explains, instead of registering callbacks on the stream, you can now work with streams in this way:\n\n```js\nfor await (const buf of fs.createReadStream('foo.txt')) {\n  // do something\n}\n```\n\n## Creating AsyncIterators\n\nRemember that any object in JavaScript that implements the `[Symbol.asyncIterator]()` method is an `AsyncIterable`. And the async generator syntax can be used to generate new async iterators\n\n```js\nasync function* asyncIterator() {\n  yield new Promise(...)\n}\n\nfor await (const x of asyncIterator()) {} // Notice parens after 'asyncIterator'\n```\n","slug":"docs/developer-guide/concepts/async-iterators","title":"AsyncIterators"},{"excerpt":"Binary Data The loaders.gl API consistently uses s to represent and transport binary data. Why ArrayBuffers? One of the design goals of…","rawMarkdownBody":"# Binary Data\n\nThe loaders.gl API consistently uses `ArrayBuffer`s to represent and transport binary data.\n\n## Why ArrayBuffers?\n\nOne of the design goals of loaders.gl is to provide applications with a single, consistent API that works across (reasonably modern) browsers, worker threads and Node.js. One of the characteristics of this API is how binary data is represented.\n\nloaders.gl \"standardizes\" on ArrayBuffers for a number of reasons:\n\n- ArrayBuffers are the \"canonical\" input format for the WebGL API, allowing efficient uploads of large binary data sets to the GPU.\n- ArrayBuffers allow ownership to be transferred between threads (Browser Main Thread and WebWorkers), massively improving performance when sending data back from loaders running on web worker to the application/main thread.\n- ArrayBuffers are used to transport raw data in most newer JavaScript APIs, including WebSockets, Web Intents, XMLHttpRequest version 2 etc.\n- ArrayBuffers are well supported by recent Node.js versions, in fact the traditional Node.js `Buffer` class is now backed by an `ArrayBuffer`.\n\n## ArrayBuffers and Typed Arrays\n\nRecall that typed arrays (e.g. `Float32Array`) are just views into array buffers. Every typed array has a `buffer` reference.\n\nMany loaders.gl functions directly accept typed arrays, which essentially means they accept the associated ArrayBuffer. However, be aware that typed arrays can represent partial views (i.e. they can have offsets) that sometimes need special handling in the application.\n\n## Converting between ArrayBuffers and Strings\n\nWe use the `TextEncoder` and `TextDecoder` classes in the JavaScript [string encoding/decoding library](https://github.com/inexorabletash/text-encoding).\n\nSince these classes are central to using ArrayBuffers correctly, loaders.gl provides polyfills for them under Node.js.\n\n## Binary Types in JavaScript\n\nBinary data types in JS:\n\n- `ArrayBuffer`\n- `Uint8Array` and other typed arrays, plus\n- `DataView`\n- `Blob`\n- `Buffer` nodejs\n\nExamples of \"semi-binary\" data types in JS:\n\n- `Array`: Array of bytes (elements are numbers between 0 and 255).\n- `String` (binary): string in “binary” form, 1 byte per char (2 bytes).\n- `String` (base64): string containing the binary data encoded in a base64 form.\n\n## Converting between ArrayBuffers and other Binary Formats.\n\nStandardizing on ArrayBuffers helps streamline the loaders.gl API. But occasionally applications need to interface with APIs that accept other binary data types/formats. To support this case, loaders.gl provides a small set of utilities (non-exhaustive) for converting from and to other binary JavaScript types/formats, e.g. `toArrayBuffer`:\n","slug":"docs/developer-guide/concepts/binary-data","title":"Binary Data"},{"excerpt":"Category: Mesh/PointCloud This category unifies the loader output for simple mesh and point clouds formats that describe a \"single geometry…","rawMarkdownBody":"# Category: Mesh/PointCloud\n\nThis category unifies the loader output for simple mesh and point clouds formats that describe a \"single geometry primitive\" (as opposed to e.g. a scenegraph consisting of multiple geometries).\n\nA single mesh is typically defined by a set of attributes, such as `positions`, `colors`, `normals` etc, as well as a draw mode.\n\n## Format Notes\n\nThe Pointcloud/Mesh loaders output mesh data in a common form that is optimized for use in WebGL frameworks:\n\n- All attributes (and indices if present) are stored as typed arrays of the proper type.\n- All attributes (and indices if present) are wrapped into glTF-style \"accessor objects\", e.g. `{size: 1-4, value: typedArray}`.\n- Attribute names are mapped to glTF attribute names (on a best-effort basis).\n- An `indices` field is added (only if present in the loaded geometry).\n- A primitive drawing `mode` value is added (the numeric value matches WebGL constants, e.g `GL.TRIANGLES`).\n\n## Data Structure\n\n| Field        | Type                | Contents                                                                                                    |\n| ------------ | ------------------- | ----------------------------------------------------------------------------------------------------------- |\n| `loaderData` | `Object` (Optional) | Loader and format specific data                                                                             |\n| `header`     | `Object`            | See [Header](#header)                                                                                       |\n| `mode`       | `Number`            | See [Mode](#mode)                                                                                           |\n| `attributes` | `Object`            | Keys are [glTF attribute names](#gltf-attribute-name-mapping) and values are [accessor](#accessor) objects. |\n| `indices`    | `Object` (Optional) | If present, describes the indices (elements) of the geometry as an [accessor](#accessor) object.            |\n\n### Header\n\nThe `header` fields are only recommended at this point, applications can not assume they will be present:\n\n| `header` Field | Type     | Contents |\n| -------------- | -------- | -------- |\n| `vertexCount`  | `Number` |          |\n\n### Mode\n\nPrimitive modes are aligned with [OpenGL/glTF primitive types](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#primitive)\n\n| Value | Primitive Mode   | Comment                                                                                              |\n| ----- | ---------------- | ---------------------------------------------------------------------------------------------------- |\n| `0`   | `POINTS`         | Used for point cloud category data                                                                   |\n| `1`   | `LINES`          | Lines are rarely used due to limitations in GPU-based rendering                                      |\n| `2`   | `LINE_LOOP`      | -                                                                                                    |\n| `3`   | `LINE_STRIP`     | -                                                                                                    |\n| `4`   | `TRIANGLES`      | Used for most meshes. Indices attributes are often used to reuse vertex data in remaining attributes |\n| `5`   | `TRIANGLE_STRIP` | -                                                                                                    |\n| `6`   | `TRIANGLE_FAN`   | -                                                                                                    |\n\n### Accessor\n\n`attributes` and `indices` are represented by glTF \"accessor objects\" with the binary data for that attribute resolved into a typed array of the proper type.\n\n| Accessors Fields | glTF? | Type                | Contents                                                                                                                                           |\n| ---------------- | ----- | ------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `value`          | No    | `TypedArray`        | Contains the typed array (corresponds to `bufferView`). The type of the array will match the GL constant in `componentType`.                       |\n| `size`           | No    | `Number`            | Number of components, `1`-`4`.                                                                                                                     |\n| `byteOffset`     | Yes   | `Number`            | Starting offset into the bufferView.                                                                                                               |\n| `count`          | Yes   | `Number`            | The number of elements/vertices in the attribute data.                                                                                             |\n| `originalName`   | No    | `String` (Optional) | If this was a named attribute in the original file, the original name (before substitution with glTF attribute names) will be made available here. |\n\n### glTF Attribute Name Mapping\n\nTo help applications manage attribute name differences between various formats, mesh loaders map known attribute names to [glTF 2.0 standard attribute names](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#geometry) a best-effort basis.\n\nWhen a loader can map an attribute name, it will replace ir with the glTF equivalent. This allows applications to use common code to handle meshes and point clouds from different formats.\n\n| Name         | Accessor Type(s)   | Component Type(s)                                                                     | Description                                                                                                        |\n| ------------ | ------------------ | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n| `POSITION`   | `\"VEC3\"`           | `5126` (FLOAT)                                                                        | XYZ vertex positions                                                                                               |\n| `NORMAL`     | `\"VEC3\"`           | `5126` (FLOAT)                                                                        | Normalized XYZ vertex normals                                                                                      |\n| `TANGENT`    | `\"VEC4\"`           | `5126` (FLOAT)                                                                        | XYZW vertex tangents where the _w_ component is a sign value (-1 or +1) indicating handedness of the tangent basis |\n| `TEXCOORD_0` | `\"VEC2\"`           | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized | UV texture coordinates for the first set                                                                           |\n| `TEXCOORD_1` | `\"VEC2\"`           | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized | UV texture coordinates for the second set                                                                          |\n| `COLOR_0`    | `\"VEC3\"`, `\"VEC4\"` | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized | RGB or RGBA vertex color                                                                                           |\n| `JOINTS_0`   | `\"VEC4\"`           | `5121` (UNSIGNED_BYTE), `5123` (UNSIGNED_SHORT)                                       |                                                                                                                    |\n| `WEIGHTS_0`  | `\"VEC4\"`           | `5126` (FLOAT), `5121` (UNSIGNED_BYTE) normalized, `5123` (UNSIGNED_SHORT) normalized |                                                                                                                    |\n\n> Note that for efficiency reasons, mesh loaders are not required to convert the format of an attribute's binary data to match the glTF specifications (i.e. if normals were encoded using BYTES then that is what will be returned even though glTF calls out for FLOAT32). Any such alignment needs to be done by the application as a second step.\n\n## Limitations\n\n### Scenegraph support\n\nFor more complex, scenegraph-type formats (i.e. formats that don't just contain single geometric primitives), loaders.gl currently focuses on glTF 2.0 support.\n\nIt is assumed that other scenegraph-type format loaders (e.g. a hyptothetical COLLADA loader) could convert their loaded data to a similar structure, essentially converting to glTF 2.0 on-the-fly as they load.\n\nFor now it is best to convert such assets off-line to glTF before attempting to loade them with loaders.gl.\n\n### Material support\n\nMaterial support is provided by some mesh formats (e.g. OBJ/MTL) and is currently not implemented by loaders.gl, however the glTF loader has full support for PBR (Physically-Based Rendering) materials.\n\n## Loaders\n\n- [LASLoader](/docs/api-reference/las/las-loader)\n- [OBJLoader](/docs/api-reference/obj/obj-loader)\n- [PCDLoader](/docs/api-reference/pcd/pcd-loader)\n- [PLYLoader](/docs/api-reference/ply/ply-loader)\n","slug":"docs/specifications/category-mesh","title":"Category: Mesh/PointCloud"},{"excerpt":"Streaming Streaming support in loaders.gl is a work-in-progress. The ambition is that many loaders would support streaming from both Node…","rawMarkdownBody":"# Streaming\n\n> Streaming support in loaders.gl is a work-in-progress. The ambition is that many loaders would support streaming from both Node and DOM streams, through a consistent API and set of conventions (for both applications and loader/writer objects).\n\n## Streaming Loads\n\n### Incremental Parsing\n\nSome loaders offer incremental parsing (chunks of incomplete data can be parsed, and updates will be sent after a certain batch size has been exceeded). In many cases, parsing is fast compared to loading of data, so incremental parsing on its own may not provide a lot of value for applications.\n\n### Incremental Loading\n\nIncremental parsing becomes more interesting when it can be powered by incremental loading, whether through request updates or streams (see below).\n\n### Streamed Loading\n\nStreamed loading means that the entire data does not need to be loaded.\n\nThis is particularly advantageous when:\n\n- loading files with sizes that exceed browser limits (e.g. 1GB in Chrome)\n- doing local processing to files (tranforming one row at a time), this allows pipe constructions that can process files that far exceed internal memory.\n\n## Batched Updates\n\nFor incemental loading and parsing to be really effective, the application needs to be able to deal efficiently with partial batches as they arrive. Each loader category (or loader) may define a batch update conventions that are appropriate for the format being loaded.\n\n## Streaming Writes\n\nTBA\n\n## Node Streams vs DOM Streams\n\nStream support is finally arriving in browsers, however DOM Streams have a slightly different API than Node streams and the support across browsers is still spotty.\n\n## Polyfills\n\nStream support across browsers can be somewhat improved with polyfills. TBA\n\n## Stream Utilities\n\n- Stream to memory, ...\n- Automatically create stream if loader/writer only supports streaming\n- ...\n","slug":"docs/developer-guide/concepts/streaming","title":"Streaming"},{"excerpt":"Worker Threads On modern browsers, many loaders.gl loaders are set up to run on JavaScript worker threads. (Refer the documentation of each…","rawMarkdownBody":"# Worker Threads\n\nOn modern browsers, many loaders.gl loaders are set up to run on JavaScript worker threads. (Refer the documentation of each loader to see if it supports worker thread loading).\n\nLoading and parsing of data on worker threads can bring significant advantages\n\n- **Avoid blocking the browser main thread** - when parsing longer files, the main thread can become blocked, effectively \"freezing\" the application's user interface until parsing completes.\n- **Parallel parsing on multi-core CPUs** - when parsing multiple files on machines that have multiple cores (essentially all machines, even modern mobile phones tend to have at least two cores), worker threads enables multiple files to be parsed in parallel which can dramatically reduce the total load times.\n\nHoever, there are a number of considerations when loading and parsing data on JavaScript worker threads:\n\n- **Serialization/deserializion overhead** when transferring resuls back to main thread can more than defeat gains from loading on a separate thread.\n- **Choice of Data Types** - Due to data transfer issues there are constraints on what data types are appropriate\n- **Build configuration** - Workers can require complex build system setup/configuration.\n- **Message Passing** - Parsing on workers requires message passing between threads. While simple it can add clutter to application code.\n- **Debugging** - Worker based code tends to be harder to debug. Being able to easily switch back to main thread parsing (or an alternate worker build) can be very helpful.\n- **Startup Times** - Worker startup times can defeat speed gains from parsing on workers.\n\n## Data Transfer\n\nThreads cannot share non-binary data structures and these have to be serialized/deserialized. This is a big issue for worker thread based loading as the purpose of loaders is typically to load and parse big datastructures, and main thread deserialization times are often comparable to or even exceed the time required to parse the data in the first place, defeating the value of moving parsing to a worker thread.\n\nThe solution is usually to use data types that support ownership transfer (see next section) as much as possible and minimize the amount of non-binary data returned from the parser.\n\n## Data Types\n\nJavaScript ArrayBuffers and Typed Arrays can be passed with minimal overhead (ownership transfer) and the value of worker based parsing usually depends on whether the loaded data can (mostly) be stored in these types.\n\n## Message Passing\n\nloaders.gl will handle message passing behind the scenes. Loading on a worker thread returns a promise that completes when the worker is done and the data has been transferred back to the main thread.\n\n## Build Configuration\n\nAll worker enabled loaders come with a pre-built, minimal worker \"executable\" to enable zero-configuration use in applications.\n\n## Bundle size concerns\n\nAll worker enabled loaders provide separate loader objects to ensure that tree-shaking bundlers will be able to remove the code for the unused case.\n\n## Debugging and Benchmarking\n\nLoaders.gl offers loader objects for main thread and worker threads. A simple switch lets you move your loading back to the main thread for easier debugging and benchmarking (comparing speeds to ensure you are gaining the benefits you expect from worker thread based loading).\n","slug":"docs/developer-guide/concepts/worker-threads","title":"Worker Threads"},{"excerpt":"","rawMarkdownBody":"","slug":"modules/i3s/docs","title":""},{"excerpt":"@loaders.gl/zip loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loaders and…","rawMarkdownBody":"# @loaders.gl/zip\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loaders and writers for the Zip Archive format.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/zip","title":"@loaders.gl/zip"},{"excerpt":"@loaders.gl/tables This module contains: Classes and APIs for manipulating tabular data output from loaders.gl table category loaders (CSV…","rawMarkdownBody":"# @loaders.gl/tables\n\nThis module contains:\n\n- Classes and APIs for manipulating tabular data output from loaders.gl table category loaders (CSV, JSON, ...).\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nPlease visit the [website](https://loaders.gl).\n","slug":"modules/tables","title":"@loaders.gl/tables"},{"excerpt":"@loaders.gl/potree (Experimental) This module contains loaders for the potree format. loaders.gl is a collection of framework independent 3D…","rawMarkdownBody":"# @loaders.gl/potree (Experimental)\n\nThis module contains loaders for the [potree](https://github.com/potree/potree) format.\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial loaders (parsers).\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/potree","title":"@loaders.gl/potree (Experimental)"},{"excerpt":"@loaders.gl/polyfills loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains…","rawMarkdownBody":"# @loaders.gl/polyfills\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains polyfills for running on older browsers (mainly Edge and IE11) as well as Node.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/polyfills","title":"@loaders.gl/polyfills"},{"excerpt":"@loaders.gl/tables Table Table APIs The table API is modelled after a subset of the Apache Arrow API: Class Arrow Counterpart Description…","rawMarkdownBody":"# @loaders.gl/tables\n\n> Table\n\n## Table APIs\n\nThe table API is modelled after a subset of the Apache Arrow API:\n\n| Class                                                              | Arrow Counterpart | Description  |\n| ------------------------------------------------------------------ | ----------------- | ------------ |\n| [`Table`](modules/tables/docs/api-reference/table.md)              | Table             | Table        |\n| [`TableSchema`](modules/tables/docs/api-reference/table-schema.md) | `Schema`          | Table schema |\n| [`TableBatch`](modules/tables/docs/api-reference/table-batch.md)   | `RecordBatch`     | Table batch  |\n\n## Micro-Loaders\n\nLoaders with limited functionality but with minimal bundle size impact:\n\n| Loader       | Description                                                                           |\n| ------------ | ------------------------------------------------------------------------------------- |\n| `JSONLoader` | A minimal non-streaming JSON loader that uses the built-in `JSON.parse` function      |\n| `XMLLoader`  | A non-streaming, browser-only XML loader that uses the browser's built-in DOM parser. |\n","slug":"modules/tables/docs","title":"@loaders.gl/tables"},{"excerpt":"Overview The  module handles compressing and decompressing of the ZIP format. Installation Attributions ZipLoader is a wrapper around the…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/zip` module handles compressing and decompressing of the [ZIP](<https://en.wikipedia.org/wiki/Zip_(file_format)>) format.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/zip\n```\n\n## Attributions\n\nZipLoader is a wrapper around the [JSZip module](https://stuk.github.io/jszip/). JSZip has extensive documentation on options (and more functionality than this loader object can expose).\n","slug":"modules/zip/docs/api-reference","title":"Overview"},{"excerpt":"Working with Tables The loaders.gl table category provides support for working interchangably with row-oriented and columnar tables.","rawMarkdownBody":"# Working with Tables\n\nThe loaders.gl table category provides support for working interchangably with row-oriented and columnar tables.\n","slug":"modules/tables/docs/table-guide","title":"Working with Tables"},{"excerpt":"@loaders.gl/ply loaders.gl is a collection of loaders for big data visualizations. This module contains loaders for the PLY format. For…","rawMarkdownBody":"# @loaders.gl/ply\n\n[loaders.gl](https://loaders.gl/docs) is a collection of loaders for big data visualizations.\n\nThis module contains loaders for the PLY format.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/ply","title":"@loaders.gl/ply"},{"excerpt":"@loaders.gl/obj loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loader for…","rawMarkdownBody":"# @loaders.gl/obj\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loader for the OBJ format.\n\nFor documentation please visit the [website](https://loaders.gl).\n\nNote: The OBJ parser in this module is a port of [three.js](https://github.com/mrdoob/three.js)'s OBJLoader.\n\nThe MIT License\n\nCopyright © 2010-2019 three.js authors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n","slug":"modules/obj","title":"@loaders.gl/obj"},{"excerpt":"@loaders.gl/pcd loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loaders for…","rawMarkdownBody":"# @loaders.gl/pcd\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loaders for the PCD format.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/pcd","title":"@loaders.gl/pcd"},{"excerpt":"ZipWriter Encodes a filemap into a Zip Archive. Returns an  that is a valid Zip Archive and can be written to file. Loader Characteristic…","rawMarkdownBody":"# ZipWriter\n\nEncodes a filemap into a Zip Archive. Returns an `ArrayBuffer` that is a valid Zip Archive and can be written to file.\n\n| Loader         | Characteristic                                                   |\n| -------------- | ---------------------------------------------------------------- |\n| File Extension | `.zip`                                                           |\n| File Type      | Binary                                                           |\n| Data Format    | \"File Map\"                                                       |\n| File Format    | [ZIP Archive](<https://en.wikipedia.org/wiki/Zip_(file_format)>) |\n| Encoder Type   | Asynchronous                                                     |\n| Worker Thread  | No                                                               |\n| Streaming      | No                                                               |\n\n## Usage\n\n```js\nimport {encode, writeFile} from '@loaders.gl/core';\nimport {ZipWriter} from '@loaders.gl/zip';\n\nconst FILEMAP = {\n  filename1: arrayBuffer1,\n  'directory/filename2': ...\n};\n\nconst arrayBuffer = await encode(FILE_MAP, ZipWriter)\nwriteFile(zipFileName, arrayBuffer);\n```\n\n## File Format\n\nThe file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either `ArrayBuffer` or `String`).\n\n## Options\n\nOptions are forwarded to [JSZip.generateAsync](https://stuk.github.io/jszip/documentation/api_jszip/generate_async.html), however type is always set to `arraybuffer` to ensure compatibility with writer driver functions in `@loaders.gl/core`.\n","slug":"modules/zip/docs/api-reference/zip-writer","title":"ZipWriter"},{"excerpt":"ZipLoader Decodes a Zip Archive into a file map. Loader Characteristic File Extension  File Type Binary File Format ZIP Archive Data Format…","rawMarkdownBody":"# ZipLoader\n\nDecodes a Zip Archive into a file map.\n\n| Loader         | Characteristic                                                   |\n| -------------- | ---------------------------------------------------------------- |\n| File Extension | `.zip`                                                           |\n| File Type      | Binary                                                           |\n| File Format    | [ZIP Archive](<https://en.wikipedia.org/wiki/Zip_(file_format)>) |\n| Data Format    | \"File Map\"                                                       |\n| Decoder Type   | Asynchronous                                                     |\n| Worker Thread  | No                                                               |\n| Streaming      | No                                                               |\n\n## Usage\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {ZipLoader} from '@loaders.gl/zip';\n\nconst fileMap = await parse(arrayBuffer, ZipLoader);\nfor (const fileName in FILE_MAP) {\n  const fileData = fileMap[key];\n  // Do something with the subfile\n}\n```\n\n## Data Format\n\nThe file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either `ArrayBuffer` or `String`).\n\n## Options\n\nOptions are forwarded to [JSZip.loadAsync](https://stuk.github.io/jszip/documentation/api_jszip/load_async.html).\n","slug":"modules/zip/docs/api-reference/zip-loader","title":"ZipLoader"},{"excerpt":"@loaders.gl/potree The potree loaders are still under development and are not yet considered ready for use. Support for loading and…","rawMarkdownBody":"# @loaders.gl/potree\n\n> The potree loaders are still under development and are not yet considered ready for use.\n\nSupport for loading and traversing [potree](http://potree.org/) format point clouds.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/potree\nnpm install @loaders.gl/core\n```\n\n## Usage\n\n> Intended usage only, not yet working!\n\n```\nimport {load} from `@loaders.gl/core`;\nimport {PotreeLoader} from `@loaders.gl/potree`;\nimport {Tileset3D} from `@loaders.gl/category-3d-tiles`;\n\nconst potree = await load(POTREE_URL);\nconst tileset = new Tileset3D(potree);\nconst tilesToRender = tileset.traverse(frameData);\n```\n\n## API\n\nThis modules provides the following exports:\n\n- `PotreeHierarchyChunkLoader` for the hierarchy indices\n\n## Roadmap\n\nThe plan is to provide the following loaders/writers:\n\n- `PotreeLoader` for individual tiles\n\n`PotreeLoader` is intended to work with the 3d tileset classes in the `@loaders.gl/3d-tiles` module.\n\n- `Tileset3D` class will be generalized to accept loaded potree tilesets.\n\n## Attribution\n\nThe `PotreeLoader` is a fork of Markus Schuetz' potree code (https://github.com/potree/potree) under BSD-2 clause license.\n","slug":"modules/potree/docs","title":"@loaders.gl/potree"},{"excerpt":"TableBatch RowTableBatch ColumnarTableBatch ArrowTableBatch","rawMarkdownBody":"# TableBatch\n\n- RowTableBatch\n- ColumnarTableBatch\n- ArrowTableBatch\n","slug":"modules/tables/docs/api-reference/table-batch","title":"TableBatch"},{"excerpt":"@loaders.gl/las loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loaders and…","rawMarkdownBody":"# @loaders.gl/las\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loaders and writers for the LAS and LAZ formats.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/las","title":"@loaders.gl/las"},{"excerpt":"TableSchema","rawMarkdownBody":"# TableSchema\n","slug":"modules/tables/docs/api-reference/table-schema","title":"TableSchema"},{"excerpt":"@loaders.gl/kml loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loaders for…","rawMarkdownBody":"# @loaders.gl/kml\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loaders for the KML format.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/kml","title":"@loaders.gl/kml"},{"excerpt":"@loaders.gl/images loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loader…","rawMarkdownBody":"# @loaders.gl/images\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loader and writers for images that follow loaders.gl conventions and work under both node and browser.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/images","title":"@loaders.gl/images"},{"excerpt":"@loaders.gl/json This module contains a table loader for the JSON and line delimited JSON formats. loaders.gl is a collection of framework…","rawMarkdownBody":"# @loaders.gl/json\n\nThis module contains a table loader for the JSON and line delimited JSON formats.\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent visualization-focused loaders (parsers).\n","slug":"modules/json","title":"@loaders.gl/json"},{"excerpt":"@loaders.gl/i3s (Experimental) This module contains a loader for i3s (Indexed SceneLayers). loaders.gl is a collection of loaders for big…","rawMarkdownBody":"# @loaders.gl/i3s (Experimental)\n\nThis module contains a loader for [i3s](https://github.com/Esri/i3s-spec) (Indexed SceneLayers).\n\n[loaders.gl](https://loaders.gl/docs) is a collection of loaders for big data visualizations.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/i3s","title":"@loaders.gl/i3s (Experimental)"},{"excerpt":"Table","rawMarkdownBody":"# Table\n","slug":"modules/tables/docs/api-reference/table","title":"Table"},{"excerpt":"@loaders.gl/gltf loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loader and…","rawMarkdownBody":"# @loaders.gl/gltf\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loader and writers for the glTF format.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/gltf","title":"@loaders.gl/gltf"},{"excerpt":"@loaders.gl/core This module contains shared utilities for loaders.gl, a collection of framework independent 3D and geospatial loaders…","rawMarkdownBody":"# @loaders.gl/core\n\nThis module contains shared utilities for loaders.gl, a collection of framework independent 3D and geospatial loaders (parsers).\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/loader-utils","title":"@loaders.gl/core"},{"excerpt":"@loaders.gl/draco loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loader…","rawMarkdownBody":"# @loaders.gl/draco\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loader and writer for Draco compressed meshes and point clouds.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/draco","title":"@loaders.gl/draco"},{"excerpt":"Overview The  module handles the the Polygon file format, or the Stanford Trangle Format, a file format for 3D graphical objects described…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/ply` module handles the the [Polygon file format](http://paulbourke.net/dataformats/ply/), or the Stanford Trangle Format, a file format for 3D graphical objects described as a collection of polygons.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/ply\n```\n\n## Attribution\n\nPLYLoader is a fork of the THREE.js PLYLoader under MIT License. The THREE.js source files contained the following attributions:\n\n@author Wei Meng / http://about.me/menway\n","slug":"modules/ply/docs","title":"Overview"},{"excerpt":"Overview The  provides optional support for Node.js and older browsers. Older browsers (mainly Edge and IE11) as well as versions of Node.js…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/polyfills` provides optional support for Node.js and older browsers.\n\nOlder browsers (mainly Edge and IE11) as well as versions of Node.js prior to v11 do not provide certain classes that loaders.gl depends on.\n\nWhile there are many good polyfill modules available on `npm`, to make the search for a version that works perfectly with loaders.gl a little easier, a polyfill module is included.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/polyfills\n```\n\n## Usage\n\nJust import `@loaders.gl/polyfills` before you start using other loaders.gl modules.\n\n```js\nimport '@loaders.gl/polyfills';\nimport '@loaders.gl/core';\n```\n\n## Included Polyfills\n\n| Polyfill                    | Node         | Browser              | Comments                                                                                                                                                                                    |\n| --------------------------- | ------------ | -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `TextEncoder`/`TextDecoder` | Node.js < 11 | Yes (Older browsers) | Only UTF8 is guaranteed to be supported                                                                                                                                                     |\n| `atob`/`btoa`               | All versions | No                   | Note: these functions are [not unicode safe](https://developer.mozilla.org/en-US/docs/Web/API/WindowBase64/Base64_encoding_and_decoding#The_Unicode_Problem), but OK to use for test cases. |\n| `fetch`                     | All versions | No                   | A subset of the fetch API is supported, see below.                                                                                                                                          |\n\n## fetch Polyfill\n\nThe Node.js `fetch` polyfill supports a subset of the browser fetch API, including:\n\n- `Response.text()`, `Response.arrayBuffer()`.\n- `Response.body` stream\n- limited support for `headers`\n- data uri / base64 decoding\n\n# TextEncoder and TextDecoder Polyfills\n\n`TextEncoder` and `TextDecoder` polyfills are provided to ensure these APIs are always available. In modern browsers these will evaluate to the built-in objects of the same name, however under Node.js polyfills are transparently installed.\n\nNote: The provided polyfills only guarantee UTF8 support.\n\n## Remarks\n\n- Applications should only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, importing this module will increase the application's bundle size.\n- Refer to browser documentation for the usage of these classes, e.g. MDN.\n- In the browser, overhead of using these imports is very low, as most polyfills are only bundled under Node.js.\n- If working under older browsers, e.g. IE11, you may need to install your own TextEncoder/TextDecoder polyfills before loading this library\n\n## Attribution\n\nThe `Header` polyfill (for Node.js `fetch`) is a fork of the implementation in https://github.com/github/fetch (MIT license).\n","slug":"modules/polyfills/docs/api-reference","title":"Overview"},{"excerpt":"@loaders.gl/core This module contains shared utilities for loaders.gl, a collection of framework independent 3D and geospatial loaders…","rawMarkdownBody":"# @loaders.gl/core\n\nThis module contains shared utilities for loaders.gl, a collection of framework independent 3D and geospatial loaders (parsers).\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/core","title":"@loaders.gl/core"},{"excerpt":"@loaders.gl/csv This module contains a table loader for the CSV and DSV formats. loaders.gl is a collection of framework independent…","rawMarkdownBody":"# @loaders.gl/csv\n\nThis module contains a table loader for the CSV and DSV formats.\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent visualization-focused loaders (parsers).\n","slug":"modules/csv","title":"@loaders.gl/csv"},{"excerpt":"Overview The  module handles the the Wavefront OBJ format, a simple ASCII format that defines 3D geometries as vertices, normals and faces…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/obj` module handles the the [Wavefront OBJ format](https://en.wikipedia.org/wiki/Wavefront_.obj_file), a simple ASCII format that defines 3D geometries as vertices, normals and faces.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/obj\nnpm install @loaders.gl/core\n```\n\n## Attribution\n\nOBJLoader is a port of [three.js](https://github.com/mrdoob/three.js)'s OBJLoader under MIT License.\n","slug":"modules/obj/docs","title":"Overview"},{"excerpt":"@loaders.gl/basis loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This module contains loader…","rawMarkdownBody":"# @loaders.gl/basis\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains loader for [basis universal textures](https://github.com/BinomialLLC/basis_universal).\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/basis","title":"@loaders.gl/basis"},{"excerpt":"@loaders.gl/arrow This module contains a table loader for the Apache Arrow format. loaders.gl is a collection of loaders for big data…","rawMarkdownBody":"# @loaders.gl/arrow\n\nThis module contains a table loader for the Apache Arrow format.\n\n[loaders.gl](https://loaders.gl/docs) is a collection of loaders for big data visualizations.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/arrow","title":"@loaders.gl/arrow"},{"excerpt":"@loaders.gl/3d-tiles (Experimental) This module contains a loader for 3D tiles. loaders.gl is a collection of loaders for big data…","rawMarkdownBody":"# @loaders.gl/3d-tiles (Experimental)\n\nThis module contains a loader for [3D tiles](https://github.com/AnalyticalGraphicsInc/3d-tiles).\n\n[loaders.gl](https://loaders.gl/docs) is a collection of loaders for big data visualizations.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/3d-tiles","title":"@loaders.gl/3d-tiles (Experimental)"},{"excerpt":"@loaders.gl/math (Experimental, Temporary) loaders.gl is a collection of framework independent 3D and geospatial parsers and encoders. This…","rawMarkdownBody":"# @loaders.gl/math (Experimental, Temporary)\n\n[loaders.gl](https://loaders.gl/docs) is a collection of framework independent 3D and geospatial parsers and encoders.\n\nThis module contains math utilities for the `@loaders.gl3d-tiles` module. As they mature, these will likely be moved to a math framework (e.g. math.gl).\n\nThis code is a fork of a subset of the Cesium math library whcih is Apache 2 licensed.\n\nFor documentation please visit the [website](https://loaders.gl).\n","slug":"modules/math","title":"@loaders.gl/math (Experimental, Temporary)"},{"excerpt":"Overview The  library is being developed to support 3D tiles and will be moved to the math.gl repository when it stabilizes. Classes and…","rawMarkdownBody":"# Overview\n\n> The `@loaders.gl/math` library is being developed to support 3D tiles and will be moved to the math.gl repository when it stabilizes.\n\nClasses and utilities to help working with geometries (arrays of vertices) stored in typed arrays according to WebGL/OpenGL layout rules.\n\n## Usage Examples\n\n## Framework Independence\n\nLike all non-core math.gl modules, this library can be used without the math.gl core classes.\n\n- Any input vectors can be supplied as length 3 JavaScript `Array` instances.\n- Any result vectors can be treated as length 3 JavaScript `Array` instances (they may be math.gl `Vector3`).\n- The core math.gl classes inherit from JavaScript `Array` and can be used directly as input.\n","slug":"modules/math/docs","title":"Overview"},{"excerpt":"Overview The  module supports the KML format. KML (Keyhole Markup Language) is an XML-based file format used to display geographic data in…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/kml` module supports the KML format.\n\nKML (Keyhole Markup Language) is an XML-based file format used to display geographic data in an Earth browser such as Google Earth (originally named \"Keyhole Earth Viewer\"). It can be used with any 2D or 3D maps.\n\nReferences:\n\n- [Keyhole Markup Language - Wikipedia](https://en.wikipedia.org/wiki/Keyhole_Markup_Language)\n- [KML Tutorial - Google](https://developers.google.com/kml/documentation/kml_tut)\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/kml\n```\n\n## Attribution\n\n`XMLLoader` is an adaptation of Nick Blackwell's [`js-simplekml`](https://github.com/nickolanack/js-simplekml) module under MIT license.\n","slug":"modules/kml/docs","title":"Overview"},{"excerpt":"Overview The  module contains loader and writers for images that follow loaders.gl conventions and work under both node and browser…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/images` module contains loader and writers for images that follow loaders.gl conventions and work under both node and browser.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/images\nnpm install @loaders.gl/core\n```\n\n## API\n\n| Loader                                                          | Description |\n| --------------------------------------------------------------- | ----------- |\n| [`ImageLoader`](modules/images/docs/api-reference/image-loader) |             |\n| [`ImageWriter`](modules/images/docs/api-reference/image-writer) |             |\n\n### Binary Image API\n\nA set of functions that can extract information from \"unparsed\" binary memory representation of certain image formats. These functions are intended to be called on raw `ArrayBuffer` data, before the `ImageLoader` parses it and converts it to a parsed image type.\n\nThese functions are used internally to autodetect if image loader can be used to parse a certain `ArrayBuffer`, but are also available to applications.\n\n| Function                                                                     | Description |\n| ---------------------------------------------------------------------------- | ----------- |\n| `isBinaryImage(imageData : ArrayBuffer [, mimeType : String]) : Boolean`     |             |\n| `getBinaryImageMIMEType(imageData : ArrayBuffer) : String | null`            |             |\n| `getBinaryImageSize(imageData : ArrayBuffer [, mimeType : String]) : Object` |             |\n\n### Parsed Image API\n\nA set of functions to work with parsed images returned by the `ImageLoader`.\n\n| Function                                        | Description                                                                                               |\n| ----------------------------------------------- | --------------------------------------------------------------------------------------------------------- |\n| `isImageTypeSupported(type : string) : boolean` | Check if type is supported by current run-time environment                                                |\n| `getSupportedImageType() : string`              | Returns the image type selected by default ( `options.image.type: 'auto'` in current run-time environment |\n| `isImage(image : any) : boolean`                | Checks any JavaScript value to see if it is an image of a type that loaders.gl can work with              |\n| `getImageType(image : any) : string`            | Returns the type name for this image.                                                                     |\n| `getImageSize(image : any) : object`            | Returns `width` and `height` for an image                                                                 |\n| `getImageData(image : any) : typedarray`        | Returns a typed array representing the pixels of an image                                                 |\n\n### Image Loading API for WebGL Textures\n\nThe images API also offers functions to load \"composite\" images for WebGL textures, cube textures and image mip levels.\n\nThese functions take a `getUrl` parameter that enables the app to supply the url for each \"sub-image\", and return a single promise enabling applications to for instance load all the faces of a cube texture, with one image for each mip level for each face in a single async operation.\n\n| Function                                                              | Description                                                                                                           |\n| --------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n| [`loadImage`](modules/images/docs/api-reference/load-image)           | Load a single image                                                                                                   |\n| [`loadImageArray`](modules/images/docs/api-reference/load-images)     | Load an array of images, e.g. for a `Texture2DArray` or `Texture3D`                                                   |\n| [`loadImageCube`](modules/images/docs/api-reference/load-cube-images) | Load a map of 6 images for the faces of a cube map, or a map of 6 arrays of images for the mip levels of the 6 faces. |\n\nAs with all loaders.gl functions, while these functions are intended for use in WebGL applications, they do not call any WebGL functions, and do not actually create any WebGL textures..\n\n## Image Types\n\nTo support image loading on older browsers and Node.js, the `ImageLoader` can return different types, i.e. different representations of the parsed image.\n\n- `ImageBitmap` - An `ImageBitmap` object represents a bitmap image that can be painted to a canvas without undue latency. This is the preferred parsed image representation in the browser. It can also be transferred efficiently between threads. Not available in some older browsers.\n- `Image` (aka `HTMLImageElement`) - The traditional HTML image class. Available in all browsers.\n- `ndarray` - a memory layout for parsed pixels in node.js. Texture creation functions in headless gl accept `ndarray` images.\n\nSee [`ImageLoader`](modules/images/docs/api-reference/image-loader) for more details on options etc.\n","slug":"modules/images/docs","title":"Overview"},{"excerpt":"Overview The  module handles the the Point Cloud Data format, which encodes point cloud data for use inside Point Cloud Library (PCL…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/pcd` module handles the the [Point Cloud Data format](http://pointclouds.org/documentation/tutorials/pcd_file_format.php), which encodes point cloud data for use inside Point Cloud Library (PCL).\n\n## Installation\n\n```bash\nnpm install @loaders.gl/pcd\nnpm install @loaders.gl/core\n```\n\n## Attribution\n\nPCDLoader is a fork of the THREE.js PCDLoader under MIT License. The THREE.js source files contained the following attributions:\n\n- @author Filipe Caixeta / http://filipecaixeta.com.br\n- @author Mugen87 / https://github.com/Mugen87\n","slug":"modules/pcd/docs/api-reference","title":"Overview"},{"excerpt":"PLYLoader The  parses simple meshes in the Polygon File Format or the Stanford Triangle Format. Loader Characteristic File Extension  File…","rawMarkdownBody":"# PLYLoader\n\nThe `PLYLoader` parses simple meshes in the Polygon File Format or the Stanford Triangle Format.\n\n| Loader                | Characteristic                                |\n| --------------------- | --------------------------------------------- |\n| File Extension        | `.ply`                                        |\n| File Type             | Binary/Text                                   |\n| File Format           | [PLY](http://paulbourke.net/dataformats/ply/) |\n| Data Format           | [Mesh](docs/specifications/category-mesh.md)  |\n| Decoder Type          | Synchronous                                   |\n| Worker Thread Support | Yes                                           |\n| Streaming Support     | No                                            |\n\n## Usage\n\n```js\nimport {PLYLoader, PLYWorkerLoader} from '@loaders.gl/ply';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, PLYLoader, options);\n// Decode on worker thread\nconst data = await load(url, PLYWorkerLoader, options);\n```\n\n## Options\n\n| Option | Type | Default | Description |\n| ------ | ---- | ------- | ----------- |\n\n","slug":"modules/ply/docs/api-reference/ply-loader","title":"PLYLoader"},{"excerpt":"Overview  The  module provides loaders and writers of the GLB/glTF formats. Installation Optionally, to support Draco encoded gltf files…","rawMarkdownBody":"# Overview\n\n![logo](./images/gltf-small.png)\n\nThe `@loaders.gl/gltf` module provides loaders and writers of the GLB/glTF formats.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/gltf\nnpm install @loaders.gl/core\n```\n\nOptionally, to support Draco encoded gltf files\n\n```bash\nnpm install @loaders.gl/draco\n```\n\n## GLTFScenegraph API\n\nTo simplify traversing and building glTF data objects, the [`GLTFScenegraph`](docs/api-reference/gltf/gltf-scenegraph) class can be used.\n\nA glTF data object can also be built programmatically using the GLTFScenegraph's \"fluent API\":\n\n```js\nimport {encode} from '@loaders.gl/gltf';\nimport {GLTFScenegraph, GLTFWriter} from '@loaders.gl/gltf';\nconst gltfScenegraph = new GLTFScenegraph()\n  .addApplicationData(...)\n  .addExtras(...)\n  .addExtension(...)\n  .addRequiredExtension(...);\n\nconst arrayBuffer = encode(gltfScenegraph, GLTFWriter);\n```\n\n## GLTF Post Processing\n\nThe [`postProcessGLTF`](docs/api-reference/gltf/post-process-gltf) function implements a number of transformations on the loaded glTF data that would typically need to be performed by the application after loading the data, and is provided as an optional function that applications can call after loading glTF data. Refer to the reference page for details on what transformations are performed.\n\nContext: the glTF data object returned by the GLTF loader contains the \"raw\" glTF JSON structure (to ensure generality and \"data fidelity\" reasons). However, most applications that are going to use the glTF data to visualize it in (typically in WebGL) will need to do some processing of the loaded data before using it.\n\n## Using GLB as a \"Binary Container\" for Arbitrary Data\n\nThe GLB binary container format used by glTF addresses a general need to store a mix of JSON and binary data, and can potentially be used as a foundation for building custom loaders and writers.\n\nTo allow for this (and also to generally improve the glTF code structure), the `GLTFLoader` and `GLTFBuilder` classes are built on top of GLB focused classes (`GLBLoader` and `GLBBuilder`) that can be used independently of the bigger glTF classes.\n\n## glTF Extension Support\n\nCertain glTF extensions are fully or partially supported by the glTF classes. For details on which extensions are supported, see [glTF Extensions](docs/api-reference/gltf-loaders/gltf-extensions).\n\n## Draco Mesh and Point Cloud Compression\n\nDraco encoding and decoding is supported by the `GLTFBuilder` and `GLTFParser` classes but requires the DracoWriter and DracoLoader dependencies to be \"injected\" by the application.\n\n```js\nimport {GLTFBuilder} from '@loaders.gl/gltf';\nimport {DracoWriter, DracoLoader} from '@loaders.gl/draco';\n\nconst gltfBuilder = new GLTFBuilder({DracoWriter, DracoLoader});\n```\n","slug":"modules/gltf/docs","title":"Overview"},{"excerpt":"PCDLoader The  loads point cloud in the Point Cloud Data (PCD) format. Loader Characteristic File Extension  File Type Text/Binary File…","rawMarkdownBody":"# PCDLoader\n\nThe `PCDLoader` loads point cloud in the Point Cloud Data (PCD) format.\n\n| Loader                | Characteristic                                                                         |\n| --------------------- | -------------------------------------------------------------------------------------- |\n| File Extension        | `.pcd`                                                                                 |\n| File Type             | Text/Binary                                                                            |\n| File Format           | [Point Cloud Data](http://pointclouds.org/documentation/tutorials/pcd_file_format.php) |\n| Data Format           | [PointCloud](docs/specifications/category-mesh.md)                                     |\n| Decoder Type          | Synchronous                                                                            |\n| Worker Thread Support | Yes                                                                                    |\n| Streaming Support     | No                                                                                     |\n\nNote: Currently only `ascii` and `binary` subformats are supported. Compressed binary files are currently not supported.\n\n## Usage\n\n```js\nimport {PCDLoader, PCDWorkerLoader} from '@loaders.gl/pcd';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, PCSLoader, options);\n// Decode on worker thread\nconst data = await load(url, PCDWorkerLoader, options);\n```\n\n## Options\n\n| Option | Type | Default | Description |\n| ------ | ---- | ------- | ----------- |\n\n","slug":"modules/pcd/docs/api-reference/pcd-loader","title":"PCDLoader"},{"excerpt":"Overview The  module contains the core API of loaders.gl The core API offers functions to parse data in various ways using loaders    To…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/core` module contains the core API of loaders.gl\n\nThe core API offers functions to parse data in various ways using loaders\n\n- [`parse`](modules/core/docs/api-reference/parse)\n- [`parseSync`](modules/core/docs/api-reference/parseSync)\n- [`parseInBatches`](modules/core/docs/api-reference/parseInBatches)\n\nTo fetch data\n\n- [`fetchFile`](modules/core/docs/api-reference/fetchFile)\n\nTo load (fetch and parse) data\n\n- [`load`](modules/core/docs/api-reference/load)\n\nTo register loaders, or select a loader that matches a file from a list of candidate loaders:\n\n- [`registerLoaders`](modules/core/docs/api-reference/registerLoaders)\n- [`selectLoader`](modules/core/docs/api-reference/selectLoader)\n\nTo encode and save data\n\n- [`encode`](modules/core/docs/api-reference/encode)\n- [`write-file`](modules/core/docs/api-reference/file)\n- [`save`](modules/core/docs/api-reference/save)\n\nAs well as some utility functions.\n","slug":"modules/core/docs","title":"Overview"},{"excerpt":"OBJLoader The  parses the OBJ half of the classic Wavefront OBJ/MTL format. Loader Characteristic File Extension  File Type Text File Format…","rawMarkdownBody":"# OBJLoader\n\nThe `OBJLoader` parses the OBJ half of the classic Wavefront OBJ/MTL format.\n\n| Loader                | Characteristic                                                          |\n| --------------------- | ----------------------------------------------------------------------- |\n| File Extension        | `.obj`                                                                  |\n| File Type             | Text                                                                    |\n| File Format           | [Wavefront OBJ file](https://en.wikipedia.org/wiki/Wavefront_.obj_file) |\n| Data Format           | [Mesh](docs/specifications/category-mesh.md)                            |\n| Decoder Type          | Synchronous                                                             |\n| Worker Thread Support | Yes                                                                     |\n| Streaming Support     | No                                                                      |\n\n## Usage\n\n```js\nimport {OBJLoader, OBJWorkerLoader} from '@loaders.gl/obj';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, OBJLoader, options);\n// Decode on worker thread\nconst data = await load(url, OBJWorkerLoader, options);\n```\n\n## Options\n\n| Option | Type | Default | Description |\n| ------ | ---- | ------- | ----------- |\n\n","slug":"modules/obj/docs/api-reference/obj-loader","title":"OBJLoader"},{"excerpt":"Overview The  module handles tabular data stored in the CSV/DSV file format. Installation Loaders and Writers Loader   Additional APIs See…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/csv` module handles tabular data stored in the [CSV/DSV file format](https://en.wikipedia.org/wiki/Comma-separated_values).\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/csv\n```\n\n## Loaders and Writers\n\n| Loader                                                         |\n| -------------------------------------------------------------- |\n| [`CSVLoader`](modules/csv/docs/api-reference/csv-loader)       |\n| [`CSVWorkerLoader`](modules/csv/docs/api-reference/csv-loader) |\n\n## Additional APIs\n\nSee table category.\n\n## Attributions\n\nCSVLoader is based on a fork of the [papaparse](https://github.com/mholt/PapaParse) module, under MIT license.\n","slug":"modules/csv/docs","title":"Overview"},{"excerpt":"Overview  The  module handles compressing and decompressing of 3D meshes and point clouds with DRACO. Installation Loaders and Writers…","rawMarkdownBody":"# Overview\n\n![logo](./images/draco-small.png)\n\nThe `@loaders.gl/draco` module handles compressing and decompressing of 3D meshes and point clouds with [DRACO](https://github.com/google/draco).\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/draco\n```\n\n## Loaders and Writers\n\n| Loader                                                               |\n| -------------------------------------------------------------------- |\n| [`DracoLoader`](modules/draco/docs/api-reference/draco-loader)       |\n| [`DracoWorkerLoader`](modules/draco/docs/api-reference/draco-loader) |\n| [`DracoWriter`](modules/draco/docs/api-reference/draco-writer)       |\n\n## Additional APIs\n\nSee point cloud / mesh category.\n\n## Dependencies\n\nDraco support requires the Draco libraries, which are quite big (see table below). By default, these will be loaded from CDN but can optionally be bundled and supplied by the application through the top-level `options.modules` option:\n\nBundling the entire `draco3d` library:\n\n```js\nimport draco from 'draco3d';\nimport {setLoaderOptions} from '@loaders.gl/core';\nsetLoaderOptions({\n  modules: {\n    draco3d\n  }\n});\n```\n\nBundling only the WebAssembly decoder\n\n```js\nimport {setLoaderOptions} from '@loaders.gl/core';\nsetLoaderOptions({\n  modules: {\n    'draco_wasm_wrapper.js': require('@loaders.gl/draco/libs/draco_wasm_wrapper.js'),\n    'draco_decoder.wasm': require('@loaders.gl/draco/libs/draco_decoder.wasm') // NOTE: importing `wasm` requires bundler config\n  }\n});\n```\n\n| Library                                 | Import                            | Install               | Size        | Description                                                                        |\n| --------------------------------------- | --------------------------------- | --------------------- | ----------- | ---------------------------------------------------------------------------------- |\n| `options.libs.draco3d`                  | `require('draco3d')`              | `npm install draco3d` | ~1.5MB      | The full Draco library (encode + decode, web assembly + IE11 javascript fallback). |\n| `options.libs['draco_decoder.wasm']`    | [`ArrayBuffer`]()                 | ~320K                 | manual copy | Web Assembly Decoder (access using `draco_wasm_wrapper.js`)                        |\n| `options.libs['draco_wasm_wrapper.js']` | `require('.../draco_decoder.js')` | ~64K                  | manual copy | JavaScript wrapper for `draco_decoder.wasm`                                        |\n| `options.libs['draco_decoder.js']`      | `require('.../draco_decoder.js')` | ~790K                 | manual copy | JavaScript decoder (fallback for IE11)                                             |\n| `options.libs['draco_encoder.js']`      | `require('.../draco_encode.js')`  | ~900K                 | manual copy | Encoder part of the library                                                        |\n\nRemarks\n\n- Due to the size of the Draco libraries, a reasonable strategy for applications that wish to bundle their dependencies (e.g to avoid relying on a potentially flaky CDN) might be to bundle and supply only `draco_decoder.wasm` and `draco_wasm_wrapper.js`, and still rely on the default setup to load the IE11 fallback library and the encoder code from CDN when needed.\n- Web Assembly code (`wasm` files) must be imported/loaded as binary data (`ArrayBuffer`). An option for webpack users is the [`arraybuffer-loader`](https://www.npmjs.com/package/arraybuffer-loader#for-wasm-file) webpack \"loader\".\n\n## Attributions\n\nBased on Draco examples, under the Apache 2.0 license.\n","slug":"modules/draco/docs","title":"Overview"},{"excerpt":"GLType Helper functions to work with WebGL data type constants. WebGL type constant JavaScript Typed Array Notes      Not yet directly…","rawMarkdownBody":"# GLType\n\nHelper functions to work with WebGL data type constants.\n\n| WebGL type constant | JavaScript Typed Array | Notes                                 |\n| ------------------- | ---------------------- | ------------------------------------- |\n| `GL.FLOAT`          | `Float32Array`         |                                       |\n| `GL.DOUBLE`         | `Float64Array`         | Not yet directly usable in WebGL/GLSL |\n| `GL.UNSIGNED_SHORT` | `Uint16Array`          |                                       |\n| `GL.UNSIGNED_INT`   | `Uint32Array`          |                                       |\n| `GL.UNSIGNED_BYTE`  | `Uint8Array`           |                                       |\n| `GL.UNSIGNED_BYTE`  | `Uint8ClampedArray`    |                                       |\n| `GL.BYTE`           | `Int8Array`            |                                       |\n| `GL.SHORT`          | `Int16Array`           |                                       |\n| `GL.INT`            | `Int32Array`           |                                       |\n\n## Usage\n\n```js\nimport {GL, GLType} from '@loaders.gl/math';\n// Returns Int8Array.BYTES_PER_ELEMENT\nvar size = GLType.getSizeInBytes(GL.BYTE);\n```\n\n## Static Methods\n\n### GLType.fromTypedArray(typedArray: Typed Array | Function) : Number\n\nReturns the size, in bytes, of the corresponding datatype.\n\n- `glType` The component datatype to get the size of.\n\nReturns\n\nThe size in bytes.\n\nThrows\n\n- glType is not a valid value.\n\nGets the {@link ComponentDatatype} for the provided TypedArray instance.\n\n- array The typed array.\n\nReturns\n\nThe ComponentDatatype for the provided array, or undefined if the array is not a TypedArray.\n\n### GLType.getArrayType(glType: Number) : Function\n\nreturns the constructor of the array\n\n### static GLType.getByteSize(glType: Number) : Number\n\nReturns the size in bytes of one element of the provided WebGL type.\n\nEquivalent to `GLType.getArrayType(glType).BYTES_PER_ELEMENT`.\n\n### static GLType.validate(glType) : Boolean\n\nReturns `true` if `glType` is a valid WebGL data type.\n\n### static GLType.createTypedArray(glType : Number, buffer : ArrayBuffer [, byteOffset : Number [, length : Number]]) : TypedArray\n\nCreates a typed view of an array of bytes.\n\n- `glType` The type of typed array (ArrayBuffer view) to create.\n- `buffer` The buffer storage to use for the view.\n- `byteOffset`=`0` The offset, in bytes, to the first element in the view.\n- `length`= The number of elements in the view. Defaults to buffer length.\n\nReturns\n\n`Int8Array`|`Uint8Array`|`Int16Array`|`Uint16Array`|`Int32Array`|`Uint32Array`|`Float32Array`|`Float64Array` A typed array view of the buffer.\n\nThrows\n\n- `glType` is not a valid value.\n","slug":"modules/math/docs/api-reference/gl-type","title":"GLType"},{"excerpt":"Overview The  module contains a loader for Basis encoded compressed textures (images). Installation API Loader Description   Compressed…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/basis` module contains a loader for Basis encoded compressed textures (images).\n\n## Installation\n\n```bash\nnpm install @loaders.gl/basis\nnpm install @loaders.gl/core\n```\n\n## API\n\n| Loader                                                         | Description |\n| -------------------------------------------------------------- | ----------- |\n| [`BasisLoader`](modules/basis/docs/api-reference/basis-loader) |             |\n\n### Compressed Texture API\n\nA set of functions that can extract information from \"unparsed\" binary memory representation of certain compressed texture image formats. These functions are intended to be called on raw `ArrayBuffer` data, before the `BasisLoader` parses it and converts it to a parsed image type.\n\nTBA\n\n| Function | Description |\n| -------- | ----------- |\n\n\n## Return Types\n\nThe `BasisLoader` returns Array of Array of ArrayBuffer\n\nTODO - Node.js handling - expand to normal image?\n\nSee [`BasisLoader`](modules/basis/docs/api-reference/image-loader) for more details on options etc.\n","slug":"modules/basis/docs","title":"Overview"},{"excerpt":"Overview  The  module handles Apache Arrow, an emerging standard for large in-memory columnar data. Installation Loaders and Writers Loader…","rawMarkdownBody":"# Overview\n\n![logo](./images/apache-arrow-small.png)\n\nThe `@loaders.gl/arrow` module handles [Apache Arrow](https://arrow.apache.org/), an emerging standard for large in-memory columnar data.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/arrow\n```\n\n## Loaders and Writers\n\n| Loader                                                               |\n| -------------------------------------------------------------------- |\n| [`ArrowLoader`](modules/arrow/docs/api-reference/arrow-loader)       |\n| [`ArrowWorkerLoader`](modules/arrow/docs/api-reference/arrow-loader) |\n\n## Additional APIs\n\nArrow provides a rich JavaScript API for working with Arrow formatted data. Please refer to the [`ArrowJS`](arrowjs/docs) API documentation.\n\n## Attributions\n\n`@loaders.gl/arrow` was developed with the benefit of extensive technical advice from Paul Taylor @ Graphistry.\n","slug":"modules/arrow/docs","title":"Overview"},{"excerpt":"Overview The  module handles the LASER file format (LAS) or its compressed version (LAZ), a public format for the interchange of…","rawMarkdownBody":"# Overview\n\nThe `@loaders.gl/las` module handles the [LASER file format](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities) (LAS) or its compressed version (LAZ), a public format for the interchange of 3-dimensional point cloud data data, developed for LIDAR mapping purposes.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/core @loaders.gl/las\n```\n\n## Attribution\n\nLASLoader is a fork of Uday Verma and Howard Butler's [plasio](https://github.com/verma/plasio/) under MIT License.\n","slug":"modules/las/docs/api-reference","title":"Overview"},{"excerpt":"LASLoader The  parses a point cloud in the LASER file format. Loader Characteristic File Extension ,  File Type Binary File Format LASER…","rawMarkdownBody":"# LASLoader\n\nThe `LASLoader` parses a point cloud in the LASER file format.\n\n| Loader                | Characteristic                                                                                                           |\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n| File Extension        | `.las`, `.laz`                                                                                                           |\n| File Type             | Binary                                                                                                                   |\n| File Format           | [LASER file format](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities) |\n| Data Format           | [PointCloud](docs/specifications/category-mesh.md)                                                                       |\n| Decoder Type          | Synchronous                                                                                                              |\n| Worker Thread Support | Yes                                                                                                                      |\n| Streaming Support     | No                                                                                                                       |\n\n## Usage\n\n```js\nimport {LASLoader, LASWorkerLoader} from '@loaders.gl/las';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, LASLoader, options);\n// Decode on worker thread\nconst data = await load(url, LASWorkerLoader, options);\n```\n\n## Options\n\n| Option               | Type     | Default | Description                                                               |\n| -------------------- | -------- | ------- | ------------------------------------------------------------------------- |\n| `options.las.skip`   | Number   | `1`     | Read one from every _n_ points.                                           |\n| `options.onProgress` | Function | -       | Callback when a new chunk of data is read. Only works on the main thread. |\n","slug":"modules/las/docs/api-reference/las-loader","title":"LASLoader"},{"excerpt":"KMLLoader The  parses KML files into GeoJSON. Loader Characteristic File Extension  File Type Text File Format KML Data Format GIS Decoder…","rawMarkdownBody":"# KMLLoader\n\nThe `KMLLoader` parses KML files into GeoJSON.\n\n| Loader                | Characteristic                                               |\n| --------------------- | ------------------------------------------------------------ |\n| File Extension        | `.kml`                                                       |\n| File Type             | Text                                                         |\n| File Format           | [KML](https://en.wikipedia.org/wiki/Keyhole_Markup_Language) |\n| Data Format           | [GIS](docs/specifications/category-gis.md)                   |\n| Decoder Type          | Synchronous                                                  |\n| Worker Thread Support | No                                                           |\n| Streaming Support     | No                                                           |\n\n## Usage\n\n```js\nimport {KMLLoader} from '@loaders.gl/kml';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, KMLLoader, options);\n```\n\n## Options\n\n| Option            | Type    | Default | Description                                                                                                |\n| ----------------- | ------- | ------- | ---------------------------------------------------------------------------------------------------------- |\n| `useLngLatFormat` | Boolean | `true`  | KML longitudes and latitudes are specified as `[lat, lng]`. This option \"normalizes\" them to `[lng, lat]`. |\n| `useColorArrays`  | Boolean | `true`  | Convert color strings to arrays.                                                                           |\n\n## Limitations\n\n- Currently XML parsing is only implemented in browsers, not in Node.js. Check `KMLLoader.supported` to check at run-time.\n","slug":"modules/kml/docs/api-reference/kml-loader","title":"KMLLoader"},{"excerpt":"Overview  The  module supports loading and traversing 3D Tiles. References 3D Tiles Specification - The living specification. 3D Tiles…","rawMarkdownBody":"# Overview\n\n![logo](./images/3d-tiles-small.png)\n\nThe `@loaders.gl/3d-tiles` module supports loading and traversing 3D Tiles.\n\nReferences\n\n- [3D Tiles Specification](https://github.com/AnalyticalGraphicsInc/3d-tiles) - The living specification.\n- [3D Tiles Standard](https://www.opengeospatial.org/standards/3DTiles) - The official standard from [OGC](https://www.opengeospatial.org/), the Open Geospatial Consortium.\n\n## Installation\n\n```bash\nnpm install @loaders.gl/3d-tiles\nnpm install @loaders.gl/core\n```\n\n## API\n\nA standard complement of loaders and writers are provided to load the individual 3d Tile file formats:\n\n- [`Tileset3DLoader`](modules/3d-tiles/docs/api-reference/tileset-3d-loader), a loader for top-level (or nested) tilesets files.\n- [`Tile3DLoader`](modules/3d-tiles/docs/api-reference/tile-3d-loader) for individual tiles.\n- [`Tile3DWriter`](modules/3d-tiles/docs/api-reference/tile-3d-writer) for individual tiles.\n\nTo handle the complex dynamic tile selection and loading required to performantly render larger-than-browser-memory tilesets, additional helper classes are provided:\n\n- [`Tileset3D`](modules/3d-tiles/docs/api-reference/tileset-3d) to work with the loaded tileset.\n- `Tile3DHeader` (currently undocumented) to access data for a specific tile.\n\n## Usage\n\nBasic API usage is illustrated in the following snippet. Create a `Tileset3D` instance, point it a valid tileset URL, set up callbacks, and keep feeding in new camera positions:\n\n```js\nimport {Tileset3D} from '@loaders.gl/3d-tiles';\nconst tileset = new Tileset3D(tilesetJson, {\n  url: TILESET_URL,\n  onTilesetLoad: tileset => console.log('Tileset loaded'),\n  onTileLoad: tileset => console.log('Tile loaded'),\n  onTileUnload: tileset => console.log('Tile unloaded')\n});\n\n// initial camera view\ntileset3d.update(cameraParameters1);\n\n// Camera changes (pan zoom etc)\ntileset3d.update(cameraParameters2);\n\n// Visible tiles\nconst visibleTiles = tileset3d.selectedTiles;\n\n// Note that visibleTiles will likely not immediately include all tiles\n// tiles will keep loading and file `onTileLoad` callbacks\n```\n\n## Remarks\n\n`@loaders.gl/3d-tiles` does not yet support the full 3D tiles standard. Notable omissions are:\n\n- [Region bounding volumes](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#bounding-volume) are supported but not optimally\n- [Styling](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification/Styling) is not yet supported\n- [Viewer request volumes](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#viewer-request-volume) are not yet supported\n\n## Attribution\n\n`@loaders.gl/3d-tiles` is a fork of 3D tile related code in the [Cesium github repository](https://github.com/AnalyticalGraphicsInc/cesium) under Apache 2 License, and is developed in collabration with the Cesium engineering team.\n","slug":"modules/3d-tiles/docs","title":"Overview"},{"excerpt":"Binary Image Utilities Allows extracting image format and size (dimensions) from binary images without parsing the full image, by reading…","rawMarkdownBody":"# Binary Image Utilities\n\nAllows extracting image format and size (dimensions) from binary images without parsing the full image, by reading format-specific headers in the encoded binary data (e.g. encoded JPEG or PNG images).\n\nThe format is reported using MIME types strings. Supported binary formats and their MIME types are:\n\n| Format | MIME Type    |\n| ------ | ------------ |\n| PNG    | `image/png`  |\n| JPEG   | `image/jpeg` |\n| BMP    | `image/bmp`  |\n| GIF    | `image/gif`  |\n\n## Usage\n\n```js\nconst arrayBuffer = await fetchFile(imageUrl).then(response => response.arrayBuffer());\n\nconst mimeType = getBinaryImageMIMEType(arrayBuffer);\nconst {width, height} = getBinaryImageSize(arrayBuffer, mimeType);\n```\n\n## Functions\n\n### isBinaryImage(imageData : ArrayBuffer [, mimeType : String]) : Boolean\n\nParameters:\n\n- `imageData`: Binary encoded image data.\n- `mimeType`: A MIME type string.\n\nReturns `true` if the binary data represents a known binary image format or matches the supplied `mimeType`.\n\nParameters:\n\n- `mimeType`: If supplied, checks if the image is of that type. If not supplied, returns `true` if imageData corresponds to a know supported image format.\n\n### getBinaryImageMIMEType(imageData : ArrayBuffer) : String | null\n\nParameters:\n\n- `imageData`: Binary encoded image data.\n\nReturns:\n\n- the MIME type of the image represented by the data, or `null` if it could not be identified.\n\n### getBinaryImageSize(imageData : ArrayBuffer, mimeType? : String) : Object\n\nExtracts the size of the image in `imageData`.\n\nParameters:\n\n- `imageData`: Binary encoded image data.\n- `mimeType`: A MIME type string\n\nReturns:\n\n- an object with fields containing the size of the image represented by the data.\n\n```js\n{\n  width: Number,\n  height: Number\n}\n```\n\nThrows:\n\n- if image is not in a supported binary format.\n\nIf `mimeType` is supplied, assumes the image is of that type. If not supplied, first attempts to auto deduce the image format (see `getImageMIMEType`).\n","slug":"modules/images/docs/api-reference/binary-image-api","title":"Binary Image Utilities"},{"excerpt":"ImageLoader An image loader that works under both Node.js (requires ) and the browser. Loader Characteristic File Extension…","rawMarkdownBody":"# ImageLoader\n\nAn image loader that works under both Node.js (requires `@loaders.gl/polyfills`) and the browser.\n\n| Loader         | Characteristic                                                   |\n| -------------- | ---------------------------------------------------------------- |\n| File Extension | `.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`, `.bmp`, `.ico`, `.svg` |\n| File Type      | Binary                                                           |\n| File Format    | Image                                                            |\n| Data Format    | `ImageBitmap`, `Image` (older browsers) or ndarray (node.js)     |\n| Supported APIs | `load`, `parse`                                                  |\n\n## Usage\n\n```js\nimport '@loaders.gl/polyfills'; // only needed if using under Node\nimport {ImageLoader} from '@loaders.gl/images';\nimport {load} from '@loaders.gl/core';\n\nconst image = await load(url, ImageLoader, options);\n```\n\n## Options\n\n| Option         | Type    | Default  | Description                                                                  |\n| -------------- | ------- | -------- | ---------------------------------------------------------------------------- |\n| `image.type`   | String  | `'auto'` | Set to `imagebitmap`, `html` or `ndarray` to control type of returned image. |\n| `image.decode` | boolean | `true`   | Ensures `html` type images are fully decoded before promise resolves.        |\n\nIn addition, for `imagebitmap` type images, it is possible to pass through options to [`createImageBitmap`](https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/createImageBitmap) to control image extraction, via the separate `options.imagebitmap` object.\n\n| Option                             | Type   | Default     | Description                                                                                                                   |\n| ---------------------------------- | ------ | ----------- | ----------------------------------------------------------------------------------------------------------------------------- |\n| `imagebitmap.imageOrientation`     | string | `'none'`    | image should be flipped vertically. Either `'none'` or `'flipY'`.                                                             |\n| `imagebitmap.premultiplyAlpha`     | string | `'default'` | Premultiply color channels by the alpha channel. One of `'none'`, `'premultiply'`, or `'default'`.                            |\n| `imagebitmap.colorSpaceConversion` | string | `'default'` | Decode using color space conversion. Either `'none'` or `'default'` default indicates implementation-specific behavior.       |\n| `imagebitmap.resizeWidth`          | number | -           | Output image width.                                                                                                           |\n| `imagebitmap.resizeHeight`         | number | -           | Output image height.                                                                                                          |\n| `imagebitmap.resizeQuality`        | string | `'low'`     | Algorithm to be used for resizing the input to match the output dimensions. One of pixelated, low (default), medium, or high. |\n\nPortability note: The exact set of `imagebitmap` options supported may depend on the browser, and also do not apply to `html` or `ndarray` images.\n\n## Remarks\n\n- While generic, the `ImageLoader` is designed with WebGL applications in mind, ensuring that loaded image data can be used to create a `WebGLTexture` both in the browser and in headless gl under Node.js\n- Node.js support requires import `@loaders.gl/polyfills` before installing this module.\n","slug":"modules/images/docs/api-reference/image-loader","title":"ImageLoader"},{"excerpt":"ImageWriter The  class can encode an image into  both under browser and Node.js Loader Characteristic File Extension , ,  File Format Binary…","rawMarkdownBody":"# ImageWriter\n\nThe `ImageWriter` class can encode an image into `ArrayBuffer` both under browser and Node.js\n\n| Loader         | Characteristic          |\n| -------------- | ----------------------- |\n| File Extension | `.png`, `.jpg`, `.jpeg` |\n| File Format    | Binary                  |\n| Data Format    | `ArrayBuffer`           |\n| File Format    | Image                   |\n| Encoder Type   | Asynchronous            |\n| Worker Thread  | No                      |\n| Streaming      | No                      |\n\n## Usage\n\n```js\nimport '@loaders.gl/polyfill'; // only if using under Node\nimport {ImageWriter} from '@loaders.gl/images';\nimport {encode} from '@loaders.gl/core';\n\nconst image = await encode(arrayBuffer, ImageWriter, options);\n```\n\n## Options\n\n| Option | Type   | Default | Description   |\n| ------ | ------ | ------- | ------------- |\n| `type` | String | `'png'` | image type \\* |\n\n\\* Supported image types (MIME types) depends on the environment. Typically PNG and JPG are supported.\n","slug":"modules/images/docs/api-reference/image-writer","title":"ImageWriter"},{"excerpt":"loadCubeImages A function that loads 6 images representing the faces of a cube. Primarily intended for loading images for WebGL  textures…","rawMarkdownBody":"# loadCubeImages\n\nA function that loads 6 images representing the faces of a cube. Primarily intended for loading images for WebGL `GL.TEXTURE_CUBE` textures.\n\n## Usage\n\nLoad images for a cubemap with one image per face\n\n```js\nimport '@loaders.gl/polyfills'; // only needed for Node.js support\nimport {loadImageCube} from `@loaders.gl/images`;\n\nconst imageCube = await loadImageCube(({direction}) => `diffuse-${direction}.png`);\n\nfor (const face in imageCube) {\n  const image = imageCube[face];\n}\n```\n\nLoad images for a cubemap with an array of mip images per face\n\n```js\nimport '@loaders.gl/polyfills'; // only needed for Node.js support\nimport {loadImageCube} from `@loaders.gl/images`;\n\nconst imageCube = await loadImageCube('mips', ({direction}) => `diffuse-${direction}.png`);\n\nfor (const face in imageCube) {\n  const imageArray = imageCube[face];\n  for (const lodImage of imageArray) {\n    ...\n  }\n}\n```\n\n## getUrl Callback Parameters\n\nThe following fields will be supplied as named parameters to the `getUrl` function when loading cube maps:\n\n| `faceIndex` | `face`                                    | `direction` | `axis` | `sign`       |\n| ----------- | ----------------------------------------- | ----------- | ------ | ------------ |\n| 0           | `GL.TEXTURE_CUBE_MAP_POSITIVE_X` (0x8515) | `'right'`   | `'x'`  | `'positive'` |\n| 1           | `GL.TEXTURE_CUBE_MAP_NEGATIVE_X` (0x8516) | `'left'`    | `'x'`  | `'negative'` |\n| 2           | `GL.TEXTURE_CUBE_MAP_POSITIVE_Y` (0x8517) | `'top'`     | `'y'`  | `'positive'` |\n| 3           | `GL.TEXTURE_CUBE_MAP_NEGATIVE_Y` (0x8518) | `'bottom'`  | `'y'`  | `'negative'` |\n| 4           | `GL.TEXTURE_CUBE_MAP_POSITIVE_Z` (0x8519) | `'front'`   | `'z'`  | `'positive'` |\n| 5           | `GL.TEXTURE_CUBE_MAP_NEGATIVE_Z` (0x851a) | `'back'`    | `'z'`  | `'negative'` |\n\nNote: In addition to these values, all `options` passed in to `loadImageCube` are also available in the `getUrl` method.\n\n### loadImageCube(getUrl : ({face, direction, index}) => String, options? : Object) : Object\n\nLoads and image cube, i.e. 6 images keyed by WebGL face constants (see table).\n\nParameters:\n\n- `getUrl`: A function that generates the url for each image, it is called for each image with the `index` of that image.\n- `options`: Supports the same options as [`ImageLoader`](modules/images/docs/api-reference/image-loader).\n\nReturns\n\n- An object with 6 key/value pairs containing images (or arrays of mip images) for for each cube face. They keys are the (stringified) numeric values of the GL constant for the respective faces of the cube\n\n## Options\n\nAccepts the same options as [`ImageLoader`](modules/images/docs/api-reference/image-loader), and\n\n| Option            | Type              | Default | Description                                            |\n| ----------------- | ----------------- | ------- | ------------------------------------------------------ |\n| `image.mipLevels` | `Number | String` | `0`     | If `'auto'` or non-zero, loads an array of mip images. |\n\nNumber of mip level images to load: Use `0` to indicate a single image with no mips. Supplying the string `'auto'` will infer the mipLevel from the size of the `lod`=`0` image.\n\n## Remarks\n\n- Returned images can be passed directly to WebGL texture methods. See [`ImageLoader`](modules/images/docs/api-reference/image-loader) for details about the type of the returned images.\n","slug":"modules/images/docs/api-reference/load-image-cube","title":"loadCubeImages"},{"excerpt":"loadImage Usage Function loadImage(getUrl : String | Function, options? : Object]) : image | image[] A basic image loading function for…","rawMarkdownBody":"# loadImage\n\n## Usage\n\n```js\nimport '@loaders.gl/polyfills'; // only needed if using under Node\nimport {loadImage} from `@loaders.gl/images`;\n\nconst image = await loadImage(url);\n```\n\n```js\nimport '@loaders.gl/polyfills'; // only needed if using under Node\nimport {loadImage} from `@loaders.gl/images`;\n\nconst URL = ...;\n\nconst image = await loadImage(({lod}) => `${URL}-${lod}.jpg`, {\n  image: {\n    mipLevels: 'auto'\n  }\n});\n\nfor (const lodImage of imageArray) {\n  ...\n}\n```\n\n## Function\n\n### loadImage(getUrl : String | Function, options? : Object]) : image | image[]\n\nA basic image loading function for loading a single image (or an array of mipmap images representing a single image).\n\n- `getUrl`: A function that generates the url for each image, it is called for each image with the `lod` of that image.\n- `options`: Supports the same options as [`ImageLoader`](modules/images/docs/api-reference/image-loader).\n\nReturns\n\n- image or array of images\n\n## Options\n\nAccepts the same options as [`ImageLoader`](modules/images/docs/api-reference/image-loader), and\n\n| Option            | Type              | Default | Description                                            |\n| ----------------- | ----------------- | ------- | ------------------------------------------------------ |\n| `image.mipLevels` | `Number | String` | `0`     | If `'auto'` or non-zero, loads an array of mip images. |\n\nNumber of mip level images to load: Use `0` to indicate a single image with no mips. Supplying the string `'auto'` will infer the mipLevel from the size of the `lod`=`0` image.\n","slug":"modules/images/docs/api-reference/load-image","title":"loadImage"},{"excerpt":"JSONLoader Streaming loader for comma-separated value and delimiter-separated value encoded files. Loader Characteristic File Extension…","rawMarkdownBody":"# JSONLoader\n\nStreaming loader for comma-separated value and [delimiter-separated value](https://en.wikipedia.org/wiki/Delimiter-separated_values) encoded files.\n\n| Loader         | Characteristic                                       |\n| -------------- | ---------------------------------------------------- |\n| File Extension | `.csv`, `.dsv`                                       |\n| File Type      | Text                                                 |\n| File Format    | [RFC4180](https://tools.ietf.org/html/rfc4180)       |\n| Data Format    | [Classic Table](/docs/specifications/category-table) |\n| Supported APIs | `load`, `parse`, `parseSync`, `parseInBatches`       |\n\n## Usage\n\n```js\nimport {JSONLoader} from '@loaders.gl/json';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, JSONLoader, {json: options});\n```\n\nThe JSONLoader supports streaming JSON parsing, in which case it will yield \"batches\" of rows of the first array it encounters in the JSON. To e.g. parse a stream of GeoJSON:\n\n```js\nimport {JSONLoader} from '@loaders.gl/json';\nimport {load} from '@loaders.gl/core';\n\nconst data = await loadInBatches('geojson.json', JSONLoader);\n\nfor await (const batch of batches) {\n  // batch.data will contain a number of rows\n  for (const feature of batch.data) {\n    switch (feature.geometry.type) {\n      case 'Polygon':\n      ...\n    }\n  }\n}\n```\n\n## Options\n\n| Option       | Type    | Default | Description                                                            |\n| ------------ | ------- | ------- | ---------------------------------------------------------------------- |\n| `json.table` | Boolean | TBD     | Parse JSON as table, i.e. return the first embedded array in the JSON. |\n","slug":"modules/json/docs/api-reference/json-loader","title":"JSONLoader"},{"excerpt":"JSONLoader (Experimental) This doc is a WIP Streaming loader for comma-separated value and delimiter-separated value encoded files. Loader…","rawMarkdownBody":"# JSONLoader (Experimental)\n\n> This doc is a WIP\n\nStreaming loader for comma-separated value and [delimiter-separated value](https://en.wikipedia.org/wiki/Delimiter-separated_values) encoded files.\n\n| Loader         | Characteristic                                       |\n| -------------- | ---------------------------------------------------- |\n| File Extension | `.json`,                                             |\n| File Type      | Text                                                 |\n| File Format    | JSON                                                 |\n| Data Format    | [Classic Table](/docs/specifications/category-table) |\n| Supported APIs | `load`, `parse`, `parseSync`, `parseInBatches`       |\n\n## Usage\n\n```js\nimport {JSONLoader} from '@loaders.gl/json';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, JSONLoader, {json: options});\n```\n\nThe JSONLoader supports streaming JSON parsing, in which case it will yield \"batches\" of rows of the first array it encounters in the JSON. To e.g. parse a stream of GeoJSON:\n\n```js\nimport {JSONLoader} from '@loaders.gl/json';\nimport {load} from '@loaders.gl/core';\n\nconst data = await loadInBatches('geojson.json', JSONLoader);\n\nfor await (const batch of batches) {\n  // batch.data will contain a number of rows\n  for (const feature of batch.data) {\n    switch (feature.geometry.type) {\n      case 'Polygon':\n      ...\n    }\n  }\n}\n```\n\n## Options\n\n| Option       | Type    | Default | Description                                                            |\n| ------------ | ------- | ------- | ---------------------------------------------------------------------- |\n| `json.table` | Boolean | TBD     | Parse JSON as table, i.e. return the first embedded array in the JSON. |\n\n### MIME Types and File Extensions\n\n| Format                          | Extension | MIME Media Type [RFC4288](https://www.ietf.org/rfc/rfc4288.txt) |\n| ------------------------------- | --------- | --------------------------------------------------------------- |\n| Standard JSON                   | `.json`   | `application/json`                                              |\n| Line-delimited JSON             | `.jsonl`  | -                                                               |\n| NewLine delimited JSON          | `.ndjson` | `application/x-ndjson`                                          |\n| Record separator-delimited JSON | -         | `application/json-seq`                                          |\n\n## Attribution\n\nThe underlying `StreamingJSONParser` class is a fork of [`clarinet`](https://github.com/dscape/clarinet) under BSD 2-clause license.\n\n## Roadmap\n\n### General Improvements\n\nJSON is supported natively and `JSON.parse`. However there are a number of reasons why you might consider a custom JSON loader:\n\nError messages: `JSON.parse` tends to have unhelpful error messages\n\n### Streaming JSON\n\n#### Autodetection of streaming JSON\n\nA number of hints can be used to determine if the data is formatted using a streaming JSON format\n\n- if the filename extension is `.jsonl`\n- if the MIMETYPE is `application/json-seq`\n- if the first value in the file is a number, assume the file is length prefixed.\n\nFor data in non-streaming JSON format, the presence of a top-level array will start streaming of objects.\n\nFor embedded arrays, a path specifier may need to be supplied (or could look for first array).\n\n#### Streaming JSON Formats\n\n- Overview of [JSON Streaming Formats](https://en.wikipedia.org/wiki/JSON_streaming) (Wikipedia).\n\n- [Line-delimited JSON](http://jsonlines.org/) (LDJSON) (aka JSON lines) (JSONL).\n- [NewLine delimited JSON](https://github.com/ndjson/ndjson-spec)\n- [Record separator-delimited JSON](https://tools.ietf.org/html/rfc7464) (IETF RFC 7464) (aka Json Text Sequences).\n- Concatenated JSON\n- Length-prefixed JSON\n\n#### Streaming Options\n\n- `streaming`: `auto` - assume the file contains multiple top level JSON objects (or a top-level array of objects).\n- `lengthPrefixed`: `auto` - if the first value in the file is a number, assumes the file is length prefixed.\n","slug":"modules/json/docs","title":"JSONLoader (Experimental)"},{"excerpt":"GLBLoader The  parses a GLB binary \"envelope\". Note: applications that want to parse GLB-formatted glTF files use the  instead. The  is…","rawMarkdownBody":"# GLBLoader\n\nThe `GLBLoader` parses a GLB binary \"envelope\".\n\nNote: applications that want to parse GLB-formatted glTF files use the `GLTFLoader` instead. The `GLBLoader` is intended to be used to load custom data that combines JSON and binary resources.\n\n| Loader          | Characteristic                                                                                          |\n| --------------- | ------------------------------------------------------------------------------------------------------- |\n| File Extensions | `.glb`                                                                                                  |\n| File Type       | Binary                                                                                                  |\n| File Format     | [GLB](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#glb-file-format-specification) |\n| Data Format     | See below                                                                                               |\n| Supported APIs  | `load`, `parse`, `parseSync`                                                                            |\n|                 |\n\n## Usage\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {GLBLoader} from '@loaders.gl/gltf';\nconst gltf = await load(url, GLBLoader);\n```\n\n## Options\n\n| Option                    | Type    | Default | Description                                                  |\n| ------------------------- | ------- | ------- | ------------------------------------------------------------ |\n| `glb.strict` (DEPRECATED) | Boolean | `false` | Whether to support non-standard JSON/BIN chunk type numbers. |\n\n## Data Format\n\nReturns\n\n```json\n{\n  \"header\": {\n    \"byteLength\": number,\n    \"byteOffset\": number\n  },\n\n  \"type\": string,\n  \"version\": number,\n\n  // JSON Chunk\n  \"json\": any,\n\n  // BIN Chunk\n  \"hasBinChunk\": boolean,\n  \"binChunks\": [\n    {\n      \"arrayBuffer\": ArrayBuffer,\n      \"byteOffset\": Number,\n      \"byteLength\": Number\n    }\n  ]\n}\n```\n\n| Field                       | Type          | Default | Description                                          |\n| --------------------------- | ------------- | ------- | ---------------------------------------------------- |\n| `type`                      | `String`      | `glTF`  | String containing the first four bytes of the file   |\n| `version`                   | `Number`      | `2`     | The version number, only version 2 is supported      |\n| `json`                      | `Object`      | `{}`    | Parsed JSON from the JSON chunk                      |\n| `binChunks`                 | `ArrayBuffer` | `null`  | The binary chunk                                     |\n| `binChunks[\\*].arrayBuffer` | `ArrayBuffer` | `null`  | The binary chunk                                     |\n| `binChunks[\\*].byteOffset`  | `Number`      | `null`  | offset of BIN (e.g. embedded in larger binary block) |\n| `binChunks[\\*].byteLength`  | `ArrayBuffer` | `null`  | length of BIN (e.g. embedded in larger binary block) |\n| `header.byteLength`         | `Number`      | -       | length of GLB (e.g. embedded in larger binary block) |\n| `header.byteOffset`         | `Number`      | 0       | offset of GLB (e.g. embedded in larger binary block) |\n","slug":"modules/gltf/docs/api-reference/glb-loader","title":"GLBLoader"},{"excerpt":"GLBWriter The  is a writer for the GLB binary \"envelope\". Note: applications that want to encode GLB-formatted glTF files use the  instead…","rawMarkdownBody":"# GLBWriter\n\nThe `GLBWriter` is a writer for the GLB binary \"envelope\".\n\nNote: applications that want to encode GLB-formatted glTF files use the `GLTFWriter` instead. The `GLBWriter` is intended to be used to save custom data that combines JSON and binary resources.\n\n| Loader          | Characteristic                                                                                          |\n| --------------- | ------------------------------------------------------------------------------------------------------- |\n| File Extensions | `.glb`                                                                                                  |\n| File Type       | Binary                                                                                                  |\n| Data Format     | See below                                                                                               |\n| File Format     | [GLB](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0#glb-file-format-specification) |\n| Supported APIs  | `encode`, `encodeSync`                                                                                  |\n\n## Usage\n\n```js\nimport {GLBWriter} from '@loaders.gl/gltf';\nimport {encodeSync} from '@loaders.gl/core';\n\nconst arrayBuffer = encodeSync(gltf, GLBWriter, options);\n```\n\n## Options\n\n| Option | Type | Default | Description |\n| ------ | ---- | ------- | ----------- |\n| N/A    | N/A  | N/A     | N/A         |\n\n## Data Format\n\nSee [`GLBLoader`](/modules/gltf/docs/api-reference/glb-loader.md).\n","slug":"modules/gltf/docs/api-reference/glb-writer","title":"GLBWriter"},{"excerpt":"glTF Extensions glTF extensions can be present in glTF files, and will be present in the parsed JSON. glTF extensions can supported by…","rawMarkdownBody":"# glTF Extensions\n\nglTF extensions can be present in glTF files, and will be present in the parsed JSON. glTF extensions can supported by applications by inspecting the `extensions` fields inside glTF objects, and it is up to each application to handle or ignore them.\n\nloaders.gl aims to provide support for glTF extensions that can be handled completely or partially during loading, and article describes glTF extensions that are fully or partially processed by the `@loaders.gl/gltf` classes.\n\nNote that many glTF extensions affect aspects that are firmly outside of the scope of loaders.gl (e.g. rendering), and no attempt is made to process those extensions in loaders.gl.\n\n| Extension                                                                                                                        | Description |\n| -------------------------------------------------------------------------------------------------------------------------------- | ----------- |\n| [KHR_draco_mesh_compression](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_draco_mesh_compression) |             |\n| [KHR_lights_punctual](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_lights_punctual)               |             |\n| [KHR_materials_unlit](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_materials_unlit)               |             |\n\n## Official Extensions\n\n### KHR_draco_mesh_compression\n\nSupports compression of mesh attributes (geometry).\n\nSpecification: [KHR_draco_mesh_compression](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_draco_mesh_compression).\n\nParsing Support:\n\n- By adding the `decompress: true` options to the `GLTFParser` any decompressed by the `GLTFParser`.\n- The expanded attributes are placed in the mesh object (effectively making it look as if it had never been compressed).\n- The extension objects are removed from the glTF file.\n\nEncoding Support:\n\n- Meshes can be compressed as they are added to the `GLTFBuilder`.\n\n### KHR_lights_punctual\n\nSupports specification of point light sources and addition of such sources to the scenegraph node.\n\nSpecification: [KHR_lights_punctual](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_lights_punctual)\n\nParsing Support:\n\n- Any nodes with a `KHR_lights_punctual` extension will get a `light` field with value containing a light definition object with properties defining the light (this object will be resolved by index from the global `KHR_lights_punctual` extension object's `lights` array) .\n- The `KHR_lights_punctual` extensions will be removed from all nodes.\n- Finally, the global `KHR_lights_punctual` extension (including its light list)) will be removed.\n\nEncoding Support:\n\n- N/A\n\n### KHR_materials_unlit\n\nSpecifies that a material should not be affected by light. Useful for pre-lit materials (e.g. photogrammetry).\n\n[KHR_materials_unlit](https://github.com/KhronosGroup/glTF/tree/master/extensions/2.0/Khronos/KHR_materials_unlit)\n\n## Custom Extensions\n\n### UBER_draco_point_cloud_compression\n\nSpecification: Similar to `KHR_draco_mesh_compression`, but supports point clouds (draw mode 0). Also does not support any fallback or non-compressed accessors/attributes.\n\nParsing support:\n\n- The primitive's accessors field will be populated after decompression.\n- After decompression, the extension will be removed (as if the point cloud was never compressed).\n\nEncoding support:\n\n- Point clouds can be compressed as they are added to the `GLTFBuilder` and decompressed by the `GLTFParser`.\n","slug":"modules/gltf/docs/api-reference/gltf-extensions","title":"glTF Extensions"},{"excerpt":"glbdump  is a utility for inspecting the structure of GLB/glTF binary container files. Installing loaders.gl/gltf makes the  command line…","rawMarkdownBody":"## glbdump\n\n`glbdump` is a utility for inspecting the structure of GLB/glTF binary container files.\n\nInstalling loaders.gl/gltf makes the `glbdump` command line tool available. It can be run using `npx`.\n\n```\n$ npx glbdump <filename>\n```\n","slug":"modules/gltf/docs/api-reference/glbdump","title":" glbdump"},{"excerpt":"GLTFScenegraph The  class provides an API for accessing and modifying glTF data. Caveat: Modification of binary data chunks has limitations…","rawMarkdownBody":"# GLTFScenegraph\n\nThe `GLTFScenegraph` class provides an API for accessing and modifying glTF data.\n\n> Caveat: Modification of binary data chunks has limitations, and this class is currently not intended to be a generic utility for modifying glTF data.\n\n## Usage\n\n```js\nimport {GLTFLoader, GLTFScenegraph} from '@loaders.gl/gltf';\nimport {load} from '@loaders.gl/core';\n\n// Load and parse a file\nconst gltfData = await parse(fetch(GLTF_URL), GLTFLoader);\n\n// Create a parser\nconst gltf = new GLTFScenegraph(gltfData);\n\n// Get the complete glTF JSON structure\nconst gltfJson = gltf.getJSON();\n\n// Get specific top-level fields from the glTF JSON chunk\nconst appData = gltf.getApplicationData('customData');\n\n// Get a top level extension from the glTF JSON chunk\nconst topLevelExtension = gltf.getExtension('ORGNAME_extensionName');\nif (topLevelExtension) {\n  ...\n}\n\n// Get images from the binary chunk (together with metadata)\nconst imageIndex = 0;\nconst image = gltf.getImage(imageIndex);\n\n// Get default glTF scenegraph\nconst scenegraph = gltf.getScene();\n// Get specific glTF scenegraph\nconst scenegraph = gltf.getScene(2);\n```\n\n## Accessor Methods\n\n### constructor(gltf : Object)\n\nCreates a new `GLTFScenegraph` instance from a pure JavaScript object.\n\n### json()\n\n### getApplicationData(key : String) : Object\n\nReturns the given data field in the top-level glTF JSON object.\n\n### getExtraData(key : String) : Object?\n\nReturns a key in the top-level glTF `extras` JSON object.\n\n### getExtension(name : String) : Object?\n\nReturns the top-level extension by `name`, if present.\n\n### getUsedExtensions() : String[]\n\nReturns an array of extension names (covering all extensions used at any level of the glTF hierarchy).\n\n### getRequiredExtensions() : String[]\n\nReturns an array of extensions at any level of the glTF hierarchy that are required to properly display this file (covering all extensions used at any level of the glTF hierarchy).\n\n### getObjectExtension(object, extensionName)\n\n### getScene([index : Number]) : Object?\n\nReturns the scene (scenegraph) with the given index, or the default scene if no index is specified.\n\n### getScene(index : Number) : Object\n\n### getNode(index : Number) : Object\n\n### getSkin(index : Number) : Object\n\n### getMesh(index : Number) : Object\n\n### getMaterial(index : Number) : Object\n\n### getAccessor(index : Number) : Object\n\n### getCamera(index : Number) : Object\n\n### getTexture(index : Number) : Object\n\n### getSampler(index : Number) : Object\n\n### getImage(index : Number) : Object\n\nReturns the image with specified index\n\n### getBufferView(index : Number) : Object\n\n### getBuffer(index : Number) : Object\n\n### getTypedArrayForBufferView(bufferView : Number | Object) : Uint8Array\n\nAccepts buffer view index or buffer view object\n\n### getTypedArrayForAccessor(accessor : Number | Object) : Uint8Array | Float32Array | ...\n\nAccepts accessor index or accessor object.\n\nReturns a typed array with type that matches the types\n\n### getTypedArrayForImageData(image : Number | Object) : Uint8Array\n\naccepts accessor index or accessor object\n\n## Modifiers\n\n### addApplicationData(key, data)\n\nAdd an extra application-defined key to the top-level data structure\n\n### addExtraData(key, data)\n\n`extras` - Standard GLTF field for storing application specific data\n\nAdd to GLTF top level extension object, mark as used\n\n### addRequiredExtension(extensionName, data)\n\nAdd GLTF top level extension object, mark as used and required\n\n### registerUsedExtension(extensionName)\n\nAdd extensionName to list of used extensions\n\n### registerRequiredExtension(extensionName)\n\nAdd extensionName to list of required extensions\n\n### removeExtension(extensionName)\n\nRemoves an extension from the top-level list\n\n### setObjectExtension(object, extensionName, data)\n\n### addMesh(attributes, indices, mode = 4)\n\n### addPointCloud(attributes)\n\n### addBufferView(buffer)\n\nAdd one untyped source buffer, create a matching glTF `bufferView`, and return its index\n\n> The binary data will not be added to the gltf buffer until `createBinChunk()` is called.\n\n### addAccessor(bufferViewIndex, accessor)\n\nAdds an accessor to a bufferView\n\n> The binary data will not be added to the gltf buffer until `createBinChunk()` is called.\n\n### addImage(imageData, mimeType)\n\nAdds a binary image. Builds glTF \"JSON metadata\" and saves buffer reference\nBuffer will be copied into BIN chunk during \"pack\"\n\n> The binary data will not be added to the gltf buffer until `createBinChunk()` is called.\n\n### createBinChunk()\n\nPacks any pending binary data into the first binary glTF buffer.\n\nNote: Overwrites the existing first buffer if present.\n","slug":"modules/gltf/docs/api-reference/gltf-scenegraph","title":"GLTFScenegraph"},{"excerpt":"GLTFWriter The  is a writer for glTF scenegraphs. Loader Characteristic File Extensions , File Types Binary, JSON, Linked Assets Data Format…","rawMarkdownBody":"# GLTFWriter\n\nThe `GLTFWriter` is a writer for glTF scenegraphs.\n\n| Loader          | Characteristic                                                             |\n| --------------- | -------------------------------------------------------------------------- |\n| File Extensions | `.glb`,`.gltf`                                                             |\n| File Types      | Binary, JSON, Linked Assets                                                |\n| Data Format     | [Scenegraph](/docs/specifications/category-scenegraph)                     |\n| File Format     | [glTF](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0) |\n| Supported APIs  | `encode`, `encodeSync`                                                     |\n\n## Usage\n\n```js\nimport {GLTFWriter} from '@loaders.gl/gltf';\nimport {encodeSync} from '@loaders.gl/core';\n\nconst arrayBuffer = encodeSync(gltf, GLTFWriter, options);\n```\n\n## Options\n\n| Option        | Type                                                  | Default | Description                                                                                   |\n| ------------- | ----------------------------------------------------- | ------- | --------------------------------------------------------------------------------------------- |\n| `DracoWriter` | [DracoWriter](/docs/api-reference/draco/draco-writer) | `null`  | To enable DRACO encoding, the application needs to import and supply the `DracoWriter` class. |\n| `DracoLoader` | [DracoLoader](/docs/api-reference/draco/draco-loader) | `null`  | To enable DRACO encoding, the application needs to import and supply the `DracoLoader` class. |\n","slug":"modules/gltf/docs/api-reference/gltf-writer","title":"GLTFWriter"},{"excerpt":"postProcessGLTF The  function transforms parsed GLTF JSON to make it easier to use. It adds loaded buffers and images to the glTF JSON…","rawMarkdownBody":"# postProcessGLTF\n\nThe `postProcessGLTF` function transforms parsed GLTF JSON to make it easier to use.\n\n- It adds loaded buffers and images to the glTF JSON objects\n- It creates typed arrays for buffer views\n\n## Usage\n\nPostprocessing is done by default by the `GLTFLoader`:\n\n```js\nimport {GLTFLoader} from '@loaders.gl/gltf';\nconst processedGLTF = await parse(..., GLTFLoader,);\n```\n\nTo turn post processing off, and then optionally post process via `postProcessGLTF` function:\n\n```js\nimport {GLTFLoader, postProcessGLTF} from '@loaders.gl/gltf';\nconst gltf = await parse(..., GLTFLoader, {gltf: {postProcess: false}});\nconst processedGLTF = postProcessGLTF(gltf);\n```\n\nAfter post-processing, the gltf scenegraphs are now easier to iterate over as indices have been resolved to object references:\n\n```js\nconst scenegraph = processedGLTF.scenegraphs[0];\nfor (const node of scenegraph.nodes) {\n  // no need to resolve indices\n  if (node.mesh.primitives) {\n    // Ditto\n    // ...\n  }\n}\n```\n\n## Functions\n\n### postProcessGLTF(gltf : Object, options? : Object) : Object\n\n- `gltf` is expected to have `json` and `buffers` fields per the GLTF Data Format Category.\n- `options.uri` - Set base URI (for image loading)\n\nThe GLTF post processor copies objects in the input gltf json field as necessary to avoid modifying the input JSON, but does not do a deep copy on sub-objects that do not need to be modified.\n\n## General Post Processing\n\n### Replace indices with references\n\nThe first thing that `postProcessGLTF` does is replace glTF indices with object references to simplify iteration over the scenegraph.\n\nBackground: The GLTF file format describes a tree structure, however it links nodes through numeric indices rather than direct references. (As an example the `nodes` field in the top-level glTF `scenegraph` array is an array of indices into the top-level `nodes` array. Each node has a `mesh` attribute that is an index into to the `meshes` array, and so on).\n\n### Adds `id` to every node\n\nThe postprocessor makes sure each node and an `id` value, unless already present.\n\n## Node Specific Post Processing\n\n### Buffers\n\nThe following fields will be populated from the supplied `gltf.buffers` parameter (this parameter is populated by the loader via `options.loadLinkedResources: true`):\n\n- `buffer.arrayBuffer` -\n- `buffer.byteOffset` -\n- `buffer.byteLength` -\n\n### BufferViews\n\n- `bufferView.data` - Typed arrays (`Uint8Arrays`) will be created for buffer views and stored in this field. These typed arrays can be used to upload data to WebGL buffers.\n\n### Accessors\n\nThe accessor parameters which are textual strings in glTF will be resolved into WebGL constants (which are just numbers, e.g. `5126` = `GL.FLOAT`), to prepare for use with WebGL frameworks.\n\n- `accessor.value` - This will be set to a typed array that is a view into the underlying bufferView.\n\nRemarks:\n\n- While it can be very convenient to initialize WebGL buffers from `accessor.value`, this approach will defeat any memory sharing on the GPU that the glTF file specifies through accessors sharing `bufferViews`. The canonical way of instantitating a glTF model is for an application to create one WebGL buffer for each `bufferView` and then use accessors to reference data chunks inside those WebGL buffers with `offset` and `stride`.\n\n## Images\n\n- `image.image` - Populated from the supplied `gltf.images` array. This array is populated by the `GLTFLoader` via `options.loadImages: true`):\n- `image.uri` - If loaded image in the `images` array is not available, uses `gltf.baseUri` or `options.baseUri` is available, to resolve a relative URI and replaces this value.\n\n### Materials\n\n- `...texture` - Since each texture object in the material has an `...index` field next to other fields, the post processor will add a `...texture` field instead of replacing the `...index` field.\n\n### Samplers\n\nModifies\n\n- `parameters` - see table\n\nSampler parameters (which are textual in glTF) will be resolved into WebGL constants.\n\n| glTF constant | WebGL constant          |\n| ------------- | ----------------------- |\n| `magFilter`   | `GL.TEXTURE_MAG_FILTER` |\n| `minFilter`   | `GL.TEXTURE_MIN_FILTER` |\n| `wrapS`       | `GL.TEXTURE_WRAP_S`     |\n| `wrapT`       | `GL.TEXTURE_WRAP_T`     |\n\n### Texture\n\nModifies\n\n- `sampler` - will be resolved the the corresponding image object.\n- `source` - will be resolved the the corresponding image object.\n","slug":"modules/gltf/docs/api-reference/post-process-gltf","title":"postProcessGLTF"},{"excerpt":"GLTFLoader Parses a glTF file. Can load both the  (binary) and  (text/json) file format variants. A glTF file contains a hierarchical…","rawMarkdownBody":"# GLTFLoader\n\nParses a glTF file. Can load both the `.glb` (binary) and `.gltf` (text/json) file format variants.\n\nA glTF file contains a hierarchical scenegraph description that can be used to instantiate corresponding hierarcy of actual `Scenegraph` related classes in most WebGL libraries.\n\n| Loader          | Characteristic                                                             |\n| --------------- | -------------------------------------------------------------------------- |\n| File Extensions | `.glb`, `.gltf`                                                            |\n| File Type       | Binary, JSON, Linked Assets                                                |\n| File Format     | [glTF](https://github.com/KhronosGroup/glTF/tree/master/specification/2.0) |\n| Data Format     | [Scenegraph](/docs/specifications/category-scenegraph)                     |\n| Supported APIs  | `load`, `parse`, `parseSync`                                               |\n\n## Usage\n\n```\nimport {load} from '@loaders.gl/core';\nimport {GLTFLoader} from '@loaders.gl/gltf';\nconst gltf = await load(url, GLTFLoader);\n```\n\nTo decompress Draco-compressed meshes:\n\n```\nimport {load} from '@loaders.gl/core';\nimport {GLTFLoader} from '@loaders.gl/gltf';\nimport {DracoLoader} from '@loaders.gl/draco';\nconst gltf = load(url, GLTFLoader, {DracoLoader, decompress: true});\n```\n\n## Overview\n\nThe `GLTFLoader` aims to take care of as much processing as possible, while remaining framework-independent.\n\nThe GLTF Loader returns an object with a `json` field containing the glTF Scenegraph. In its basic mode, the `GLTFLoader` does not modify the loaded JSON in any way. Instead, the results of additional processing are placed in parallel top-level fields such as `buffers` and `images`. This ensures that applications that want to work with the standard glTF data structure can do so.\n\nOptionally, the loaded gltf can be \"post processed\", which lightly annotates and transforms the loaded JSON structure to make it easier to use. Refer to [postProcessGLTF](docs/api-reference/gltf-loaders/gltf-extensions.md) for details.\n\nIn addition, certain glTF extensions, in particular Draco mesh encoding, can be fully or partially processed during loading. When possible (and extension processing is enabled), such extensions will be resolved/decompressed and replaced with standards conformant representations. See [glTF Extensions](docs/api-reference/gltf-loaders/gltf-extensions.md) for more information.\n\nNote: while supported, synchronous parsing of glTF (e.g. using `parseSync()`) has significant limitations. When parsed asynchronously (using `await parse()` or `await load()`), the following additional capabilities are enabled:\n\n- linked binary resource URI:s will be loaded and resolved (assuming a valid base url is available).\n- base64 encoded binary URI:s inside the JSON payload will be decoded.\n- linked image URI:s can be loaded and decoded.\n- Draco meshes can be decoded asynchronously on worker threads (in parallel!).\n\n## Options\n\n| Option             | Type    | Default |                                                                                | Description |\n| ------------------ | ------- | ------- | ------------------------------------------------------------------------------ | ----------- |\n| `gltf.fetchImages` | Boolean | `false` | Fetch any referenced image files (and decode base64 encoded URIS). Async only. |\n| `gltf.parseImages` | Boolean | `false` |\n| `gltf.decompress`  | Boolean | `true`  | Decompress Draco compressed meshes (if DracoLoader available).                 |\n| `gltf.postProcess` | Boolean | `true`  | Perform additional post processing on the loaded glTF data.                    |\n\nRemarks:\n\n- The `gltf.postProcess` option activates additional [post processing](docs/api-reference/post-process-gltf) that transforms parts of JSON structure in the loaded glTF data, to make glTF data easier use in applications and WebGL libraries, however this changes the format of the data returned by the `GLTFLoader`.\n\n## Data Format\n\n### With Post Processing\n\nWhen the `GLTFLoader` is called with `gltf.postProcess` option set to `true` (the default),the parsed JSON chunk will be returned, and [post processing](docs/api-reference/post-process-gltf) will have been performed, which will link data from binary buffers into the parsed JSON structure using non-standard fields, and also modify the data in other ways to make it easier to use.\n\nAt the top level, this will look like a standard glTF JSON structure:\n\n```json\n{\n  scenes: [...],\n  scene: ...,\n  nodes: [...],\n  ...\n}\n```\n\nHowever, the objects inside these arrays will have been pre-processed to simplify usage. For details on changes and extra fields added to the various glTF objects, see [post processing](docs/api-reference/post-process-gltf).\n\n### Without Post Processing\n\nBy setting `gltf.postProcess` to `false`, an unprocessed glTF/GLB data structure will be returned, with binary buffers provided as an `ArrayBuffer` array.\n\n```json\n{\n  // The base URI used to load this glTF, if any. For resolving relative uris to linked resources.\n  baseUri: String,\n\n  // JSON Chunk\n  json: Object, // Containse the parsed glTF JSON or the parsed GLB JSON chunk\n\n  // Length and indices of this array will match `json.buffers`\n  // The GLB bin chunk, if present, will be found in buffer 0.\n  // Additional glTF json `buffers` are fetched and base64 decoded from the JSON uri:s.\n  buffers: [{\n    arrayBuffer: ArrayBuffer,\n    byteOffset: Number,\n    byteLength: Number\n  }],\n\n  // Images can optionally be loaded and decoded, they will be stored here\n  // Length and indices of this array will match `json.buffers`\n  images: Image[],\n\n  // GLBLoader output, if this was a GLB encoded glTF\n  _glb?: Object\n}\n```\n\n| Field                     | Type          | Default                                                   | Description                                                      |\n| ------------------------- | ------------- | --------------------------------------------------------- | ---------------------------------------------------------------- |\n| `baseUri`                 | `String`      | `` | length of GLB (e.g. embedded in larger binary block) |\n| `json`                    | `Object`      | `{}`                                                      | Parsed JSON from the JSON chunk                                  |\n| `buffers`                 | `Object[]`    | `[]`                                                      | The version number                                               |\n| `buffers[\\*].arrayBuffer` | `ArrayBuffer` | `null`                                                    | The binary chunk                                                 |\n| `buffers[\\*].byteOffset`  | `Number`      | `null`                                                    | offset of buffer (embedded in larger binary block)               |\n| `buffers[\\*].byteLength`  | `ArrayBuffer` | `null`                                                    | length of buffer (embedded in larger binary block)               |\n| `_glb`?                   | `Object`      | N/A                                                       | The output of the GLBLoader if the parsed file was GLB formatted |\n","slug":"modules/gltf/docs/api-reference/gltf-loader","title":"GLTFLoader"},{"excerpt":"DracoLoader The  decodes a mesh or point cloud (maps of attributes) using DRACO compression. Loader Characteristic File Extension  File Type…","rawMarkdownBody":"# DracoLoader\n\nThe `DracoLoader` decodes a mesh or point cloud (maps of attributes) using [DRACO](https://google.github.io/draco/) compression.\n\n| Loader         | Characteristic                               |\n| -------------- | -------------------------------------------- |\n| File Extension | `.drc`                                       |\n| File Type      | Binary                                       |\n| File Format    | [Draco](https://google.github.io/draco/)     |\n| Data Format    | [Mesh](docs/specifications/category-mesh.md) |\n| Supported APIs | `parse`, `parseSync`                         |\n\n## Usage\n\n```js\nimport {DracoLoader} from '@loaders.gl/draco';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, DracoLoader, options);\n```\n\n## Options\n\n| Option | Type | Default | Description |\n| ------ | ---- | ------- | ----------- |\n\n\n## Dependencies\n\nDraco libraries by default are loaded from CDN, but can be bundled and injected. See [modules/draco/docs] for details.\n","slug":"modules/draco/docs/api-reference/draco-loader","title":"DracoLoader"},{"excerpt":"DracoWriter The  encodes a mesh or point cloud (maps of attributes) using Draco3D compression. Loader Characteristic File Extension  File…","rawMarkdownBody":"# DracoWriter\n\nThe `DracoWriter` encodes a mesh or point cloud (maps of attributes) using [Draco3D](https://google.github.io/draco/) compression.\n\n| Loader                | Characteristic                               |\n| --------------------- | -------------------------------------------- |\n| File Extension        | `.drc`                                       |\n| File Typoe            | Binary                                       |\n| Data Format           | [Mesh](docs/specifications/category-mesh.md) |\n| File Format           | [Draco](https://google.github.io/draco/)     |\n| Encoder Type          | Synchronous                                  |\n| Worker Thread Support | Yes                                          |\n| Streaming Support     | No                                           |\n\n## Usage\n\n```js\nimport {DracoWriter} from '@loaders.gl/draco';\nimport {encode} from '@loaders.gl/core';\n\nconst mesh = {\n  attributes: {\n    POSITION: {...}\n  }\n};\n\nconst data = await encode(mesh, DracoWriter, options);\n```\n\n## Options\n\n| Option       | Type             | Default                     | Description                                                        |\n| ------------ | ---------------- | --------------------------- | ------------------------------------------------------------------ |\n| `pointcloud` | Boolean          | `false`                     | set to `true` to compress pointclouds (mode=`0` and no `indices`). |\n| `method`     | String           | `MESH_EDGEBREAKER_ENCODING` | set Draco encoding method (applies to meshes only).                |\n| `speed`      | [Number, Number] | set Draco speed options.    |\n| `log`        | Function         | callback for debug info.    |\n","slug":"modules/draco/docs/images/draco-writer","title":"DracoWriter"},{"excerpt":"Image Accessors Unless specified via , the actual JavaScript type of parsed images returned by the  depends on whether you are running in a…","rawMarkdownBody":"# Image Accessors\n\nUnless specified via `options.type`, the actual JavaScript type of parsed images returned by the [`ImageLoader`](modules/images/docs/api-reference/image-loader.md) depends on whether you are running in a newer or older browser, or under Node.js.\n\nTo simplify writing cross-platform image handling code that is optimized on newer browsers and still works on older browsers and on Node.js, a set of image accessor functions are provided.\n\n## Usage\n\nTo get width, height and pixel data from an image returned by the `ImageLoader` in a cross-platform way:\n\n```js\nimport {ImageLoader, getImageSize, getImageData} from `@loaders.gl/images`;\nimport {load} from `@loaders.gl/core`;\n\nconst image = await load(URL, ImageLoader);\n\nconst {width, height} = getImageSize(image);\nconst pixelArray = getImageData(image);\n```\n\n### isImageTypeSupported(type : string) : boolean\n\n- `type`: value to test\n\nReturns `true` if `type` is one of the types that `@loaders.gl/images` can use on the current platform (depends on browser, or whether running under Node.js).\n\n### isImage(image : any) : boolean\n\n- `image`: value to test\n\nReturns `true` if `image` is one of the types that `@loaders.gl/images` can return.\n\n### getImageType(image : any) : String\n\nReturns the type of an image. Can be used when loading images with the default setting of `options.type: 'auto'` to discover what type was actually returned.\n\n- `image`: value to test\n\nReturns\n\n- a string describing the type of the image.\n\nThrows\n\n- if `image` is not of a recognized type.\n\n| Type      | JavaScript Type                                         | Description                                        |\n| --------- | ------------------------------------------------------- | -------------------------------------------------- |\n| `bitmap`  | `ImageBitmap`                                           | The newer HTML5 image class (modern browsers only) |\n| `html`    | `Image` aka `HTMLImageElement`                          | The older, less flexible HTML image element        |\n| `ndarray` | `Object` of ndarray shape: `data`, `width`, `height` .. | Node.js representation                             |\n\n### getImageSize(image : any) : Object\n\n- `image`: an image instance\n\nReturns\n\n- And object `{widht, height}` describing the size of the image\n\nThrows\n\n- if `image` is not of a recognized type.\n\n### getImageData(image : any) : TypedArray\n\n- `image`: an image instance\n\nReturns\n\n- And typed array containing the pixels of the image\n\nThrows\n\n- if `image` is not of a recognized type.\n","slug":"modules/images/docs/api-reference/parsed-image-api","title":"Image Accessors"},{"excerpt":"DracoWriter The  encodes a mesh or point cloud using Draco compression. Loader Characteristic File Extension  File Type Binary File Format…","rawMarkdownBody":"# DracoWriter\n\nThe `DracoWriter` encodes a mesh or point cloud using [Draco](https://google.github.io/draco/) compression.\n\n| Loader         | Characteristic                               |\n| -------------- | -------------------------------------------- |\n| File Extension | `.drc`                                       |\n| File Type      | Binary                                       |\n| File Format    | [Draco](https://google.github.io/draco/)     |\n| Data Format    | [Mesh](docs/specifications/category-mesh.md) |\n| Support API    | `encode`, `encodeSync`                       |\n\n## Usage\n\n```js\nimport {DracoWriter} from '@loaders.gl/draco';\nimport {encode} from '@loaders.gl/core';\n\nconst data = encode(url, DracoWriter, options);\n```\n\n## Options\n\n| Option               | Type               | Default | Description                                                                         |\n| -------------------- | ------------------ | ------- | ----------------------------------------------------------------------------------- |\n| `draco.pointcloud`   | `Boolean`          | `false` | Whether to compress as point cloud (GL.POINTS)                                      |\n| `draco.speed`        | `Number`           |         | Speed vs Quality, see [Draco](https://google.github.io/draco/) documentation        |\n| `draco.method`       | `String`           |         | Compression method, see [Draco](https://google.github.io/draco/) documentation      |\n| `draco.quantization` | `[Number, Number]` |         | Quantization parameters, see [Draco](https://google.github.io/draco/) documentation |\n\n## Dependencies\n\nDraco libraries by default are loaded from CDN, but can be bundled and injected. See [modules/draco/docs] for details.\n","slug":"modules/draco/docs/api-reference/draco-writer","title":"DracoWriter"},{"excerpt":"encode Functions encode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object , url : String]) : Promise.Any Encodes…","rawMarkdownBody":"# encode\n\n## Functions\n\n### encode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object [, url : String]]) : Promise.Any\n\nEncodes data asynchronously using the provided writer.\n\n- `data` - loaded data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, options for the writer (see documentation of the specific writer).\n- `url` - optional, assists in the autoselection of a writer if multiple writers are supplied to `writer`.\n\n- `options.log`=`console` Any object with methods `log`, `info`, `warn` and `error`. By default set to `console`. Setting log to `null` will turn off logging.\n\n### encodeSync(fileData : ArrayBuffer | String, writer : Object | Array, [, options : Object [, url : String]]) : any\n\nEncodes data synchronously using the provided writer, if possible. If not, returns `null`, in which case asynchronous loading is required.\n\n- `data` - loaded data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, options for the writer (see documentation of the specific writer).\n- `url` - optional, assists in the autoselection of a writer if multiple writers are supplied to `writer`.\n","slug":"modules/core/docs/api-reference/encode","title":"encode"},{"excerpt":"fetchProgress This function is still experimental A function that tracks a fetch response object and calls  callbacks. Usage _fetchProgress…","rawMarkdownBody":"# fetchProgress\n\n> This function is still experimental\n\nA function that tracks a fetch response object and calls `onProgress` callbacks.\n\n## Usage\n\n```js\nimport {_fetchProgress} from '@loaders.gl/core';\n\nfunction onProgress(percent, {loadedBytes, totalBytes}) {\n  console.log(`${percent}% ${Math.round(loadedBytes/1000)} of ${Math.round(totalBytes/1000)} Kbytes`);\n}\n\nasync function main() {\n  const response = await _fetchProgress(fetch(PROGRESS_IMAGE_URL, onProgress),\n  const data = await response.arrayBuffer();\n  // At this point, onProgress will have been called one or more times.\n  ...\n}\n```\n\n## \\_fetchProgress(response : Response | Promise, onProgress : function, onDone : function, onError : function) : Response\n\n`onProgress: (percent: number, {loadedBytes : number, totalBytes : number}) => void`\n","slug":"modules/core/docs/api-reference/fetch-progress","title":"fetchProgress"},{"excerpt":"fetchFile The  function is a wrapper around  which provides support for path prefixes and some additional loading capabilities. Usage Use…","rawMarkdownBody":"# fetchFile\n\nThe `fetchFile` function is a wrapper around `fetch` which provides support for path prefixes and some additional loading capabilities.\n\n## Usage\n\nUse the `fetchFile` function as follows:\n\n```js\nimport {fetchFile} from '@loaders.gl/core';\n\nconst response = await fetchFile(url);\n\n// Now use standard browser Response APIs\n\n// Note: headers are case-insensitive\nconst contentLength = response.headers.get('content-length');\nconst mimeType = response.headers.get('content-type');\n\nconst arrayBuffer = await response.arrayBuffer();\n```\n\nThe `Response` object from `fetchFile` is usually passed to `parse` as follows:\n\n```js\nimport {fetchFile, parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\nconst data = await parse(fetchFile(url), OBJLoader);\n```\n\nNote that if you don't need the extra features in `fetchFile`, you can just use the browsers built-in `fetch` method.\n\n```js\nimport {parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\nconst data = await parse(fetch(url), OBJLoader);\n```\n\n## Functions\n\n### fetchFile(url : String [, options : Object]) : Promise.Response\n\nA wrapper around the platform [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/fetch) function with some additions:\n\n- Supports `setPathPrefix`: If path prefix has been set, it will be appended if `url` is relative (e.g. does not start with a `/`).\n- Supports `File` and `Blob` objects on the browser (and returns \"mock\" fetch response objects).\n\nReturns:\n\n- A promise that resolves into a fetch [`Response`](https://developer.mozilla.org/en-US/docs/Web/API/Response) object, with the following methods/fields:\n  - `headers`: `Headers` - A [`Headers`](https://developer.mozilla.org/en-US/docs/Web/API/Headers) object.\n  - `arrayBuffer()`: Promise.ArrayBuffer`- Loads the file as an`ArrayBuffer`.\n  - `text()`: Promise.String` - Loads the file and decodes it into text.\n  - `json()`: Promise.String` - Loads the file and decodes it into JSON.\n  - `body` : ReadableStream` - A stream that can be used to incrementally read the contents of the file.\n\nOptions:\n\nUnder Node.js, options include (see [fs.createReadStream](https://nodejs.org/api/fs.html#fs_fs_createreadstream_path_options)):\n\n- `options.highWaterMark` (Number) Default: 64K (64 \\* 1024) - Determines the \"chunk size\" of data read from the file.\n\n### readFileSync(url : String [, options : Object]) : ArrayBuffer | String\n\n> This function only works on Node.js or using data URLs.\n\nReads the raw data from a file asynchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n## Remarks\n\n- `fetchFile` will delegate to `fetch` after resolving the URL.\n- For some data sources such as node.js and `File`/`Blob` objects a mock `Response` object will be returned, and not all fields/members may be implemented.\n- When possible, `Content-Length` and `Content-Type` `headers` are also populated for non-request data sources including `File`, `Blob` and Node.js files.\n- `fetchFile` is intended to be a small (in terms of bundle size) function to help applications work with files in a portable way. The `Response` object returned on Node.js does not implement all the functionality the browser does. If you run into the need\n- In fact, the use of any of the file utilities including `readFile` and `readFileAsync` functions with other loaders.gl functions is entirely optional. loader objects can be used with data loaded via any mechanism the application prefers, e.g. directly using `fetch`, `XMLHttpRequest` etc.\n- The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases.\n- The stream utilities are intended to be small optional helpers that facilitate writing platform independent code that works with streams. This can be valuable as JavaScript Stream APIs are still maturing and there are still significant differences between platforms. However, streams and iterators created directly using platform specific APIs can be used as parameters to loaders.gl functions whenever a stream is expected, allowing the application to take full control when desired.\n","slug":"modules/core/docs/api-reference/fetch-file","title":"fetchFile"},{"excerpt":"Binary Utilities loaders.gl provides a set of functions to simplify working with binary data. There are a couple of different ways to deal…","rawMarkdownBody":"# Binary Utilities\n\nloaders.gl provides a set of functions to simplify working with binary data. There are a couple of different ways to deal with binary data in the JavaScript APIs for browser and Node.js, and some small but annoying \"gotchas\" that can trip up programmers when working with binary data.\n\n## Usage\n\n```js\nimport {toArrayBuffer} from '@loaders.gl/core';\n```\n\n## Functions\n\n### toArrayBuffer(binaryData : \\*) : ArrayBuffer\n\n\"Repackages\" a binary data in non-array-buffer form as an `ArrayBuffer`.\n\n- binaryData - ArrayBuffer, Buffer (Node.js), typed array, blob, ...\n\n## Remarks\n\n- Most functions in loaders.gl that accept binary data call `toArrayBuffer(...)` on input parameters before starting processing, thus ensuring that functions work on all types of input data.\n","slug":"modules/core/docs/api-reference/binary-utilities","title":"Binary Utilities"},{"excerpt":"Iterator Utilities Functions getStreamIterator(stream : Stream) : AsyncIterator Returns an async iterator that can be used to read chunks of…","rawMarkdownBody":"# Iterator Utilities\n\n## Functions\n\n### getStreamIterator(stream : Stream) : AsyncIterator\n\nReturns an async iterator that can be used to read chunks of data from the stream (or write chunks of data to the stream, in case of writable streams).\n\nWorks on both Node.js 8+ and browser streams.\n","slug":"modules/core/docs/api-reference/iterator-utilities","title":"Iterator Utilities"},{"excerpt":"load The  function can be used with any loader object. They takes a  and one or more loader objects, checks what type of data that loader…","rawMarkdownBody":"# load\n\nThe `load` function can be used with any _loader object_. They takes a `url` and one or more _loader objects_, checks what type of data that loader prefers to work on (e.g. text, JSON, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader.\n\n### load(url : String | File, loaders : Object | Object[][, options : object]) : Promise.Response\n\n### load(url : String | File [, options : Object]) : Promise.Response\n\nThe `load` function is used to load and parse data with a specific _loader object_. An array of loader objects can be provided, in which case `load` will attempt to autodetect which loader is appropriate for the file.\n\nThe `loaders` parameter can also be omitted, in which case any _loader objects_ previously registered with [`registerLoaders`](docs/api-reference/core/register-loaders) will be used.\n\n- `url` - Urls can be data urls (`data://`) or a request (`http://` or `https://`) urls, or a file name (Node.js only). Also accepts `File` or `Blob` object (Browser only). Can also accept any format that is accepted by [`parse`](https://github.com/uber-web/loaders.gl/blob/master/docs/api-reference/core/parse.md), with the exception of strings that are interpreted as urls.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options` - optional, contains both options for the read process and options for the loader (see documentation of the specific loader).\n\nReturns:\n\n- Return value depends on the _loader category_.\n\nNotes:\n\n- If `url` is not a `string`, `load` will call `parse` directly.\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n- `load` takes a `url` and a loader object, checks what type of data that loader prefers to work on (e.g. text, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader.\n- If `@loaders.gl/polyfills` is installed, `load` will work under Node.js as well.\n\n## Options\n\nA loader object, that can contain a mix of options defined by:\n\n- any loader(s) being used\n- the `parse` function\n\nIn addition to the following options\n\n| Option             | Type   | Default       | Description                                                      |\n| ------------------ | ------ | ------------- | ---------------------------------------------------------------- |\n| `options.dataType` | string | `arraybuffer` | Default depends on loader object. Set to 'text' to read as text. |\n","slug":"modules/core/docs/api-reference/load","title":"load"},{"excerpt":"parseSync Synchronous parsing is not supported by all loaders. Refer to the documentation for each loader. For supporting loaders, the…","rawMarkdownBody":"# parseSync\n\n> Synchronous parsing is not supported by all loaders. Refer to the documentation for each loader.\n\nFor supporting loaders, the synchronous `parseSync` function works on already loaded data.\n\n## Usage\n\n```js\nimport {fetchFile, parseSync} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\nconst response = await fetchFile(url);\nconst arraybuffer = await response.arrayBuffer();\n\ndata = parseSync(arraybuffer, OBJLoader);\n// Application code here\n...\n```\n\nHandling errors\n\n```js\ntry {\n  const data = await parseSync(data);\n} catch (error) {\n  console.log(error);\n}\n```\n\n## Functions\n\n### parseSync(fileData : ArrayBuffer | String, loaders : Object | Object\\[], [, options : Object [, url : String]]) : any\n\n### parseSync(fileData : ArrayBuffer | String, [, options : Object [, url : String]]) : any\n\nParses data synchronously using the provided loader, if possible. If not, returns `null`, in which case asynchronous parsing is required.\n\n- `data`: already loaded data, either in binary or text format. This parameter can be any of the following types:\n  - `Response`: `fetch` response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer`: Parse from binary data in an array buffer\n  - `String`: Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator`: Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n    can also be supplied.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options`: optional, options for the loader (see documentation of the specific loader).\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nReturns:\n\n- Return value depends on the _loader object_ category\n","slug":"modules/core/docs/api-reference/parse-sync","title":"parseSync"},{"excerpt":"parseInBatches Streaming parsing is not supported by all loaders. Refer to the documentation for each loader. For supporting loaders, the…","rawMarkdownBody":"# parseInBatches\n\n> Streaming parsing is not supported by all loaders. Refer to the documentation for each loader.\n\nFor supporting loaders, the streaming `parseInBatches` function can parse incrementally from a stream as data arrives and emit \"batches\" of parsed data.\n\nBatched (streaming) parsing is supported by some loaders\n\n```js\nimport {fetchFile, parseInBatches} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/obj';\n\nconst batchIterator = await parseInBatches(fetchFile(url), CSVLoader);\nfor await (const batch of batchIterator) {\n  console.log(batch.length);\n}\n```\n\n## Functions\n\n### parseInBatches(data : any, loaders : Object | Object\\[] [, options : Object [, url : String]]) : AsyncIterator\n\n### parseInBatches(data : any [, options : Object [, url : String]]) : AsyncIterator\n\nParses data in batches from a stream, releasing each batch to the application while the stream is still being read.\n\nParses data with the selected _loader object_. An array of `loaders` can be provided, in which case an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching).\n\nThe `loaders` parameter can also be ommitted, in which case any _loaders_ previously registered with [`registerLoaders`](docs/api-reference/core/register-loaders) will be used.\n\n- `data`: loaded data or an object that allows data to be loaded. This parameter can be any of the following types:\n  - `Response` - `fetch` response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer` - Parse from binary data in an array buffer\n  - `String` - Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator` - Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n  - `AsyncIterator` - iterator that yeilds promises that resolve to binary (`ArrayBuffer`) chunks or string chunks.\n  - `ReadableStream` - A DOM or Node stream.\n  - `Promise` - A promise that resolves to any of the other supported data types can also be supplied.\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see `registerLoaders`)\n- `options`: optional, options for the loader (see documentation of the specific loader).\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nReturns:\n\n- Returns an async iterator that yields batches of data. The exact format for the batches depends on the _loader object_ category.\n","slug":"modules/core/docs/api-reference/parse-in-batches","title":"parseInBatches"},{"excerpt":"parse This functions parse data. As important special cases, the async  function can also load (and then parse) data from a  (or )  object…","rawMarkdownBody":"# parse\n\nThis functions parse data. As important special cases, the async `parse` function can also load (and then parse) data from a `fetch` (or `fetchFile`) `Response` object, and the streaming `parseInBatches` version can parse incrementally from a stream as data arrives.\n\n## Usage\n\nThe return value from `fetch` or `fetchFile` is a `Promise` that resolves to the fetch `Response` object and can be passed directly to the non-sync parser functions:\n\n```js\nimport {fetchFile, parse} from '@loaders.gl/core';\nimport {OBJLoader} from '@loaders.gl/obj';\n\ndata = await parse(fetchFile(url), OBJLoader);\n// Application code here\n...\n```\n\nBatched (streaming) parsing is supported by some loaders\n\n```js\nimport {fetchFile, parseInBatches} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/obj';\n\nconst batchIterator = await parseInBatches(fetchFile(url), CSVLoader);\nfor await (const batch of batchIterator) {\n  console.log(batch.length);\n}\n```\n\nHandling errors\n\n```js\ntry {\n  const response = await fetch(url); // fetch can throw in case of network errors\n  const data = await parse(response); // parse will throw if server reports an error\n} catch (error) {\n  console.log(error);\n}\n```\n\n## Functions\n\n### parse(data : ArrayBuffer | String, loaders : Object | Object\\[] [, options : Object [, url : String]]) : Promise.Any\n\n### parse(data : ArrayBuffer | String, [, options : Object [, url : String]]) : Promise.Any\n\nParses data asynchronously either using the provided loader or loaders, or using the pre-registered loaders (see `register-loaders`).\n\n- `data`: loaded data or an object that allows data to be loaded. This parameter can be any of the following types:\n\n  - `Response` - response object returned by `fetchFile` or `fetch`.\n  - `ArrayBuffer` - Parse from binary data in an array buffer\n  - `String` - Parse from text data in a string. (Only works for loaders that support textual input).\n  - `Iterator` - Iterator that yeilds binary (`ArrayBuffer`) chunks or string chunks (string chunks only work for loaders that support textual input).\n  - `AsyncIterator` - iterator that yeilds promises that resolve to binary (`ArrayBuffer`) chunks or string chunks.\n  - `ReadableStream` - A DOM or Node stream.\n  - `File` - A browser file object (from drag-and-drop or file selection operations).\n  - `Promise` - A promise that resolves to any of the other supported data types can also be supplied.\n\n- `loaders` - can be a single loader or an array of loaders. If ommitted, will use the list of pre-registered loaders (see `registerLoaders`)\n\n- `options`: optional, options for the loader (see documentation of the specific loader).\n\n- `url`: optional, assists in the autoselection of a loader if multiple loaders are supplied to `loader`.\n\nReturns:\n\n- Return value depends on the _loader object_ category\n\nNotes:\n\n- If multiple `loaders` are provided (or pre-registered), an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching).\n\n## Options\n\nTop-level options\n\n| Option           | Type    | Default   | Description                                                                                                              |\n| ---------------- | ------- | --------- | ------------------------------------------------------------------------------------------------------------------------ |\n| `options.log`    | object  | `console` | By default set to a `console` wrapper. Setting log to `null` will turn off logging.                                      |\n| `options.worker` | boolean | `true`    | If the selected loader is equipped with a worker url (and the runtime environment supports it) parse on a worker thread. |\n","slug":"modules/core/docs/api-reference/parse","title":"parse"},{"excerpt":"selectLoader  is considered experimental as loader auto detection is still being improved. A core feature of loaders.gl is the ability to…","rawMarkdownBody":"# selectLoader\n\n> `selectLoader` is considered experimental as loader auto detection is still being improved.\n\nA core feature of loaders.gl is the ability to automatically select an appropriate loader for a specific file among a list of candidate loaders. This feature is built-in to the `parse` and `load` functions, but applications can also access this feature directly through the `selectLoader` API.\n\nLoader selection heuristics are based on both filename (url) extensions as well as comparison of initial data content against known headers for each file format.\n\n`selectLoader` is also aware of the [loader registry](docs/api-reference/core/register-loaders.md). If no loaders are provided (by passing in a falsy value such as `null`) `selectLoader` will search the list of pre-registered loaders.\n\n## Usage\n\nSelect a loader from a list of provided loaders:\n\n```js\nimport {_selectLoader} from '@loaders.gl/core';\nimport {ArrowLoader} from '@loaders.gl/arrow';\nimport {CSVLoader} from '@loaders.gl/csv';\n\n_selectLoader([ArrowLoader, CSVLoader], 'filename.csv'); // => CSVLoader\n```\n\nSelect a loader from pre-registered loaders in the loader registry:\n\n```js\nimport {registerLoaders, _selectLoader} from '@loaders.gl/core';\nimport {ArrowLoader} from '@loaders.gl/arrow';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nregisterLoaders(ArrowLoader, CSVLoader);\n\n// By passing null instead of a loader list, selectLoader returns null.\n_selectLoader(null, 'filename.csv'); // => CSVLoader\n```\n\n## Functions\n\n### \\_selectLoader(loaders : Object | Object[] | null, url? : String, data? : ArrayBuffer | String, options? : Object)\n\nSelects an appropriate loader for a file from a list of candidate loaders by examining a URL and/or an initial data chunk.\n\nParameters:\n\n- `loaders` - can be a single loader or an array of loaders, or null.\n- `url` - An optional URL to perform autodetection against.\n- `data` - Optional data to perform autodetection against\n- `options.nothrow`=`false` - Return null instead of throwing exception if no loader can be found\n\nReturns:\n\n- A single loader (or null if `options.nothrow` was set and no matching loader was found).\n\nThrows:\n\n- If no matching loader was found, and `options.nothrow` was not set.\n\nRegarding the `loaders` parameter:\n\n- A single loader object will be returned without matching.\n- a `null` loader list will use the pre-registered list of loaders.\n- A supplied list of loaders will be searched for a matching loader.\n\n## Remarks\n\n- File extensions - An attempt will be made to extract a file extension by stripping away query parameters and base path before matching against known loader extensions.\n- Stream autodetection - Currently not well supported.\n","slug":"modules/core/docs/api-reference/select-loader","title":"selectLoader"},{"excerpt":"registerLoaders The loader registry allows applications to cherry-pick which loaders to include in their application bundle by importing…","rawMarkdownBody":"# registerLoaders\n\nThe loader registry allows applications to cherry-pick which loaders to include in their application bundle by importing just the loaders they need and registering them during initialization.\n\nApplications can then make all those imported loaders available (via format autodetection) to all subsequent `parse` and `load` calls, without those calls having to specify which loaders to use.\n\n## Usage\n\nSample application initialization code that imports and registers loaders:\n\n```js\nimport {registerLoaders} from '@loaders.gl/core';\nimport {CSVLoader} from '@loaders.gl/csv';\n\nregisterLoaders(CSVLoader);\n```\n\nSome other file that needs to load CSV:\n\n```js\nimport {load} from '@loaders.gl/core';\n\n// The pre-registered CSVLoader gets auto selected based on file extension...\nconst data = await load('data.csv');\n```\n\n## Functions\n\n### registerLoaders(loaders : Object | Object[])\n\nRegisters one or more _loader objects_ to a global _loader object registry_, these loaders will be used if no loader object is supplied to `parse` and `load`.\n\n- `loaders` - can be a single loader or an array of loaders. The specified loaders will be added to any previously registered loaders.\n","slug":"modules/core/docs/api-reference/register-loaders","title":"registerLoaders"},{"excerpt":"setLoaderOptions Usage Bundling the entire  library: Functions setLoaderOptions(options : Object) : void Merge the options with the global…","rawMarkdownBody":"# setLoaderOptions\n\n## Usage\n\nBundling the entire `draco3d` library:\n\n```js\nimport draco from 'draco3d';\nimport {setLoaderOptions} from '@loaders.gl/core';\nsetLoaderOptions({\n  modules: {\n    draco3d\n  }\n});\n```\n\n## Functions\n\n### setLoaderOptions(options : Object) : void\n\nMerge the options with the global options\n\n## Options\n\nTop-level options\n\n| Option            | Type    | Default   | Description                                                                                                              |\n| ----------------- | ------- | --------- | ------------------------------------------------------------------------------------------------------------------------ |\n| `options.log`     | object  | `console` | By default set to a `console` wrapper. Setting log to `null` will turn off logging.                                      |\n| `options.worker`  | boolean | `true`    | If the selected loader is equipped with a worker url (and the runtime environment supports it) parse on a worker thread. |\n| `options.cdn`     | boolean | string    | `true`                                                                                                                   | `true` loads from `unpkg.com/@loaders.gl`. `false` load from local urls. `string` alternate CDN url. |\n| `options.modules` | Object  | -         | Supply bundles modules or override local urls.                                                                           |\n","slug":"modules/core/docs/api-reference/set-loader-options","title":"setLoaderOptions"},{"excerpt":"save Needs update  and  function can be used with any writer.  takes a  and a writer object, checks what type of data that writer prefers to…","rawMarkdownBody":"# save\n\n> Needs update\n\n`save` and `saveSync` function can be used with any writer. `save` takes a `url` and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer.\n\n## Functions\n\n### save(url : String | File, writer : Object [, options : Object]) : Promise.ArrayBuffer| Promi\n\nse.String\n\nThe `save` function can be used with any writer.\n\n`save` takes a `url` and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer.\n\n- `url` - Can be a string, either a data url or a request url, or in Node.js, a file name, or in the browser, a File object.\n- `data` - saveed data, either in binary or text format.\n- `writer` - can be a single writer or an array of writers.\n- `options` - optional, contains both options for the read process and options for the writer (see documentation of the specific writer).\n- `options.dataType`=`arraybuffer` - By default reads as binary. Set to 'text' to read as text.\n\nReturns:\n\n- Return value depends on the category\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n### saveSync(url : String [, options : Object]) : ArrayBuffer | String\n\nSimilar to `save` except saves and parses data synchronously.\n\nNote that for `saveSync` to work, the `url` needs to be saveable synchronously _and_ the writer used must support synchronous parsing. Synchronous saveing only works on data URLs or files in Node.js. In many cases, the asynchronous `save` is more appropriate.\n","slug":"modules/core/docs/api-reference/save","title":"save"},{"excerpt":"writeFile A file save utilities that (attempts to) work consistently across browser and node. Usage Functions writeFile(url : String…","rawMarkdownBody":"# writeFile\n\nA file save utilities that (attempts to) work consistently across browser and node.\n\n## Usage\n\n```js\nimport {writeFile} from '@loaders.gl/core';\nimport {DracoWriter} from '@loaders.gl/draco';\n\nawait writeFile(url, DracoWriter);\n```\n\n## Functions\n\n### writeFile(url : String [, options : Object]) : Promise.ArrayBuffer\n\nReads the raw data from a file asynchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n### writeFileSync(url : String [, options : Object]) : ArrayBuffer\n\n> Only works on Node.js or using data URLs.\n\nReads the raw data from a \"file\" synchronously.\n\nNotes:\n\n- Any path prefix set by `setPathPrefix` will be appended to relative urls.\n\n## Remarks\n\n- The use of the loaders.gl `writeFile` and `writeFileAsync` functions is optional, loaders.gl loaders can be used with any data loaded via any mechanism the application prefers, e.g. `fetch`, `XMLHttpRequest` etc.\n- The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases.\n","slug":"modules/core/docs/api-reference/write-file","title":"writeFile"},{"excerpt":"BasisLoader An image loader that works under both Node.js (requires ) and the browser. Loader Characteristic File Extension  File Type…","rawMarkdownBody":"# BasisLoader\n\nAn image loader that works under both Node.js (requires `@loaders.gl/polyfills`) and the browser.\n\n| Loader         | Characteristic                  |\n| -------------- | ------------------------------- |\n| File Extension | `.basis`                        |\n| File Type      | Binary                          |\n| File Format    | Basis                           |\n| Data Format    | `Array of Array of ArrayBuffer` |\n| Supported APIs | `load`, `parse`                 |\n\n## Usage\n\n```js\nimport '@loaders.gl/polyfills'; // only needed if using under Node\nimport {ImageLoader} from '@loaders.gl/images';\nimport {load} from '@loaders.gl/core';\n\nconst image = await load(url, ImageLoader, options);\n```\n\n## Options\n\n| Option       | Type   | Default  | Description                                                                  |\n| ------------ | ------ | -------- | ---------------------------------------------------------------------------- |\n| `basis.type` | String | `'auto'` | Set to `imagebitmap`, `html` or `ndarray` to control type of returned image. |\n| `basis.type` | String | `'auto'` | Set to `imagebitmap`, `html` or `ndarray` to control type of returned image. |\n\n## Remarks\n\n- While generic, the `BasisLoader` is designed with WebGL applications in mind, ensuring that loaded image data can be used to create a `WebGLTexture` both in the browser and in headless gl under Node.js\n- Node.js support requires import `@loaders.gl/polyfills` before installing this module.\n","slug":"modules/basis/docs/api-reference/basis-loader","title":"BasisLoader"},{"excerpt":"setPathPrefix resolvePath(path : String) : String Applies aliases and path prefix, in that order. Returns an updated path. setPathPrefix…","rawMarkdownBody":"# setPathPrefix\n\n### resolvePath(path : String) : String\n\nApplies aliases and path prefix, in that order. Returns an updated path.\n\n### setPathPrefix(prefix : String)\n\nThis sets a path prefix that is automatically prepended to relative path names provided to load functions.\n\n### getPathPrefix() : String\n\nReturns the current path prefix set by `setPathPrefix`.\n","slug":"modules/core/docs/api-reference/set-path-prefix","title":"setPathPrefix"},{"excerpt":"CompressedTextureLoader (Experimental) Loader for compressed Texture Files. Loader Characteristic File Extension ,  File Type Binary File…","rawMarkdownBody":"# CompressedTextureLoader (Experimental)\n\nLoader for compressed Texture Files.\n\n| Loader         | Characteristic                                             |\n| -------------- | ---------------------------------------------------------- |\n| File Extension | `.dds`, `.pvr`                                             |\n| File Type      | Binary                                                     |\n| File Format    | Compressed Texture                                         |\n| Data Format    | Array of `{compressed: true, format, width, height, data}` |\n| Supported APIs | `load`, `parse`                                            |\n\n## Usage\n\n```js\nimport '@loaders.gl/polyfills'; // only needed if using under Node\nimport {CompressedTextureLoader} from '@loaders.gl/basis';\nimport {load} from '@loaders.gl/core';\n\nconst images = await load(url, CompressedTextureLoader, options);\n```\n\n## Data Format\n\nReturns an array of image objects representing mip levels.\n\n## Options\n\n| Option       | Type   | Default  | Description                                                                  |\n| ------------ | ------ | -------- | ---------------------------------------------------------------------------- |\n| `image.type` | String | `'auto'` | Set to `imagebitmap`, `html` or `ndarray` to control type of returned image. |\n\nIn addition, for `imagebitmap` type images, it is possible to pass through options to [`createImageBitmap`](https://developer.mozilla.org/en-US/docs/Web/API/WindowOrWorkerGlobalScope/createImageBitmap) to control image extraction, via the separate `options.imagebitmap` object.\n\n## Remarks\n\n- While generic, the `CompressedTextureLoader` is designed with WebGL applications in mind, ensuring that loaded image data can be used to create a `WebGLTexture` both in the browser and in headless gl under Node.js\n- Node.js support requires import `@loaders.gl/polyfills` before installing this module.\n","slug":"modules/basis/docs/api-reference/compressed-texture-loader","title":"CompressedTextureLoader (Experimental)"},{"excerpt":"CSVLoader Streaming loader for comma-separated value and delimiter-separated value encoded files. Loader Characteristic File Extension…","rawMarkdownBody":"# CSVLoader\n\nStreaming loader for comma-separated value and [delimiter-separated value](https://en.wikipedia.org/wiki/Delimiter-separated_values) encoded files.\n\n| Loader         | Characteristic                                       |\n| -------------- | ---------------------------------------------------- |\n| File Extension | `.csv`, `.dsv`                                       |\n| File Type      | Text                                                 |\n| File Format    | [RFC4180](https://tools.ietf.org/html/rfc4180)       |\n| Data Format    | [Classic Table](/docs/specifications/category-table) |\n| Supported APIs | `load`, `parse`, `parseSync`, `parseInBatches`       |\n\n## Usage\n\n```js\nimport {CSVLoader} from '@loaders.gl/csv';\nimport {load} from '@loaders.gl/core';\n\nconst data = await load(url, CSVLoader, {csv: options});\n```\n\n## Options\n\n| Option                  | Type     | Default                 | Description                                                                                                                                                                                                                                                                                     |\n| ----------------------- | -------- | ----------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `csv.delimiter`         | String   | auto-detect             | The delimiting character.                                                                                                                                                                                                                                                                       |\n| `csv.header`            | Boolean  | auto-detect             | If `true`, the first row of parsed data will be interpreted as field names. If `false`, the first row is interpreted as data.                                                                                                                                                                   |\n| `csv.newline`           | String   | auto-detect             | The newline sequence. Must be `\\r`, `\\n`, or `\\r\\n`.                                                                                                                                                                                                                                            |\n| `csv.quoteChar`         | String   | `\"`                     | The character used to quote fields.                                                                                                                                                                                                                                                             |\n| `csv.escapeChar`        | String   | `\"`                     | The character used to escape the quote character within a field.                                                                                                                                                                                                                                |\n| `csv.dynamicTyping`     | Boolean  | `true`                  | If `true`, numeric and boolean data values will be converted to their type (instead if strings).                                                                                                                                                                                                |\n| `csv.comments`          | String   | `false`                 | Comment indicator (for example, \"#\" or \"//\"). Lines starting with this string are skipped.                                                                                                                                                                                                      |\n| `csv.skipEmptyLines`    | String   | `false`                 | If `true`, lines that are completely empty (those which evaluate to an empty string) will be skipped. If set to `'greedy'`, lines that don't have any content (those which have only whitespace after parsing) will also be skipped.                                                            |\n| `csv.transform`         | Function | -                       | A function to apply on each value. The function receives the value as its first argument and the column number or header name when enabled as its second argument. The return value of the function will replace the value it received. The transform function is applied before dynamicTyping. |\n| `csv.delimitersToGuess` | Array    | `[',', '\\t', '|', ';']` | An array of delimiters to guess from if the `delimiter` option is not set.                                                                                                                                                                                                                      |\n| `csv.fastMode`          | Boolean  | auto-detect             | Force set \"fast mode\". Fast mode speeds up parsing significantly for large inputs but only works when the input has no quoted fields. Fast mode will be auto enabled if no `\"` characters appear in the input.                                                                                  |\n\nRemarks:\n\n- Many options are passed on to papaparse, if necessary [papaparse docs](https://www.papaparse.com/docs#config) could serve as a source for more information.\n","slug":"modules/csv/docs/api-reference/csv-loader","title":"CSVLoader"},{"excerpt":"ArrowLoader The Arrow loaders are still under development. The  parses the Apache Arrow columnar table format. Loader Characteristic File…","rawMarkdownBody":"# ArrowLoader\n\n> The Arrow loaders are still under development.\n\nThe `ArrowLoader` parses the Apache Arrow columnar table format.\n\n| Loader                | Characteristic                                                            |\n| --------------------- | ------------------------------------------------------------------------- |\n| File Extension        | `.arrow`                                                                  |\n| File Type             | Binary                                                                    |\n| File Format           | [IPC: Encapsulated Message Format](http://arrow.apache.org/docs/ipc.html) |\n| Data Format           | [Columnar Table](/docs/specifications/category-table)                     |\n| Decoder Type          | Synchronous                                                               |\n| Worker Thread Support | Yes                                                                       |\n| Streaming Support     | Yes                                                                       |\n\n## Usage\n\n```js\nimport {ArrowLoader, ArrowWorkerLoader} from '@loaders.gl/arrow';\nimport {load} from '@loaders.gl/core';\n\n// Decode on main thread\nconst data = await load(url, ArrowLoader, options);\n// Decode on worker thread\nconst data = await load(url, ArrowWorkerLoader, options);\n```\n\n## Options\n\n| Option | Type | Default | Description |\n| ------ | ---- | ------- | ----------- |\n\n","slug":"modules/arrow/docs/api-reference/arrow-loader","title":"ArrowLoader"},{"excerpt":"loadImages A function that loads an array of images. Primarily intended for loading: an array of images for a WebGL  or  textures an array…","rawMarkdownBody":"# loadImages\n\nA function that loads an array of images. Primarily intended for loading:\n\n- an array of images for a WebGL `TEXTURE_2D_ARRAY` or `TEXTURE_3D` textures\n- an array of images representing mip levels of a single WebGL `TEXTURE_2D` texture or one `TEXTURE_CUBE` face.\n\n## Usage\n\nLoading an array of images\n\n```js\nimport '@loaders.gl/polyfills'; // only needed for Node.js support\nimport {loadImageArray} from `@loaders.gl/images`;\n\nconst images = await loadImageArray(count, ({index}) => `filename-${index}`);\n\nfor (const image of images) {\n  ...\n}\n```\n\n```js\nimport '@loaders.gl/polyfills'; // only needed for Node.js support\nimport {loadImageArray} from `@loaders.gl/images`;\n\nconst images = await loadImageArray(count,  ({index}) => `filename-${index}`, {\n  mipLevels: 'auto'\n});\n\nfor (const imageArray of images) {\n  for (const lodImage of imageArray) {\n    ...\n  }\n}\n```\n\n## getUrl Callback Parameters\n\nthe `getUrl` callback will be called for each image with the following parameters:\n\n| Parameter | Description                                                    |\n| --------- | -------------------------------------------------------------- |\n| `index`   | The index of the image being loaded, from `0` to `count - 1`.  |\n| `lod`     | The mip level image being loaded, from `0` to `mipLevels - 1`. |\n\nNote: In addition to these values, all `options` passed in to `loadImageArray` are also available in the `getUrl` method.\n\n### loadImageArray(count : Number | String, getUrl : ({index}) => String, options? : Object) : image[] | image[][]\n\nParameters:\n\n- `count`: Number of images to load.\n- `getUrl`: A function that generates the url for each image, it is called for each image with the `index` of that image.\n- `options`: Supports the same options as [`ImageLoader`](modules/images/docs/api-reference/image-loader).\n\nReturns\n\n- an array of images (or array of arrays of mip images)\n\n## Options\n\nAccepts the same options as [`ImageLoader`](modules/images/docs/api-reference/image-loader), and\n\n| Option            | Type              | Default | Description                                            |\n| ----------------- | ----------------- | ------- | ------------------------------------------------------ |\n| `image.mipLevels` | `Number | String` | `0`     | If `'auto'` or non-zero, loads an array of mip images. |\n\nNumber of mip level images to load: Use `0` to indicate a single image with no mips. Supplying the string `'auto'` will infer the mipLevel from the size of the `lod`=`0` image.\n\n## Remarks\n\n- Returned images can be passed directly to WebGL texture methods. See [`ImageLoader`](modules/images/docs/api-reference/image-loader) for details about the type of the returned images.\n","slug":"modules/images/docs/api-reference/load-image-array","title":"loadImages"},{"excerpt":"Tile3DLoader Parses a 3D tile. Loader Characteristic File Extensions ,, ,  File Type Binary (with linked assets) File Format glTF Data…","rawMarkdownBody":"# Tile3DLoader\n\nParses a [3D tile](https://github.com/AnalyticalGraphicsInc/3d-tiles).\n\n| Loader                | Characteristic                                                                                                 |\n| --------------------- | -------------------------------------------------------------------------------------------------------------- |\n| File Extensions       | `.b3dm`,`.i3dm`, `.pnts`, `.cmpt`                                                                              |\n| File Type             | Binary (with linked assets)                                                                                    |\n| File Format           | [glTF](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#tile-format-specifications) |\n| Data Format           | [Scenegraph](/docs/specifications/category-scenegraph)                                                         |\n| Decoder Type          | Synchronous (limited), Asynchronous                                                                            |\n| Worker Thread Support | No                                                                                                             |\n| Streaming Support     | No \\*                                                                                                          |\n\n\\* Streaming is not supported for invididual tiles, however tilesets are streamed by loading only the tiles needed for the current view.\n\n## Usage\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {Tile3DLoader} from '@loaders.gl/3d-tiles';\nconst gltf = await load(url, Tile3DLoader);\n```\n\nTo decompress tiles containing Draco compressed glTF models or Draco compressed point clouds:\n\n```js\nimport {load} from '@loaders.gl/core';\nimport {Tile3DLoader} from '@loaders.gl/3d-tiles';\nimport {DracoLoader} from '@loaders.gl/draco';\nconst gltf = await load(url, Tile3DLoader, {DracoLoader, decompress: true});\n```\n\n## Options\n\nTo enable parsing of DRACO compressed point clouds and glTF tiles, make sure to first register a [DracoLoader](/docs/api-reference/draco/draco-loader). The `DracoWorkerLoader` will usually give best loading performance and interactivity.\n\nPoint cloud tie options\n\n| Option                              | Type      | Default | Description                          |\n| ----------------------------------- | --------- | ------- | ------------------------------------ |\n| `3d-tiles.decodeQuantizedPositions` | `Boolean` | `false` | Pre-decode quantized position on CPU |\n| `3d-tiles.decodeQuantizedPositions` | `Boolean` | `false` | Pre-decode quantized position on CPU |\n\nFor i3dm and b3dm tiles:\n\n| Option              | Type    | Default | Description                           |\n| ------------------- | ------- | ------- | ------------------------------------- |\n| `3d-tiles.loadGLTF` | Boolean | `true`  | Fetch and parse any linked glTF files |\n\nIf `options['3d-tiles'].loadGLTF` is `true`, GLTF loading can be controlled by providing [`GLTFLoader` options](modules/gltf/docs/api-reference/gltf-loader.md) via the `options.gltf` sub options.\n\n## Notes about Tile Types\n\n### b3dm, i3dm\n\nglTF file into a hirearchical scenegraph description that can be used to instantiate an actual Scenegraph in most WebGL libraries. Can load both binary `.glb` files and JSON `.gltf` files.\n","slug":"modules/3d-tiles/docs/api-reference/tile-3d-loader","title":"Tile3DLoader"},{"excerpt":"Tileset3DLoader The 3D tile loaders are still under development. Parses a main tileset JSON file as the entry point to define a 3D tileset…","rawMarkdownBody":"# Tileset3DLoader\n\n> The 3D tile loaders are still under development.\n\nParses a main tileset JSON file as the entry point to define a 3D tileset.\n\n| Loader                | Characteristic                                                                                              |\n| --------------------- | ----------------------------------------------------------------------------------------------------------- |\n| File Extensions       | `.json`                                                                                                     |\n| File Type             | JSON                                                                                                        |\n| File Format           | [3D Tileset JSON](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#tileset-json) |\n| Data Format           | JSON                                                                                                        |\n| Decoder Type          | Synchronous                                                                                                 |\n| Worker Thread Support | No                                                                                                          |\n| Streaming Support     | No                                                                                                          |\n\n## Usage\n\n```js\nimport {Tileset3DLoader, Tileset3D} from '^loaders.gl/3d-tiles';\nconst tilesetJson = await load(\n  'http://localhost:8002/tilesets/Seattle/tileset.json',\n  Tileset3DLoader\n);\nconst tileset = new Tileset3D(tilesetJson);\n```\n\n## Options\n\n| Option | Type | Default | Description |\n| ------ | ---- | ------- | ----------- |\n\n","slug":"modules/3d-tiles/docs/api-reference/tileset-3d-loader","title":"Tileset3DLoader"},{"excerpt":"Tileset3D The  class is being generalized to handle more use cases. Since this may require modifying some APIs, this class should be…","rawMarkdownBody":"# Tileset3D\n\n> The `Tileset3D` class is being generalized to handle more use cases. Since this may require modifying some APIs, this class should be considered experiemental.\n\nThe `Tileset3D` class can be instantiated with tileset data formatted according to the [3D Tiles Category](docs/specifications/3d-tiles), which is supported by the [Tileset3DLoader](docs/api-reference/3d-tiles/tileset-3d-loader).\n\nReferences\n\n- [3D Tiles](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification).\n\n## Usage\n\nLoading a tileset and instantiating a `Tileset3D` instance.\n\n```js\nimport {Tileset3DLoader, Tileset3D} from '@loaders.gl/3d-tiles';\nimport {parse} from '@loaders.gl/core';\n\nconst tilesetJSON = await parse(fetch(tileset));\nconst tileset = new Tileset3D(tilesetJson);\n\nconsole.log(`Maximum building height: ${tileset.properties.height.maximum}`);\nconsole.log(`Minimum building height: ${tileset.properties.height.minimum}`);\n```\n\nCommon setting for the `skipLevelOfDetail` optimization\n\n```js\nimport {Tileset3D} from '@loaders.gl/3d-tiles';\n\nconst tileset = new Tileset3D(tilesetJson, {\n  url: 'http://localhost:8002/tilesets/Seattle/tileset.json',\n  baseScreenSpaceError: 1024,\n  skipScreenSpaceErrorFactor: 16\n});\n```\n\nCommon settings for the `dynamicScreenSpaceError` optimization\n\n```js\nimport {Tileset3D} from '^loaders.gl/3d-tiles';\nconst tileset = new Tileset3D({\n  url: 'http://localhost:8002/tilesets/Seattle/tileset.json',\n  dynamicScreenSpaceError: true,\n  dynamicScreenSpaceErrorDensity: 0.00278,\n  dynamicScreenSpaceErrorFactor: 4.0\n});\n```\n\n### Properties\n\n### asset : Object (readonly)\n\nGets the tileset's asset object property, which contains metadata about the tileset.\n\nSee the [asset schema reference](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#reference-asset) in the 3D Tiles spec for the full set of properties.\n\n### properties : Object (readonly)\n\nGets the tileset's properties dictionary object, which contains metadata about per-feature properties.\n\nSee the [properties schema reference](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#reference-properties) in the 3D Tiles spec for the full set of properties.\n\n### tilesLoaded : boolean (readonly)\n\nWhen `true`, all tiles that meet the screen space error this frame are loaded. The tileset is\ncompletely loaded for this view.\n\nSee Tileset3D#allTilesLoaded\n\n### url : String (readonly)\n\nThe url to a tileset JSON file.\n\n### basePath : String (readonly) (deprecated)\n\nThe base path that non-absolute paths in tileset JSON file are relative to.\n\n### maximumScreenSpaceError : Number\n\nThe maximum screen space error used to drive level of detail refinement. This value helps determine when a tile refines to its descendants, and therefore plays a major role in balancing performance with visual quality.\n\nA tile's screen space error is roughly equivalent to the number of pixels wide that would be drawn if a sphere with a\nradius equal to the tile's <b>geometric error</b> were rendered at the tile's position. If this value exceeds\n`maximumScreenSpaceError` the tile refines to its descendants.\n\nDepending on the tileset, `maximumScreenSpaceError` may need to be tweaked to achieve the right balance. Higher values provide better performance but lower visual quality. \\*\n\n### maximumMemoryUsage : Number\n\n^default 16 \\*\n^exception `maximumScreenSpaceError` must be greater than or equal to zero.\n\nThe maximum amount of GPU memory (in MB) that may be used to cache tiles. This value is estimated from\ngeometry, textures, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata.\n\nTiles not in view are unloaded to enforce this.\n\nIf decreasing this value results in unloading tiles, the tiles are unloaded the next frame.\n\nIf tiles sized more than `maximumMemoryUsage` are needed\nto meet the desired screen space error, determined by `Tileset3D.maximumScreenSpaceError`,\nfor the current view, then the memory usage of the tiles loaded will exceed\n`maximumMemoryUsage`. For example, if the maximum is 256 MB, but\n300 MB of tiles are needed to meet the screen space error, then 300 MB of tiles may be loaded. When\nthese tiles go out of view, they will be unloaded.\n\n^default 512 \\*\n^exception `maximumMemoryUsage` must be greater than or equal to zero.\n^see Tileset3D#totalMemoryUsageInBytes\n\n### root : Tile3DHeader\n\nThe root tile header.\n\n### boundingSphere : BoundingSphere\n\nThe tileset's bounding sphere.\n\n```js\nvar tileset = viewer.scene.primitives.add(\n  new Tileset3D({\n    url: 'http://localhost:8002/tilesets/Seattle/tileset.json'\n  })\n);\n\ntileset.readyPromise.then(function(tileset) {\n  // Set the camera to view the newly added tileset\n  viewer.camera.viewBoundingSphere(tileset.boundingSphere, new HeadingPitchRange(0, -0.5, 0));\n});\n```\n\n### modelMatrix : Matrix4\n\nA 4x4 transformation matrix that transforms the entire tileset.\n\n```js\n// Adjust a tileset's height from the globe's surface.\nvar heightOffset = 20.0;\nvar boundingSphere = tileset.boundingSphere;\nvar cartographic = Cartographic.fromCartesian(boundingSphere.center);\nvar surface = Cartesian3.fromRadians(cartographic.longitude, cartographic.latitude, 0.0);\nvar offset = Cartesian3.fromRadians(cartographic.longitude, cartographic.latitude, heightOffset);\nvar translation = Cartesian3.subtract(offset, surface, new Cartesian3());\ntileset.modelMatrix = Matrix4.fromTranslation(translation);\n```\n\n### maximumMemoryUsage : Number\n\n### totalMemoryUsageInBytes : Number\n\nThe total amount of GPU memory in bytes used by the tileset. This value is estimated from\ngeometry, texture, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata.\n\n### stats : Stats\n\nAn instance of a probe.gl `Stats` object that contains information on how many tiles have been loaded etc. Easy to display using a probe.gl `StatsWidget`.\n\n### ellipsoid : Ellipsoid\n\nGets an ellipsoid describing the shape of the globe.\n\nReturns the `extras` property at the top-level of the tileset JSON, which contains application specific metadata.\nReturns `undefined` if `extras` does not exist.\n\nException The tileset is not loaded. Use Tileset3D.readyPromise or wait for Tileset3D.ready to be true.\n\nSee [Extras](https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification#specifying-extensions-and-application-specific-extras) in the 3D Tiles specification.}\n\n### unloadTileset\n\nUnloads all tiles that weren't selected the previous frame. This can be used to\nexplicitly manage the tile cache and reduce the total number of tiles loaded below\n`Tileset3D.maximumMemoryUsage`.\n\nTile unloads occur at the next frame to keep all the WebGL delete calls\nwithin the render loop.\n\n### isDestroyed() : Boolean\n\nReturns true if this object was destroyed; otherwise, false.\n\nIf this object was destroyed, it should not be used; calling any function other than\n`isDestroyed` will result in an exception.\n\n^returns `Boolean`: `true` if this object was destroyed; otherwise, `false`.\n\n### destroy()\n\nDestroys the WebGL resources held by this object. Destroying an object allows for deterministic\nrelease of WebGL resources, instead of relying on the garbage collector to destroy this object.\n\nOnce an object is destroyed, it should not be used; calling any function other than `isDestroyed` will result in an exception. Therefore, assign the return value `undefined` to the object as done in the example.\n\nWxception This object was destroyed, i.e., destroy() was called.\n\n## Methods\n\n### constructor(tileset : Object, url : String [, options : Object])\n\n- `tileset`: The loaded tileset (parsed JSON)\n- `url`: The url to a tileset JSON file.\n- `options`: Options object, see the options section below for available options.\n\nNotes:\n\n- The `version` tileset must be 3D Tiles version 0.0 or 1.0.\n\n### hasExtension(extensionName : String) : Boolean\n\n`true` if the tileset JSON file lists the extension in extensionsUsed; otherwise, `false`.\n^param {String} extensionName The name of the extension to check. \\*\n^returns {Boolean} `true` if the tileset JSON file lists the extension in extensionsUsed; otherwise, `false`.\n\n## Options\n\n> Tileset3D class is still being developed, not all options are guaranteed to be working.\n\nThe `Tileset3D` class supports a number of options\n\n- `options.url` (`Resource|String|Promise.Resource|Promise.String`) The url to a tileset JSON file.\n- `options.show`=`true` (`Boolean`) - Determines if the tileset will be shown.\n- `options.modelMatrix`=`Matrix4.IDENTITY` (`Matrix4`) - A 4x4 transformation matrix that transforms the tileset's root tile.\n- `options.maximumScreenSpaceError`=`16`] (`Number`) - The maximum screen space error used to drive level of detail refinement.\n- `options.maximumMemoryUsage`=`512`] (`Number`) - The maximum amount of memory in MB that can be used by the tileset.\n- `options.dynamicScreenSpaceError`=`false`] (`Boolean`) - Optimization option. Reduce the screen space error for tiles that are further away from the camera.\n- `options.dynamicScreenSpaceErrorDensity`=`0.00278`] (`Number`) - Density used to adjust the dynamic screen space error, similar to fog density.\n- `options.dynamicScreenSpaceErrorFactor`=`4.0`] (`Number`) - A factor used to increase the computed dynamic screen space error.\n- `options.baseScreenSpaceError`=`1024` (`Number`) - When `skipLevelOfDetail` is `true`, the screen space error that must be reached before skipping levels of detail.\n- `options.skipScreenSpaceErrorFactor`=`16` (`Number`) - When `skipLevelOfDetail` is `true`, a multiplier defining the minimum screen space error to skip. Used in conjunction with `skipLevels` to determine which tiles to load.\n- `options.ellipsoid`=`Ellipsoid.WGS84` (`Ellipsoid`) - The ellipsoid determining the size and shape of the globe.\n\nCallbacks\n\n- `options.onTileLoad` (`(tileHeader : Tile3DHeader) : void`) -\n- `options.onTileUnload` (`(tileHeader : Tile3DHeader) :void`) -\n- `options.onTileError` (`void(tileHeader : Tile3DHeader, message : String) : void`) -\n\n### dynamicScreenSpaceError\n\n=`false`\n\nOptimization option. Whether the tileset should refine based on a dynamic screen space error. Tiles that are further away will be rendered with lower detail than closer tiles. This improves performance by rendering fewer tiles and making less requests, but may result in a slight drop in visual quality for tiles in the distance.\n\nThe algorithm is biased towards \"street views\" where the camera is close to the ground plane of the tileset and looking at the horizon. In addition results are more accurate for tightly fitting bounding volumes like box and region.\n\n### dynamicScreenSpaceErrorDensity\n\n=`0.00278`\n\nA scalar that determines the density used to adjust the dynamic screen space error (similar to \"fog\"). Increasing this value has the effect of increasing the maximum screen space error for all tiles, but in a non-linear fashion.\n\nThe error starts at 0.0 and increases exponentially until a midpoint is reached, and then approaches 1.0 asymptotically. This has the effect of keeping high detail in the closer tiles and lower detail in the further tiles, with all tiles beyond a certain distance all roughly having an error of 1.0.\n\nThe dynamic error is in the range [0.0, 1.0) and is multiplied by `dynamicScreenSpaceErrorFactor` to produce the\nfinal dynamic error. This dynamic error is then subtracted from the tile's actual screen space error.\n\nIncreasing `dynamicScreenSpaceErrorDensity` has the effect of moving the error midpoint closer to the camera.\nIt is analogous to moving fog closer to the camera.\n\n### dynamicScreenSpaceErrorFactor\n\n= 4.0;\n\nA factor used to increase the screen space error of tiles for dynamic screen space error. As this value increases less tiles\nare requested for rendering and tiles in the distance will have lower detail. If set to zero, the feature will be disabled.\n\n### dynamicScreenSpaceErrorHeightFalloff\n\n= 0.25;\n\nA ratio of the tileset's height at which the density starts to falloff. If the camera is below this height the\nfull computed density is applied, otherwise the density falls off. This has the effect of higher density at\nstreet level views.\n\nValid values are between 0.0 and 1.0.\n\n### onTileLoad(tileHeader : Tile3DHeader) : void\n\nIndicate ssthat a tile's content was loaded.\n\nThe loaded `Tile3DHeader` is passed to the event listener.\n\nThis event is fired during the tileset traversal while the frame is being rendered\nso that updates to the tile take effect in the same frame. Do not create or modify\nentities or primitives during the event listener.\n\n```js\n  new Tileset3D({\n    onTileLoad(tileHeader => console.log('A tile was loaded.'));\n  });\n```\n\n### onTileUnload(tileHeader : Tile3DHeader) : void\n\nIndicates that a tile's content was unloaded.\n\nThe unloaded `Tile3DHeaders` is passed to the event listener.\n\nThis event is fired immediately before the tile's content is unloaded while the frame is being\nrendered so that the event listener has access to the tile's content. Do not create\nor modify entities or primitives during the event listener.\n\n```js\n  new Tileset3D({\n    onTileUnload(tile =>  console.log('A tile was unloaded from the cache.'));\n  });\n```\n\nSee\n\n- Tileset3D#maximumMemoryUsage\n- Tileset3D#trimLoadedTiles\n\n### onTileError(tileHeader : Tile3DHeader) : void\n\nCalled to indicate that a tile's content failed to load. By default, error messages will be logged to the console.\n\nThe error object passed to the listener contains two properties:\n\n- `url`: the url of the failed tile.\n- `message`: the error message.\n\n```js\nnew Tileset3D({\n  onTileFailed(tileHeader, url, message) {\n    console.log('An error occurred loading tile: ', url);\n    console.log('Error: ', message);\n  }\n});\n```\n\n### skipLevelOfDetail : Boolean\n\nDefault: true\n\nOptimization option. Determines if level of detail skipping should be applied during the traversal.\n\nThe common strategy for replacement-refinement traversal is to store all levels of the tree in memory and require\nall children to be loaded before the parent can refine. With this optimization levels of the tree can be skipped\nentirely and children can be rendered alongside their parents. The tileset requires significantly less memory when\nusing this optimization.\n\n### baseScreenSpaceError : Number\n\nDefault: 1024\n\nThe screen space error that must be reached before skipping levels of detail.\n\nOnly used when `skipLevelOfDetail` is `true`.\n\n### skipScreenSpaceErrorFactor : Number\n\nDefault: 16\n\nMultiplier defining the minimum screen space error to skip.\nFor example, if a tile has screen space error of 100, no tiles will be loaded unless they\nare leaves or have a screen space error `<= 100 / skipScreenSpaceErrorFactor`.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n\n### skipLevels\n\nDefault: 1\n\nConstant defining the minimum number of levels to skip when loading tiles. When it is 0, no levels are skipped.\nFor example, if a tile is level 1, no tiles will be loaded unless they are at level greater than 2.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n\n### immediatelyLoadDesiredLevelOfDetail : false\n\nWhen true, only tiles that meet the maximum screen space error will ever be downloaded.\nSkipping factors are ignored and just the desired tiles are loaded.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n\n### loadSiblings: false\n\nDetermines whether siblings of visible tiles are always downloaded during traversal.\nThis may be useful for ensuring that tiles are already available when the viewer turns left/right.\n\nOnly used when `Tileset3D.skipLevelOfDetail` is `true`.\n","slug":"modules/3d-tiles/docs/api-reference/tileset-3d","title":"Tileset3D"},{"excerpt":"","rawMarkdownBody":"","slug":"arrowjs/docs/paul-drafts/visitors/index","title":""},{"excerpt":"","rawMarkdownBody":"","slug":"arrowjs/docs/paul-drafts/vectors/index","title":""},{"excerpt":"","rawMarkdownBody":"","slug":"arrowjs/docs/paul-drafts/tables/index","title":""},{"excerpt":"","rawMarkdownBody":"","slug":"arrowjs/docs/paul-drafts/builders/index","title":""},{"excerpt":"","rawMarkdownBody":"","slug":"arrowjs/docs/paul-drafts/data-types/index","title":""},{"excerpt":"","rawMarkdownBody":"","slug":"arrowjs/docs/paul-drafts/ipc/index","title":""},{"excerpt":"Roadmap What's Next for Apache Arrow in Javascript There are a lot of features we'd like to add over the next few Javascript releases…","rawMarkdownBody":"# Roadmap\n\nWhat's Next for Apache Arrow in Javascript\n\nThere are a lot of features we'd like to add over the next few Javascript releases:\n\n* **Inline predicates**: Function calls in the inner loop of a scan over millions of records can be very expensive. We can potentially save that time by generating a new scan function with the predicates inlined when a filter is created.\n\n* **Cache filter results**: Right now every time we do a scan on a filtered DataFrame we re-check the predicate on every row. There should be an (optional?) lazily computed index to store the predicate results for subsequent re-use.\n\n* **Friendlier API**: I shouldn't have to write a custom scan function just to take a look at the results of a filter! Every DataFrame should have a toJSON() function (See ARROW-2202).\n\n* **node.js ↔ (Python, C++, Java, ...) interaction**: A big benefit of Arrow's common in-memory format is that different tools can operate on the same memory. Unfortunately we're pretty closed off in the browser, but node doesn't have that problem! Finishing ARROW-1700, node.js Plasma store client should make this type of interaction possible.\n\nHave an idea? Tell us! Generally JIRAs are preferred but we'll take GitHub issues too. If you just want to discuss something, reach out on the mailing list or slack. But PRs are the best of all, we can always use more contributors!\n\n\n## Feature Completeness\n\nIdeally each Apache Arrow language binding would offer the same set of features, at least to the extent that the language/platform in question allows. In practice however, not all features have been implemented in all language bindings.\n\nIn comparison with the C++ Arrow API bindings, there are some missing features in the JavaScript bindings:\n\n- Tensors are not yet supported.\n- No explicit support for Apache Arrow Flight\n","slug":"arrowjs/docs/roadmap","title":"Roadmap"},{"excerpt":"Contributing This page contains information for Arrow JS contributors. API Design Notes Understanding some of the design decisions made when…","rawMarkdownBody":"# Contributing\n\nThis page contains information for Arrow JS contributors.\n\n## API Design Notes\n\nUnderstanding some of the design decisions made when defining the JavaScript binding API may help facilitate a better appreciateion of why the API is designed the way it is:\n\n- To facilitate keeping the evolution of the JavaScript bindings matched to other bindings, the JavaScript Arrow API is designed to be close match to the C++ Arrow API, although some differences have been made where it makes sense. Some design patterns, like the way `RecordBatchReader.from()` returns different `RecordBatchReader` subclasses depending on what source is being read.\n\n\n## Editing Documentation\n\n### Markdown vs JSDoc\n\nSince the Arrow JavaScript API includes both manually written markdown and \"automatically\" generated jsdoc. Some main differences are:\n\n- The markdown version contains a \"Developer Guide\" which is not present in the jsdoc.\n- The markdown version of the \"API reference\" focuses on readability. It contains more text with semantic descriptions and examples of usage of classes and functions. It also omits more complex typescript annotions for function prototypes to ensure that the API documentation is easy to digest for all JavaScript programmers.\n- The jsdoc version includes the full Typescript type information and is more richly hyperlinked and can be valuable to developers as a supplement to the markdown reference when those particular details matter.\n\n### Updating Docs\n\nIn general, the markdown docs should be considered the source of truth for the JavaScript API:\n\n* To avoid excessive duplication and possible divergence between markdown and JSDoc, it is recommended that the JSDoc version contains brief summary texts only.\n* Reviewers should make sure that PRs affecting the JS API (bothk features and bug fixes) contain appropriate changes to the markdown docs (in the same way that such PRs must contain appropriate changes to e.g. test cases).\n* When appropriate, to ensure the markdown docs remain \"the source of truth\" for the Arrow JS API, bugs should be reviewed first towards the markdown documentation, e.g. to see if the documented behavior is incorrectly specified and needs to be fixed.\n","slug":"arrowjs/docs/contributing","title":"Contributing"},{"excerpt":"Introduction Apache Arrow is a binary specification and set of libraries for representing Tables and Columns of strongly-typed fixed-width…","rawMarkdownBody":"# Introduction\n\nApache Arrow is a binary specification and set of libraries for representing Tables and Columns of strongly-typed fixed-width, variable-width, and nested data structures in-memory and over-the-wire.\n\nArrow represents columns of values in sets of contiguous buffers. This is in contrast to a row-oriented representation, where the values for each row are stored in a contiguous buffer. The columnar representation makes it easier to take advantage of SIMD instruction sets in modern CPUs and GPUs, and can lead to dramatic performance improvements processing large amounts of data.\n\n## Components\n\nThe Arrow library is organized into separate components responsible for creating, reading, writing, serializing, deserializing, or manipulating Tables or Columns.\n\n* [Data Types](docs/paul-drafts/introduction.md#arrow-data-types) - Classes that define the fixed-width, variable-width, and composite data types Arrow can represent\n* [Vectors](docs/paul-drafts/introduction.md#arrow-vectors) - Classes to read and decode JavaScript values from the underlying buffers or Vectors for each data type\n* [Builders](docs/paul-drafts/introduction.md#arrow-builders) - Classes to write and encode JavaScript values into the underlying buffers or Vectors for each data type\n* [Visitors](docs/paul-drafts/introduction.md#arrow-visitors) - Classes to traverse, manipulate, read, write, or aggregate values from trees of Arrow Vectors or DataTypes\n* [IPC Readers and Writers](docs/paul-drafts/introduction.md#arrow-ipc-primitives) - Classes to read and write the Arrow IPC (inter-process communication) binary file and stream formats\n* [Fields, Schemas, RecordBatches, Tables, and Columns](docs/paul-drafts/introduction.md#fields-schemas-recordbatches-tables-and-columns) - Classes to describe, manipulate, read, and write groups of strongly-typed Vectors or Columns\n\n## [Data Types](docs/paul-drafts/data-types/index.md)\n\nAt the heart of Arrow is set of well-known logical [data types](docs/paul-drafts/data-types/index.md), ensuring each Column in an Arrow Table is strongly-typed. These data types define how a Column's underlying buffers should be constructed and read, and includes configurable (and custom) metadata fields for further annotating a Column. A Schema describing each Column's name and data type is encoded alongside each Column's data buffers, allowing you to consume an Arrow data source without knowing the data types or column layout beforehand.\n\nEach data type falls into one of three rough categories: Fixed-width types, variable-width types, or composite types that contain other Arrow data types. All data types can represent null values, which are stored in a separate validity [bitmask](https://en.wikipedia.org/wiki/Mask_(computing)). Follow the links below for a more detailed description of each data type.\n\n### [Fixed-width Data Types](docs/paul-drafts/data-types/index.md#fixed-width-data-types)\n\nFixed-width data types describe physical primitive values (bytes or bits of some fixed size), or logical values that can be represented as primitive values. In addition to an optional [`Uint8Array`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Uint8Array) validity bitmask, these data types have a physical data buffer (a [`TypedArray`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/TypedArray#TypedArray_objects) corresponding to the data type's physical element width).\n\n * [Null](docs/paul-drafts/data-types/index.md#null) - A column of NULL values having no physical storage\n * [Bool](docs/paul-drafts/data-types/index.md#bool) - Booleans as either 0 or 1 (bit-packed, LSB-ordered)\n * [Int](docs/paul-drafts/data-types/index.md#int) - Signed or unsigned 8, 16, 32, or 64-bit little-endian integers\n * [Float](docs/paul-drafts/data-types/index.md#float) - 2, 4, or 8-byte floating point values\n * [Decimal](docs/paul-drafts/data-types/index.md#decimal) - Precision-and-scale-based 128-bit decimal values\n * [FixedSizeBinary](docs/paul-drafts/data-types/index.md#fixedsizebinary) - A list of fixed-size binary sequences, where each value occupies the same number of bytes\n * [Date](docs/paul-drafts/data-types/index.md#date) - Date as signed 32-bit integer days or 64-bit integer milliseconds since the UNIX epoch\n * [Time](docs/paul-drafts/data-types/index.md#time) - Time as signed 32 or 64-bit integers, representing either seconds, millisecond, microseconds, or nanoseconds since midnight (00:00:00)\n * [Timestamp](docs/paul-drafts/data-types/index.md#timestamp) - Exact timestamp as signed 64-bit integers, representing either seconds, milliseconds, microseconds, or nanoseconds since the UNIX epoch\n * [Interval](docs/paul-drafts/data-types/index.md#interval) - Time intervals as pairs of either (year, month) or (day, time) in SQL style\n * [FixedSizeList](docs/paul-drafts/data-types/index.md#fixedsizelist) - Fixed-size sequences of another logical Arrow data type\n\n### [Variable-width Data Types](docs/paul-drafts/data-types/index.md#variable-width-data-types)\n\nVariable-width types describe lists of values with different widths, including binary blobs, Utf8 code-points, or slices of another underlying Arrow data type. These types store the values contiguously in memory, and have a physical [`Int32Array`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Int32Array) of offsets that describe the start and end indicies of each list element.\n\n * [List](docs/paul-drafts/data-types/list.md) - Variable-length sequences of another logical Arrow data type\n * [Utf8](docs/paul-drafts/data-types/utf8.md) - Variable-length byte sequences of UTF8 code-points (strings)\n * [Binary](docs/paul-drafts/data-types/binary.md) - Variable-length byte sequences (no guarantee of UTF8-ness)\n\n### [Composite Data Types](docs/paul-drafts/data-types/index.md#composite-data-types)\n\nComposite types don't have physical data buffers of their own. They contain other Arrow data types and delegate work to them.\n\n * [Union](docs/paul-drafts/data-types/union.md) - Union of logical child data types\n * [Map](docs/paul-drafts/data-types/map.md) - Map of named logical child data types\n * [Struct](docs/paul-drafts/data-types/struct.md) - Struct of ordered logical child data types\n","slug":"arrowjs/docs/paul-drafts/introduction","title":"Introduction"},{"excerpt":"What's New v0.4.1 TBA v0.4.0 TBA v0.3.0 TBA v0.3.0 TBA","rawMarkdownBody":"# What's New\n\n# v0.4.1\n\nTBA\n\n\n# v0.4.0\n\nTBA\n\n\n# v0.3.0\n\nTBA\n\n\n# v0.3.0\n\nTBA\n","slug":"arrowjs/docs/whats-new","title":"What's New"},{"excerpt":"Introduction The Arrow JavaScript API is designed to helps applications tap into the full power of working with binary columnar data in the…","rawMarkdownBody":"# Introduction\n\nThe Arrow JavaScript API is designed to helps applications tap into the full power of working with binary columnar data in the Apache Arrow format. Arrow JS has a rich set of classes that supports use cases such as batched loading and writing, as well performing data frame operations on Arrow encoded data, including applying filters, iterating over tables, etc.\n\n## Getting Started\n\nTo install and start coding with Apache Arrow JS bindings, see the [Getting Started](docs/get-started).\n\n\n## About Apache Arrow\n\nApache Arrow is a performance-optimized binary columnar memory layout specification for encoding vectors and table-like containers of flat and nested data. The Arrow spec is design to eliminate memory copies and aligns columnar data in memory to minimize cache misses and take advantage of the latest SIMD (Single input multiple data) and GPU operations on modern processors.\n\nApache Arrow is emerging as the standard for large in-memory columnar data (Spark, Pandas, Drill, Graphistry, ...). By standardizing on a common binary interchange format, big data systems can reduce the costs and friction associated with cross-system communication.\n\n\n## Resources\n\nThere are some excellent resources available that can help you quickly get a feel for what capabilities the Arrow JS API offers:\n\n* Observable: [Introduction to Apache Arrow](https://observablehq.com/@theneuralbit/introduction-to-apache-arrow)\n* Observable: [Using Apache Arrow JS with Large Datasets](https://observablehq.com/@theneuralbit/using-apache-arrow-js-with-large-datasets)\n* Observable: [Manipulating Flat Arrays, Arrow-Style](https://observablehq.com/@lmeyerov/manipulating-flat-arrays-arrow-style)\n* [Manipulating Flat Arrays](https://observablehq.com/@mbostock/manipulating-flat-arrays) General article on Columnar Data and Data Frames\n\nApache Arrow project links:\n\n* [Apache Arrow Home](https://arrow.apache.org/)\n* [Apache Arrow JS on github](https://github.com/apache/arrow/tree/master/js)\n* [Apache Arrow JS on npm](https://www.npmjs.com/package/apache-arrow)\n","slug":"arrowjs/docs","title":"Introduction"},{"excerpt":"Examples Some short examples Get a table from an Arrow file on disk (in IPC format) Create a Table when the Arrow file is split across…","rawMarkdownBody":"# Examples\n\nSome short examples\n\n### Get a table from an Arrow file on disk (in IPC format)\n\n```js\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\n\nconst arrow = readFileSync('simple.arrow');\nconst table = Table.from([arrow]);\n\nconsole.log(table.toString());\n\n/*\n foo,  bar,  baz\n   1,    1,   aa\nnull, null, null\n   3, null, null\n   4,    4,  bbb\n   5,    5, cccc\n*/\n```\n\n### Create a Table when the Arrow file is split across buffers\n\n```js\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\n\nconst table = Table.from([\n    'latlong/schema.arrow',\n    'latlong/records.arrow'\n].map((file) => readFileSync(file)));\n\nconsole.log(table.toString());\n\n/*\n        origin_lat,         origin_lon\n35.393089294433594,  -97.6007308959961\n35.393089294433594,  -97.6007308959961\n35.393089294433594,  -97.6007308959961\n29.533695220947266, -98.46977996826172\n29.533695220947266, -98.46977996826172\n*/\n```\n\n### Create a Table from JavaScript arrays\n\n```js\nimport {\n  Table,\n  FloatVector,\n  DateVector\n} from 'apache-arrow';\n\nconst LENGTH = 2000;\n\nconst rainAmounts = Float32Array.from(\n  { length: LENGTH },\n  () => Number((Math.random() * 20).toFixed(1)));\n\nconst rainDates = Array.from(\n  { length: LENGTH },\n  (_, i) => new Date(Date.now() - 1000 * 60 * 60 * 24 * i));\n\nconst rainfall = Table.new(\n  [FloatVector.from(rainAmounts), DateVector.from(rainDates)],\n  ['precipitation', 'date']\n);\n```\n\n### Load data with `fetch`\n\n```js\nimport { Table } from \"apache-arrow\";\n\nconst table = await Table.from(fetch((\"/simple.arrow\")));\nconsole.log(table.toString());\n\n```\n\n### Columns look like JS Arrays\n\n```js\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\n\nconst table = Table.from([\n    'latlong/schema.arrow',\n    'latlong/records.arrow'\n].map(readFileSync));\n\nconst column = table.getColumn('origin_lat');\n\n// Copy the data into a TypedArray\nconst typed = column.toArray();\nassert(typed instanceof Float32Array);\n\nfor (let i = -1, n = column.length; ++i < n;) {\n    assert(column.get(i) === typed[i]);\n}\n```\n","slug":"arrowjs/docs/get-started/examples","title":"Examples"},{"excerpt":"Installing Installing Arrow JS The Apache Arrow JS bindings are published as an npm module. Importing Arrow JS You should now be able to…","rawMarkdownBody":"# Installing\n\n## Installing Arrow JS\n\nThe Apache Arrow JS bindings are published as an npm module.\n\n```sh\nnpm install apache-arrow\n# or\nyarn add apache-arrow\n```\n\n\n## Importing Arrow JS\n\nYou should now be able to import arrow into your projects\n\n```js\nimport {Table} from 'apache-arrow';\n```\n","slug":"arrowjs/docs/get-started/installing","title":"Installing"},{"excerpt":"Extracting Data While keeping data in Arrow format allows for efficient data frame operations, there are of course cases where data needs to…","rawMarkdownBody":"# Extracting Data\n\nWhile keeping data in Arrow format allows for efficient data frame operations, there are of course cases where data needs to be extracted in a form that can be use with non-Arrow-aware JavaScript code.\n\n### Converting Data\n\nMany arrow classes support the following methods:\n\n* `toArray()` - Typically returns a typed array.\n* `toJSON()` - Arrow JS types can be converted to JSON.\n* `toString()` - Arrow JS types can be converted to strings.\n\n### Extracting Data by Row\n\nYou can get a temporary object representing a row in a table.\n\n```js\nconst row = table.get(0);\n```\n\nNote that the `row` does not retain the schema, so you'll either need to know the order of columns `row.get(0)`, or use the `to*()` methods.\n\n### Extracting Data by Column\n\nMore efficient is to get a column.\n\n```js\nconst column = table.getColumn('data');\n```\n\nThe column can be chunked, so to get a contiguous (typed) array, call\n\n```js\nconst array = table.getColumn('columnName').toArray();\n```\n\nNote that if there are multiple chunks in the array, this will create a new typed array and copy the typed arrays in the chunks into that array.\n\n### Extracting data by Column and Batch\n\nA more efficient (zero-copy) way to get access to data (especially if the table has not been sliced or filtered) could be to walk through the chunks in each column and get the underlying typed array for that chunk.\n","slug":"arrowjs/docs/developer-guide/converting-data","title":"Extracting Data"},{"excerpt":"Working with BigInts Arrow supports big integers. If the JavaScript platform supports the recently introduced  typed array, Arrow JS will…","rawMarkdownBody":"# Working with BigInts\n\nArrow supports big integers.\n\nIf the JavaScript platform supports the recently introduced `BigInt64Array` typed array, Arrow JS will use this type.\n\nFor convenience ArrowJS inject additional methods (on the object instance) that lets it be converted to JSON, strings, values and primitives\n\n* `bigIntArray.toJSON()`\n* `bigIntArray.toString()`\n* `bigIntArray.valueOf()`\n* `bigIntArray[Symbol.toPrimitive](hint: 'string' | 'number' | 'default')`\n\n## Notes about Conversion Methods\n\nWhen you have one of the wide numeric types (`Int64`, `Uint64`, or `Decimal` which is 128bit), those `Vector` instances always return/accept subarray slices of the underlying 32bit typed arrays.\n\nBut to make life easier for people consuming the typed arrays, the Arrow JS API adds some [extra methods](https://github.com/apache/arrow/blob/3eb07b7ed173e2ecf41d689b0780dd103df63a00/js/src/util/bn.ts#L31) to the typed arrays before they're returned. The goal of these methods is to handle conversion to and from the various primitive types (`number`, `string`, `bigint`, and `JSON.stringify()`) so people usually \"fall into the pit of success\".\n\nOne of the added methods is an implementation of [`[Symbol.toPrimitive]`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Symbol/toPrimitive), which JS will use when doing certain kinds of implicit primitive coercion.\n\n The implementation of these methods is [bifurcated](https://github.com/apache/arrow/blob/3eb07b7ed173e2ecf41d689b0780dd103df63a00/js/src/util/bn.ts#L125), so if you're in an environment with `BigInt` support we use the native type, but if not, we'll make a best-effort attempt to return something meaningful (usually the unsigned decimal representation of the number as a string, though we'd appreciate help if someone knows how to compute the signed decimal representation).\n\nExamples:\n```js\nimport { Int64Vector } from 'apache-arrow';\nimport assert from 'assert';\n\nconst bigIntArr = new BigInt64Array([ 1n + BigInt(Number.MAX_SAFE_INTEGER) ])\nconst lilIntArr = new Int32Array(bigIntArr.buffer)\nassert(bigIntArr.length === 1)\nassert(lilIntArr.length === 2)\n\nconst bigIntVec = Int64Vector.from(bigIntArr)\nassert(bigIntVec.length === 1)\n\nconst bigIntVal = bigIntVec.get(0)\nassert(bigIntVal instanceof Int32Array)\nassert(bigIntVal[0] === 0)\nassert(bigIntVal[1] === 2097152)\n\n// these implicitly call bigIntVal[Symbol.toPrimitive]()\nassert(('' + bigIntVal) == '9007199254740992') // aka bigIntVal[Symbol.toPrimitive]('string')\nassert((0 + bigIntVal) == 9007199254740992) // aka bigIntVal[Symbol.toPrimitive]('number')\nassert((0n + bigIntVal) == 9007199254740992n) // aka bigIntVal[Symbol.toPrimitive]('default')```\n```\n","slug":"arrowjs/docs/developer-guide/big-ints","title":"Working with BigInts"},{"excerpt":"Data Sources and Sinks The Arrow JavaScript API is designed to make it easy to work with data sources both in the browser and in Node.js…","rawMarkdownBody":"# Data Sources and Sinks\n\nThe Arrow JavaScript API is designed to make it easy to work with data sources both in the browser and in Node.js.\n\n\n## Streams\n\nBoth Node and DOM/WhatWG Streams can be used directly as input sources by the Arrow JS API.\n\n## Fetch Responses\n\nFetch responses (Promises) can be used where a data source is expected.\n\n## ArrayBuffers\n\nMost data sources accept `Uint8Arrays`.\n\n## AsyncIterators\n\nAsync iterators are the most general way to abstract \"streaming\" data sources and data sinks and are consistently accepted (and in many cased returned) by the Arrow JS API.\n","slug":"arrowjs/docs/developer-guide/data-sources","title":"Data Sources and Sinks"},{"excerpt":"Data Types Arrow supports a rich set of data types: Fixed-length primitive types: numbers, booleans, date and times, fixed size binary…","rawMarkdownBody":"# Data Types\n\nArrow supports a rich set of data types:\n\n* Fixed-length primitive types: numbers, booleans, date and times, fixed size binary, decimals, and other values that fit into a given number\n* Variable-length primitive types: binary, string\n* Nested types: list, struct, and union\n* Dictionary type: An encoded categorical type\n\n\n### Converting Dates\n\nApache Arrow Timestamp is a 64-bit int of milliseconds since the epoch, represented as two 32-bit ints in JS to preserve precision. The fist number is the \"low\" int and the second number is the \"high\" int.\n\n```js\nfunction toDate(timestamp) {\n  return new Date((timestamp[1] * Math.pow(2, 32) + timestamp[0])/1000);\n}\n```\n\n","slug":"arrowjs/docs/developer-guide/data-types","title":"Data Types"},{"excerpt":"Using Predicates The Arrow API provides standard predicates that allow for the comparison of column values against literals (equality…","rawMarkdownBody":"# Using Predicates\n\n\nThe Arrow API provides standard predicates that allow for the comparison of column values against literals (equality, greater or equal than, less or eqial than) as well as the creation of composite logical expressions (`and`, `or` and `not`) out of individual column comparisons.\n\nIt is of course also possible to write custom predicates, however the performance is best when using the built-ins. Note that for performance reasons, filters are specified using \"predicates\" rather than custom JavaScript functions. For details on available predicates see [Using Predicates]().\n\n## Filtering using Predicates\n\n> Note that calling `filter()` on a `DataFrame` doesn't actually do anything (other than store the predicates). It's not until you call `countBy()` or `scan()` on the resulting object that Arrow actually scans through all of the data.\n\n```js\ntable = table.filter(arrow.predicate.col('winnername').eq(winner));\n\nfor (const row of table) {\n  // only returns rows that match criteria\n}\n```\n","slug":"arrowjs/docs/developer-guide/predicates","title":"Using Predicates"},{"excerpt":"Notes on Memory Management Apache Arrow is a performance-optimized architecture, and the foundation of that performance is the approach to…","rawMarkdownBody":"# Notes on Memory Management\n\nApache Arrow is a performance-optimized architecture, and the foundation of that performance is the approach to memory management. It can be useful to have an understanding of how.\n\n## How Arrow Stores Data\n\nArrow reads in arrow data as arraybuffer(s) and then creates chunks that are \"sub array views\" into that big array buffer, and lists of those chunks are then composed into \"logical\" arrays.\n\nChunks are created for each column in each RecordBatch.\n\nThe chunks can be \"sliced and diced\" by operations on `Column`, `Table` and `DataFrame` objects, but are never copied (as long as flattening is not requested) and are conceptually immutable. (There is a low-level `Vector.set()` method however given that it could modify data that is used by multiple objects its use should be reserved for cases where implications are fully understood).\n","slug":"arrowjs/docs/developer-guide/memory-management","title":"Notes on Memory Management"},{"excerpt":"Using with Typescript This documentation does not include advanced type definitions in the interest of simplicity and making the…","rawMarkdownBody":"# Using with Typescript\n\nThis documentation does not include advanced type definitions in the interest of simplicity and making the documentation accessible to more JavaScript developers. If you are working with Typescript in your application and would benefit from documentation that includes the Typescript definitions, you can refer to the auto generated JSDocs for the API.\n\n## Considerations when Using Typescript\n\nTo ensure that type information \"flows\" correctly from the types of function/constructor arguments to the types of returned objects, some special methods are provided (effectively working around limitations in Typescript).\n\nA key example is the availability of static `new()` methods on a number of classes that are intended to be used instead of calling `new` on the constructor. Accordingly, `Table.new()` is an alternative to `new Table()`, that provides stronger type inference on the returned Table.\n\nYou may want to leverage this syntax if your application is written in Typescript.\n","slug":"arrowjs/docs/developer-guide/typescript","title":"Using with Typescript"},{"excerpt":"Reading and Writing Arrow Data About RecordBatches Arrow tables are typically split into record batches, allowing them to be incrementally…","rawMarkdownBody":"# Reading and Writing Arrow Data\n\n## About RecordBatches\n\nArrow tables are typically split into record batches, allowing them to be incrementally loaded or written, and naturally the Arrow API provides classes to facilite this reading.\n\n\n## Reading Arrow Data\n\nThe `Table` class provides a simple `Table.from` convenience method for reading an Arrow formatted data file into Arrow data structures:\n\n```\nimport { readFileSync } from 'fs';\nimport { Table } from 'apache-arrow';\nconst arrow = readFileSync('simple.arrow');\nconst table = Table.from([arrow]);\nconsole.log(table.toString());\n```\n\n### Using RecordBatchReader to read from a Data Source\n\nTo read Arrow tables incrementally, you use the `RecordBatchReader` class.\n\nIf you only have one table in your file (the normal case), then you'll only need one `RecordBatchReader`:\n\n```js\nconst reader = await RecordBatchReader.from(fetch(path, {credentials: 'omit'}));\nfor await (const batch of reader) {\n  console.log(batch.length);\n}\n```\n\n### Reading Multiple Tables from a Data Source\n\nThe JavaScript Arrow API supports arrow data streams that contain multiple tables (this is an \"extension\" to the arrow spec). Naturally, each Table comes with its own set of record batches, so to read all batches from all tables in the data source you will need a double loop:\n\n```js\nconst readers = RecordBatchReader.readAll(fetch(path, {credentials: 'omit'}));\nfor await (const reader of readers) {\n  for await (const batch of reader) {\n    console.log(batch.length);\n  }\n}\n```\n\nNote: this code also works if there is only one table in the data source, in which case the outer loop will only execute once.\n\n\n# Writing Arrow Data\n\nThe `RecordStreamWriter` class allows you to write Arrow `Table` and `RecordBatch` instances to a data source.\n\n\n## Using Transform Streams\n\n### Connecting to Python Processes\n\nA more complicated example of using Arrow to go from node -> python -> node:\n\n```js\nconst { AsyncIterable } = require('ix');\nconst { child } = require('event-stream');\nconst { fork } = require('child_process');\nconst { RecordBatchStreamWriter } = require('apache-arrow');\n\nconst compute_degrees_via_gpu_accelerated_sql = ((scriptPath) => (edgeListColumnName) =>\n    spawn('python3', [scriptPath, edgeListColumnName], {\n        env: process.env,\n        stdio: ['pipe', 'pipe', 'inherit']\n    })\n)(require('path').resolve(__dirname, 'compute_degrees.py'));\n\nfunction compute_degrees(colName, recordBatchReaders) {\n    return AsyncIterable\n        .as(recordBatchReaders).mergeAll()\n        .pipe(RecordBatchStreamWriter.throughNode())\n        .pipe(compute_degrees_via_gpu_accelerated_sql(colName));\n}\n\nmodule.exports = compute_degrees;\n\n```\n\nThis example construct pipes of streams of events and that python process just reads from stdin, does a GPU-dataframe operation, and writes the results to stdout. (This example uses Rx/IxJS style functional streaming pipelines).\n\n`compute_degrees_via_gpu_accelerated_sql` returns a node `child_process` that is also a duplex stream, similar to the [`event-stream#child()` method](https://www.npmjs.com/package/event-stream#child-child_process)\n","slug":"arrowjs/docs/developer-guide/reading-and-writing","title":"Reading and Writing Arrow Data"},{"excerpt":"Apache Arrow JavaScript API Reference Class List TODO - This is a class list from the C++ docs, it has only been partially updated to match…","rawMarkdownBody":"# Apache Arrow JavaScript API Reference\n\n## Class List\n\n> TODO - This is a class list from the C++ docs, it has only been partially updated to match JS API\n\n| Class             | Summary |\n| ---               | ---     |\n| `Array`           | Array base type Immutable data array with some logical type and some length |\n| `ArrayData`       | Mutable container for generic Arrow array data  |\n| `BinaryArray`     | Concrete Array class for variable-size binary data |\n| `BooleanArray`    | Concrete Array class for boolean data  |\n| `Buffer`          | Object containing a pointer to a piece of contiguous  memory with a particular size |\n| `ChunkedArray`    | A data structure managing a list of primitive Arrow arrays logically as one large array |\n| `Column`          | An immutable column data structure consisting of a field (type metadata) and a chunked data array |\n| `Decimal128`      | Represents a signed 128-bit integer in two's  complement |\n| `Decimal128Array` | Concrete Array class for 128-bit decimal data  |\n| `DictionaryArray` | Concrete Array class for dictionary data  |\n| `Field`           | The combination of a field name and data type, with  optional metadata |\n| `FixedSizeBinaryArray` | Concrete Array class for fixed-size  binary data |\n| `FixedWidthType`  | Base class for all fixed-width data types  |\n| `FlatArray`       | Base class for non-nested arrays  |\n| `FloatingPoint`   | Base class for all floating-point data types  |\n| `Int16Type`       | Concrete type class for signed 16-bit integer data  |\n| `Int32Type`       | Concrete type class for signed 32-bit integer data  |\n| `Int64Type`       | Concrete type class for signed 64-bit integer data  |\n| `Int8Type`        | Concrete type class for signed 8-bit integer data  |\n| `Integer`         | Base class for all integral data types  |\n| `ListArray`       | Concrete Array class for list data  |\n| `ListType`        | Concrete type class for list data  |\n| `NestedType`      | |\n| `NullArray`       | Degenerate null type Array  |\n| `NullType`        | Concrete type class for always-null data  |\n| `Number`          | Base class for all numeric data types  |\n| `NumericArray`    | |\n| `PrimitiveArray`  | Base class for arrays of fixed-size logical  types |\n| `RecordBatch`     | Collection of equal-length arrays matching a  particular Schema |\n| `RecordBatchReader` | Abstract interface for reading stream of  record batches |\n| `Schema`          | Sequence of arrow::Field objects describing the  columns of a record batch or table data structure |\n| `Status`          | |\n| `StringArray`     | Concrete Array class for variable-size string ( utf-8) data |\n| `StructArray`     | Concrete Array class for struct data  |\n| `Table`           | Logical table as sequence of chunked arrays  |\n| `TableBatchReader` | Compute a sequence of record batches from a ( possibly chunked) Table |\n| `TimeUnit`        | |\n| `UnionArray`      | Concrete Array class for union data  |\n","slug":"arrowjs/docs/api-reference","title":"Apache Arrow JavaScript API Reference"},{"excerpt":"Data Frame Operations Part of the power of data frame operations is that they typically do not actually perform any modifications (copying…","rawMarkdownBody":"# Data Frame Operations\n\nPart of the power of data frame operations is that they typically do not actually perform any modifications (copying etc) of the underlying data, and ultimately only impact how iteration over that data is done, and what \"view\" of the data is presented. This allows data frame operations to be extremely performant, even when applied on very big (multi-gigabyte) data aset.\n\nNote that the Arrow JS `Table` class inherits from the `DataFrame` class which is why the examples in this section can use `DataFrame` methods to `Table` instances.\n\nAlso, most of the data frame operations do not modify the original `Table` or `DataFrame`, but rather return a new similar object with new filtering or \"iteration constraints\" applied. So memory is usually not changed or modified during these operations.\n\nReferences:\n* Much of the text in this section is adapted from Brian Hulette's [Introduction to Apache Arrow](https://observablehq.com/@theneuralbit/introduction-to-apache-arrow)\n\n\n## Removing Rows\n\nA simplest way to remove rows from a data frame mey be use `Table.slice(start, end)`. As usual, rather than actually modifying memory, this operation returns a new `Table`/`DataFrame` with iteration constrained to a sub set of the rows in the original frame.\n\n\n## Removing Columns\n\nThe `Table.select(keys: String[])` method drops all columns except the columns with names that match the supplied `keys`.\n\n```js\ntable.select(['name', 'age']); // Drop all colums except name and age\n````\n\n\n## Filtering Rows\n\nAnother way to \"remove\" rows from data frames is to apply filters. Filters effectively \"removes\" rows that don't fullfill the predicates in the filter. For details see the note below.\n\n```js\nconst selectedName = 'myname';\n// Remove all rows with name === 'myname'\nconst dataFrame = table.filter(arrow.predicate.col('name').eq(selectedName));\n```\n\nThe predicates classes provided by arrow allows for the comparison of column values against literals or javascript values (equality, greater or equal than, less or equal than) as well as the creation of composite logical expressions (`and`, `or` and `not`) out of individual column comparisons.\n\nIt is also possible to write custom predicates by supplying an arbitrary JavaScript function to filter a row, however performance is usually best when using the built-in comparison predicates.\n\n> Note that calling `filter()` on a `DataFrame` doesn't actually remove any rows from the underlying data store (it just stores the predicates). It's not until you iterate over the date, e.g. by calling `countBy()` or `scan()` that we actually apply the filter on the rows.\n\n\n## Counting Rows\n\nTo count the number of times different values appear in a table, use `countBy()`.\n\n```js\nconst newTable = table.countBy('column_name');\n```\n\nNote that `countBy()` does not return a modified data frame or table, but instead returns a new `Table` that contains two columns, `value` and `count`. Each distinct value in the specified column in the original table is listed once in `value`, and the corresponding `count` field in the same row indicates how many times it was present in the original table.\n\nNote that the results are not sorted.\n\n## Sorting\n\nDataFrames do not currently support sorting. To sort you need to move the data back to JavaScript arrays.\n\n## Iterating over a DataFrame (Scanning)\n\nThe `DataFrame.scan()` method lets you define a custom function that will be called for each (non-filtered) record in the `DataFrame`.\n\nNote: For simpler use cases, it is recommended to use the Arrow API provided predicates etc rather than writing a custom scan function, as performance will often be better.\n\n\n### Writing a `next` callback for `scan()`\n\nIn order to be more efficient, Arrow data is broken up into batches of records (which is what makes it possible to do concatenations despite the columnar layout, and `DataFrame.scan()` does not hide this implementation detail from you.\n\n\n### Optimizing `scan()` performance with `bind()` callbacks\n\nIn addition to the `next` callback, you can supply a `bind` function for scan to call each time it starts reading from a new `RecordBatch`. `scan` will call these functions as illustrated in the following pseudo-code:\n\n```js\nfor (const batch of batches) {\n  bind(batch);\n  for (const index in batch) {\n    next(index, batch);\n  }\n}\n```\n\nNote:\n* The `index` passed to next only applies to the current RecordBatch, it is not a global index.\n* The current `RecordBatch` is passed to `next`, so it is possible to access data without writing a bind function, but there will be a performance penalty if your data has a lot of batches.\n","slug":"arrowjs/docs/developer-guide/data-frame-operations","title":"Data Frame Operations"},{"excerpt":"Data Untyped storage backing for . Can be thought of as array of  instances. Also contains slice offset (including null bitmaps). Fields…","rawMarkdownBody":"# Data\n\nUntyped storage backing for `Vector`.\n\nCan be thought of as array of `ArrayBuffer` instances.\n\nAlso contains slice offset (including null bitmaps).\n\n\n## Fields\n\nreadonly type: T;\n\nreadonly length: Number;\n\nreadonly offset: Number;\n\nreadonly stride: Number;\n\nreadonly childData: Data[];\n\nreadonly values: Buffers<T>[BufferType.DATA];\n\nreadonly typeIds: Buffers<T>[BufferType.TYPE];\n\nreadonly nullBitmap: Buffers<T>[BufferType.VALIDITY];\n\nreadonly valueOffsets: Buffers<T>[BufferType.OFFSET];\n\nreadonly ArrayType: any;\n\nreadonly typeId: T['TType'];\n\nreadonly buffers: Buffers<T>;\n\nreadonly nullCount: Number;\n\n\n## Static Methods\n\nConvenience methods for creating Data instances for each of the Arrow Vector types.\n\n### Data.Null<T extends Null>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer) : Data\n\n### Data.Int<T extends Int>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Dictionary<T extends Dictionary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Float<T extends Float>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Bool<T extends Bool>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Decimal<T extends Decimal>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Date<T extends Date_>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Time<T extends Time>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Timestamp<T extends Timestamp>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Interval<T extends Interval>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.FixedSizeBinary<T extends FixedSizeBinary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data\n\n### Data.Binary<T extends Binary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data\n\n### Data.Utf8<T extends Utf8>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data\n\n### Data.List<T extends List>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, child: Data<T['valueType']> | Vector<T['valueType']>) : Data\n\n### Data.FixedSizeList<T extends FixedSizeList>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, child: Data | Vector) : Data\n\n### Data.Struct<T extends Struct>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data\n\n### Data.Map<T extends Map_>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data\n\n### Data.Union<T extends SparseUnion>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, children: (Data | Vector)[]) : Data\n\n### Data.Union<T extends DenseUnion>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, valueOffsets: ValueOffsetsBuffer, children: (Data | Vector)[]) : Data\n}\n\n\n## Methods\n\n### constructor(type: T, offset: Number, length: Number, nullCount?: Number, buffers?: Partial<Buffers<T>> | Data<T>, childData?: (Data | Vector)[]);\n\n### clone(type: DataType, offset?: Number, length?: Number, nullCount?: Number, buffers?: Buffers<R>, childData?: (Data | Vector)[]) : Data;\n\n### slice(offset: Number, length: Number) : Data\n\n\n","slug":"arrowjs/docs/api-reference/data","title":"Data"},{"excerpt":"Working with Tables References: Much of the text in this section is adapted from Brian Hulette's Using Apache Arrow JS with Large Datasets…","rawMarkdownBody":"# Working with Tables\n\nReferences:\n* Much of the text in this section is adapted from Brian Hulette's [Using Apache Arrow JS with Large Datasets](https://observablehq.com/@theneuralbit/using-apache-arrow-js-with-large-datasets)\n\n\n## Loading Arrow Data\n\nApplications often start with loading some Arrow formatted data. The Arrow API provides several ways to do this, but in many cases, the simplest approach is to use `Table.from()`.\n\n```js\nimport {Table} from 'apache-arrow';\nconst response = await fetch(dataUrl);\nconst arrayBuffer = await response.arrayBuffer();\nconst dataTable = arrow.Table.from(new Uint8Array(arrayBuffer));\n```\n\n## Getting Records Count\n\n```js\nconst count = table.count();\n```\n\n### Getting Arrow Schema Metadata\n\n```js\nconst fieldNames = table.schema.fields.map(f => f.name);\n// Array(3) [\"Latitude\", \"Longitude\", \"Date\"]\n```\n\n```js\nconst fieldTypes = tables.schema.fields.map(f => f.type)\n// Array(3) [Float, Float, Timestamp]\n\nconst fieldTypeNames = ...;\n// Array(3) [\"Float64\", \"Float64\", \"Timestamp<MICROSECOND>\"]\n```\n\n### Accessing Arrow Table Row Data\n\n```js\nconst firstRow = tables.get(0) // 1st row data\nconst lastRow = tables.get(rowCount-1)\n```\n\n## Record toJSON and toArray\n\nIt is easy to converting Rows to JSON/Arrays/Strings:\n\n```js\ntoJSON = Array(3) [41.890751259, -87.71617311899999, Int32Array(2)]\ntoArray = Array(3) [41.933659084, -87.72369064600001, Int32Array(2)]\n```\n\nSimilar conversion methods are avaiable on many Arrow classes.\n\ntables.get(0).toJSON()\n\n## Slicing Arrow Data\n\nevery10KRow = Array(17) [Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3)]\n\nOur custom arrow data range stepper for sampling data:\n\nrange = ƒ(start, end, step)\n\n### Iterating over Rows and Cells\n\n```js\nfor (let row of dataFrame) {\n  for (let cell of row) {\n    if ( Array.isArray(cell) ) {\n      td = '[' + cell.map((value) => value == null ? 'null' : value).join(', ') + ']';\n    } else if (fields[k] === 'Date') {\n      td = toDate(cell); // convert Apache arrow Timestamp to Date\n    } else {\n      td = cell.toString();\n    }\n    k++;\n  }\n}\n```\n\n\n### Converting Dates\n\nApache Arrow Timestamp is a 64-bit int of milliseconds since the epoch, represented as two 32-bit ints in JS to preserve precision. The fist number is the \"low\" int and the second number is the \"high\" int.\n\n```js\nfunction toDate(timestamp) {\n  return new Date((timestamp[1] * Math.pow(2, 32) + timestamp[0])/1000);\n}\n```\n\n\n### Column Data Vectors\n\nApache Arrow stores columns in typed arrays and vectors:\n\nTyped vectors have convinience methods to convert Int32 arrays data to JS values you can work with.\n\nFor example, to get timestamps in milliseconds:\n\ntimestamps = Array(10) [2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01]\n\n### Filtering Timestamped Data\n\n```js\nfunction filterByDate(startDate, endDate) {\n  const dateFilter = arrow.predicate.custom(i => {\n  \tconst arrowDate = table.getColumn('Date').get(i);\n    const date = toDate(arrowDate);\n    return date >= startDate && date <= endDate;\n  }, b => 1);\n\n  const getDate;\n  const results = [];\n  table.filter(dateFilter)\n    .scan(\n      index => {\n        results.push({\n          'date': toDate(getDate(index))\n        });\n      },\n      batch => {\n        getDate = arrow.predicate.col('Date').bind(batch);\n      }\n    );\n\n  return results;\n}\n```\n\nOur custom filter by date method uses custom arrow table predicate filter and scan methods to generate JS friendly data you can map or graph:\n","slug":"arrowjs/docs/developer-guide/tables","title":"Working with Tables"},{"excerpt":"DataFrame Extends  Methods filter(predicate: Predicate) : FilteredDataFrame Returns: A  which is a subclass of , allowing you to chain…","rawMarkdownBody":"# DataFrame\n\nExtends `Table`\n\n## Methods\n\n### filter(predicate: Predicate) : FilteredDataFrame\n\nReturns: A `FilteredDataFrame` which is a subclass of `DataFrame`, allowing you to chain additional data frame operations, including applying additional filters.\n\nNote that this operation just registers filter predicates and is this very cheap to call. No actual filtering is done until iteration starts.\n\n### scan(next: Function, bind?: Function)\n\nPerformantly iterates over all non-filtered rows in the data frame.\n\n* `next` `(idx: number, batch: RecordBatch) => void` -\n* `bind` `(batch: RecordBatch) => void` - Optional, typically used to generate high-performance per-batch accessor functions for `next`.\n\n### countBy(name: Col | String) : CountByResult\n\n","slug":"arrowjs/docs/api-reference/data-frame","title":"DataFrame"},{"excerpt":"Dictionary A  stores index-to-value maps for dictionary encoded columns. Fields indices: V readonly dictionary: Vector readonly Static…","rawMarkdownBody":"# Dictionary\n\nA `Dictionary` stores index-to-value maps for dictionary encoded columns.\n\n\n## Fields\n\n### indices: V<TKey> readonly\n### dictionary: Vector<T> readonly\n\n## Static Methods\n\n### Dictionary.from(values: Vector, indices: TKey, keys: ArrayLike<number> | TKey['TArray']) : Dictionary\n\n## Methods\n\n### constructor(data: Data)\n\n### reverseLookup(value: T): number\n\n### getKey(idx: number): TKey['TValue'] | null\n\n### getValue(key: number): T['TValue'] | null\n\n### setKey(idx: number, key: TKey['TValue'] | null): void\n\n### setValue(key: number, value: T['TValue'] | null): void\n","slug":"arrowjs/docs/api-reference/dictionary","title":"Dictionary"},{"excerpt":"Column An immutable column data structure consisting of a field (type metadata) and a chunked data array. Usage Copy a column Get a…","rawMarkdownBody":"# Column\n\nAn immutable column data structure consisting of a field (type metadata) and a chunked data array.\n\n## Usage\n\nCopy a column\n```js\nconst typedArray = column.slice();\n```\n\nGet a contiguous typed array from a `Column` (creates a new typed array unless only one chunk)\n```js\nconst typedArray = column.toArray();\n```\n\ncolumns are iterable\n```js\nlet max = column.get(0);\nlet min = max;\nfor (const value of column) {\n  if      (value > max) max = value;\n  else if (value < min) min = value;\n}\n```\n\n\n## Inheritance\n\nColumn extends [`Chunked`](modules/arrow/docs/api-reference/chunked.md)\n\n\n## Fields\n\nIn addition to fields inherited from `Chunked`, Colum also defines\n\n### name : String\n\nThe name of the column (short for `field.name`)\n\n### field : Field\n\nReturns the `Field` instance that describes for the column.\n\n\n## Methods\n\n\n### constructor(field : Field, vectors: Vector, offsets?: Uint32Array)\n\n\n### clone\n\nReturns a new `Column` instance with the same properties.\n\n\n### getChildAt(index : Number) : Vector\n\nReturns the `Vector` that contains the element with \n","slug":"arrowjs/docs/api-reference/column","title":"Column"},{"excerpt":"RecordBatchReader The RecordBatchReader is the IPC reader for reading chunks from a stream or file Usage The JavaScript API supports…","rawMarkdownBody":"# RecordBatchReader\n\nThe RecordBatchReader is the IPC reader for reading chunks from a stream or file\n\n## Usage\n\nThe JavaScript API supports streaming multiple arrow tables over a single socket.\n\nTo read all batches from all tables in a data source:\n\n```js\nconst readers = RecordBatchReader.readAll(fetch(path, {credentials: 'omit'}));\nfor await (const reader of readers) {\n    for await (const batch of reader) {\n        console.log(batch.length);\n    }\n}\n```\n\nIf you only have one table (the normal case), then there'll only be one RecordBatchReader/the outer loop will only execute once. You can also create just one reader via\n\n```js\nconst reader = await RecordBatchReader.from(fetch(path, {credentials: 'omit'}));\n```\n\n\n## Methods\n\n### readAll() : `AsyncIterable<RecordBatchReader>`\n\nReads all batches from all tables in the data source.\n\n\n### from(data : \\*) : RecordBatchFileReader \\| RecordBatchStreamReader\n\n`data`\n* Array\n* fetch response object\n* stream\n\n\nThe `RecordBatchReader.from` method will also detect which physical representation it's working with (Streaming or File), and will return either a `RecordBatchFileReader` or `RecordBatchStreamReader` accordingly.\n\n\n\nRemarks:\n* if you're fetching the table from a node server, make sure the content-type is `application/octet-stream`\n\n\n\n### toNodeStream()\n### pipe()\n\nYou can also turn the RecordBatchReader into a stream\nif you're in node, you can use either toNodeStream() or call the pipe(writable) methods\n\n\n\nin the browser (assuming you're using the UMD or \"browser\" fields in webpack), you can call\n\n### toDOMStream() or\n### pipeTo(writable)/pipeThrough(transform)\n\nIn the browser (assuming you're using the UMD or \"browser\" fields in webpack), you can call `toDOMStream()` or `pipeTo(writable)`/`pipeThrough(transform)`\n\nYou can also create a transform stream directly, instead of using `RecordBatchReader.from()`\n\nYou can also create a transform stream directly, instead of using `RecordBatchReader.from()`\n\n### throughNode\n### throughDOM\n\nvia `throughNode()` and `throughDOM()` respectively:\n\n1. https://github.com/apache/arrow/blob/49b4d2aad50e9d18cb0a51beb3a2aaff1b43e168/js/test/unit/ipc/reader/streams-node-tests.ts#L54\n2. https://github.com/apache/arrow/blob/49b4d2aad50e9d18cb0a51beb3a2aaff1b43e168/js/test/unit/ipc/reader/streams-dom-tests.ts#L50\n\nBy default the transform streams will only read one table from the source readable stream and then close, but you can change this behavior by passing `{ autoDestroy: false }` to the transform creation methods\n\n\n## Remarks\n\n* Reading from multiple tables (`readAll()`) is technically an extension in the JavaScript Arrow API compared to the Arrow C++ API. The authors found it was useful to be able to send multiple tables over the same physical socket\nso they built the ability to keep the underlying socket open and read more than one table from a stream.\n* Note that Arrow has two physical representations, one for streaming, and another for random-access so this only applies to the streaming representation.\n* The IPC protocol is that a stream of ordered Messages are consumed atomically. Messages can be of type `Schema`, `DictionaryBatch`, `RecordBatch`, or `Tensor` (which we don't support yet). The Streaming format is just a sequence of messages with Schema first, then `n` `DictionaryBatches`, then `m` `RecordBatches`.\n","slug":"arrowjs/docs/api-reference/record-batch-reader","title":"RecordBatchReader"},{"excerpt":"Field The combination of a field name and data type, with optional metadata. Fields are used to describe the individual constituents of a…","rawMarkdownBody":"# Field\n\nThe combination of a field name and data type, with optional metadata. Fields are used to describe the individual constituents of a nested DataType or a Schema.\n\n\n## Members\n\n### name : String (read only)\n\nThe name of this field.\n\n### type : Type (read only)\n\nThe type of this field.\n\n### nullable : Boolean (read only)\n\nWhether this field can contain `null` values, in addition to values of `Type` (this creates an extra null value map).\n\n### metadata : Object | null (read only)\n\nA field's metadata is represented by a map which holds arbitrary key-value pairs. Returns `null` if no metadata has been set.\n\n### typeId : ?\n\nTBD?\n\n### indices : ?\n\nTBD? Used if data type is a dictionary.\n\n\n## Methods\n\n### constructor(name : String, nullable?: Boolean, metadata?: Object)\n\nCreates an instance of `Field` with parameters initialized as follows:\n\n* `name` - Name of the column\n* `nullable`=`false` - Whether a null-array is maintained.\n* `metadata`=`null` - Map of metadata\n","slug":"arrowjs/docs/api-reference/field","title":"Field"},{"excerpt":"RecordBatchWriter The  \"serializes\" Arrow Tables (or streams of RecordBatches) to the Arrow File, Stream, or JSON representations for inter…","rawMarkdownBody":"## RecordBatchWriter\n\nThe `RecordBatchWriter` \"serializes\" Arrow Tables (or streams of RecordBatches) to the Arrow File, Stream, or JSON representations for inter-process communication (see also: [Arrow IPC format docs](https://arrow.apache.org/docs/format/IPC.html#streaming-format)).\n\nThe RecordBatchWriter is conceptually a \"transform\" stream that transforms Tables or RecordBatches into binary `Uint8Array` chunks that represent the Arrow IPC messages (`Schema`, `DictionaryBatch`, `RecordBatch`, and in the case of the File format, `Footer` messages).\n\nThese binary chunks are buffered inside the `RecordBatchWriter` instance until they are consumed, typically by piping the RecordBatchWriter instance to a Writable Stream (like a file or socket), enumerating the chunks via async-iteration, or by calling `toUint8Array()` to create a single contiguous buffer of the concatenated results once the desired Tables or RecordBatches have been written.\n\nRecordBatchWriter conforms to the `AsyncIterableIterator` protocol in all environments, and supports two additional stream primitives based on the environment (nodejs or browsers) available at runtime.\n\n* In nodejs, the `RecordBatchWriter` can be converted to a `ReadableStream`, piped to a `WritableStream`, and has a static method that returns a `TransformStream` suitable in chained `pipe` calls.\n* browser environments that support the [DOM/WhatWG Streams Standard](https://github.com/whatwg/streams), corresponding methods exist to convert `RecordBatchWriters` to the DOM `ReadableStream`, `WritableStream`, and `TransformStream` variants.\n\n*Note*: The Arrow JSON representation is not suitable as an IPC mechanism in real-world scenarios. It is used inside the Arrow project as a human-readable debugging tool and for validating interoperability between each language's separate implementation of the Arrow library.\n\n\n## Member Fields\n\nclosed: Promise (readonly)\n\nA Promise which resolves when this `RecordBatchWriter` is closed.\n\n## Static Methods\n\n### RecordBatchWriter.throughNode(options?: Object): DuplexStream\n\nCreates a Node.js `TransformStream` that transforms an input `ReadableStream` of Tables or RecordBatches into a stream of `Uint8Array` Arrow Message chunks.\n\n- `options.autoDestroy`: boolean - (default: `true`) Indicates whether the RecordBatchWriter should close after writing the first logical stream of RecordBatches (batches which all share the same Schema), or should continue and reset each time it encounters a new Schema.\n- `options.*` - Any Node Duplex stream options can be supplied\n\nReturns: A Node.js Duplex stream\n\nExample:\n\n```js\n\nconst fs = require('fs');\nconst { PassThrough, finished } = require('stream');\nconst { Table, RecordBatchWriter } = require('apache-arrow');\n\nconst table = Table.new({\n    i32: Int32Vector.from([1, 2, 3]),\n    f32: Float32Vector.from([1.0, 1.5, 2.0]),\n});\n\nconst source = new PassThrough({ objectMode: true });\n\nconst result = source\n    .pipe(RecordBatchWriter.throughNode())\n    .pipe(fs.createWriteStream('table.arrow'));\n\nsource.write(table);\nsource.end();\n\nfinished(result, () => console.log('done writing table.arrow'));\n```\n\n### RecordBatchWriter.throughDOM(writableStrategy? : Object, readableStrategy? : Object) : Object\n\nCreates a DOM/WhatWG `ReadableStream`/`WritableStream` pair that together transforms an input `ReadableStream` of Tables or RecordBatches into a stream of `Uint8Array` Arrow Message chunks.\n\n- `options.autoDestroy`: boolean - (default: `true`) Indicates whether the RecordBatchWriter should close after writing the first logical stream of RecordBatches (batches which all share the same Schema), or should continue and reset each time it encounters a new Schema.\n- `writableStrategy.*`= - Any options for QueuingStrategy\\<RecordBatch\\>\n- `readableStrategy.highWaterMark`? : Number\n- `readableStrategy.size`?: Number\n\nReturns: an object with the following fields:\n\n- `writable`: WritableStream\\<Table | RecordBatch\\>\n- `readable`: ReadableStream\\<Uint8Array\\>\n\n\n\n\n## Methods\n\nconstructor(options? : Object)\n\n* `options.autoDestroy`: boolean -\n\n\n### toString(sync: Boolean): string | Promise<string>\n\n### toUint8Array(sync: Boolean): Uint8Array | Promise<Uint8Array>\n\n\n### writeAll(input: Table | Iterable<RecordBatch>): this\n### writeAll(input: AsyncIterable<RecordBatch>): Promise<this>\n### writeAll(input: PromiseLike<AsyncIterable<RecordBatch>>): Promise<this>\n### writeAll(input: PromiseLike<Table | Iterable<RecordBatch>>): Promise<this>\n\n* [Symbol.asyncIterator](): AsyncByteQueue<Uint8Array>\n\nReturns An async iterator that produces Uint8Arrays.\n\n### toDOMStream(options?: Object): ReadableStream<Uint8Array>\n\nReturns a new DOM/WhatWG stream that can be used to read the Uint8Array chunks produced by the RecordBatchWriter\n\n- `options` - passed through to the DOM ReadableStream constructor, any DOM ReadableStream options.\n\n### toNodeStream(options?: Object): Readable\n\n- `options` - passed through to the Node ReadableStream constructor, any Node ReadableStream options.\n\n### close() : void\n\nClose the RecordBatchWriter. After close is called, no more chunks can be written.\n\n### abort(reason?: any) : void\n### finish() : this\n### reset(sink?: WritableSink<ArrayBufferViewInput>, schema?: Schema | null): this\n\nChange the sink\n\n### write(payload?: Table | RecordBatch | Iterable<Table> | Iterable<RecordBatch> | null): void\n\nWrites a `RecordBatch` or all the RecordBatches from a `Table`.\n\n\n## Remarks\n\n* Just like the `RecordBatchReader`, a `RecordBatchWriter` is a factory base class that returns an instance of the subclass appropriate to the situation: `RecordBatchStreamWriter`, `RecordBatchFileWriter`, `RecordBatchJSONWriter`\n","slug":"arrowjs/docs/api-reference/record-batch-writer","title":" RecordBatchWriter"},{"excerpt":"Predicates Value Literal Col The Col predicate gets the value of the specified column bind(batch : RecordBatch) : Function Returns a more…","rawMarkdownBody":"# Predicates\n\n\n\n\n## Value\n\n## Literal\n\n## Col\n\nThe Col predicate gets the value of the specified column\n\n### bind(batch : RecordBatch) : Function\n\nReturns a more efficient accessor for the column values in this batch, taking local indices.\n\nNote: These accessors are typically created in the `DataFrame.scan` bind method, and then used in the the `DataFrame.next` method.\n\n## ComparisonPredicate\n\n## And\n\n## Or\n\n## Equals\n\n## LTEq\n\n## GTEq\n\n## Not\n\n## CustomPredicate\n","slug":"arrowjs/docs/api-reference/predicates","title":"Predicates"},{"excerpt":"Row A  is an Object that retrieves each value at a certain index across a collection of child Vectors. Rows are returned from the  function…","rawMarkdownBody":"# Row\n\nA `Row` is an Object that retrieves each value at a certain index across a collection of child Vectors. Rows are returned from the `get()` function of the nested `StructVector` and `MapVector`, as well as `RecordBatch` and `Table`.\n\nA `Row` defines read-only accessors for the indices and (if applicable) names of the child Vectors. For example, given a `StructVector` with the following schema:\n\n```ts\nconst children = [\n    Int32Vector.from([0, 1]),\n    Utf8Vector.from(['foo', 'bar'])\n];\n\nconst type = new Struct<{ id: Int32, value: Utf8 }>([\n    new Field('id', children[0].type),\n    new Field('value', children[1].type)\n]);\n\nconst vector = new StructVector(Data.Struct(type, 0, 2, 0, null, children));\n\nconst row = vector.get(1);\n\nassert((row[0] ===   1  ) && (row.id    === row[0]));\nassert((row[1] === 'bar') && (row.value === row[1]));\n```\n\n`Row` implements the Iterator interface, enumerating each value in order of the child vectors list.\n\nNotes:\n\n- If the Row's parent type is a `Struct`, `Object.getOwnPropertyNames(row)` returns the child vector indices.\n- If the Row's parent type is a `Map`, `Object.getOwnPropertyNames(row)` returns the child vector field names, as defined by the `children` Fields list of the `Map` instance.\n\n## Methods\n\n### [key: string]: T[keyof T]['TValue']\n### [kParent]: MapVector<T> | StructVector<T>\n### [kRowIndex]: number\n### [kLength]: number (readonly)\n### [Symbol.iterator](): IterableIterator<T[keyof T][\"TValue\"]>\n### get(key: K): T[K][\"TValue\"]\n\nReturns the value at the supplied `key`, where `key` is either the integer index of the set of child vectors, or the name of a child Vector\n\n### toJSON(): any\n### toString(): any\n","slug":"arrowjs/docs/api-reference/row","title":"Row"},{"excerpt":"RecordBatch Overview A Record Batch in Apache Arrow is a collection of equal-length array instances. Usage A record batch can be created…","rawMarkdownBody":"# RecordBatch\n\n## Overview\n\nA Record Batch in Apache Arrow is a collection of equal-length array instances.\n\n## Usage\n\nA record batch can be created from this list of arrays using `RecordBatch.from`:\n```\nconst data = [\n  new Array([1, 2, 3, 4]),\n  new Array(['foo', 'bar', 'baz', None]),\n  new Array([True, None, False, True])\n]\n\nconst recordBatch = RecordBatch.from(arrays);\n```\n\n\n## Inheritance\n\n`RecordBatch` extends [`StructVector`](docs-arrow/api-reference/struct-vector) extends [`BaseVector`](docs-arrow/api-reference/vector)\n\n\n## Members\n\n### schema : Schema (readonly)\n\nReturns the schema of the data in the record batch\n\n### numCols : Number (readonly)\n\nReturns number of fields/columns in the schema (shorthand for `this.schema.fields.length`).\n\n\n## Static Methods\n\n### RecordBatch.from(vectors: Array, names: String[] = []) : RecordBatch\n\nCreates a `RecordBatch`, see `RecordBatch.new()`.\n\n\n### RecordBatch.new(vectors: Array, names: String[] = []) : RecordBatch\n\nCreates new a record batch.\n\nSchema is auto inferred, using names or index positions if `names` are not supplied.\n\n\n## Methods\n\n### constructor(schema: Schema, numRows: Number, childData: (Data | Vector)[])\n\nCreate a new `RecordBatch` instance with `numRows` rows of child data.\n\n* `numRows` - \n* `childData` - \n\n\n### constructor(schema: Schema, data: Data, children?: Vector[])\n\nCreate a new `RecordBatch` instance with `numRows` rows of child data.\n\n### constructor(...args: any[])\n\n### clone(data: Data, children?: Array) : RecordBatch\n\nReturns a newly allocated copy of this `RecordBatch`\n\n### concat(...others: Vector[]) : Table\n\nConcatenates a number of `Vector` instances.\n\n### select(...columnNames: K[]) : RecordBatch\n\nReturn a new `RecordBatch` with a subset of columns.\n","slug":"arrowjs/docs/api-reference/record-batch","title":"RecordBatch"},{"excerpt":"StructVector Methods asMap(keysSorted: boolean = false) TBA","rawMarkdownBody":"# StructVector\n\n\n## Methods\n\n### asMap(keysSorted: boolean = false)\n\nTBA\n","slug":"arrowjs/docs/api-reference/struct-vector","title":"StructVector"},{"excerpt":"Table Logical table as sequence of chunked arrays Overview The JavaScript  class is not part of the Apache Arrow specification as such, but…","rawMarkdownBody":"# Table\n\nLogical table as sequence of chunked arrays\n\n\n## Overview\n\nThe JavaScript `Table` class is not part of the Apache Arrow specification as such, but is rather a tool to allow you to work with multiple record batches and array pieces as a single logical dataset.\n\nAs a relevant example, we may receive multiple small record batches in a socket stream, then need to concatenate them into contiguous memory for use in NumPy or pandas. The Table object makes this efficient without requiring additional memory copying.\n\nA Table’s columns are instances of `Column`, which is a container for one or more arrays of the same type.\n\n\n## Usage\n\n`Table.new()` accepts an `Object` of `Columns` or `Vectors`, where the keys will be used as the field names for the `Schema`:\n\n```js\nconst i32s = Int32Vector.from([1, 2, 3]);\nconst f32s = Float32Vector.from([.1, .2, .3]);\nconst table = Table.new({ i32: i32s, f32: f32s });\nassert(table.schema.fields[0].name === 'i32');\n```\n\nIt also accepts a a list of Vectors with an optional list of names or\nFields for the resulting Schema. If the list is omitted or a name is\nmissing, the numeric index of each Vector will be used as the name:\n\n```ts\nconst i32s = Int32Vector.from([1, 2, 3]);\nconst f32s = Float32Vector.from([.1, .2, .3]);\nconst table = Table.new([i32s, f32s], ['i32']);\nassert(table.schema.fields[0].name === 'i32');\nassert(table.schema.fields[1].name === '1');\n```\n\nIf the supplied arguments are `Column` instances, `Table.new` will infer the `Schema` from the `Column`s:\n\n```ts\nconst i32s = Column.new('i32', Int32Vector.from([1, 2, 3]));\nconst f32s = Column.new('f32', Float32Vector.from([.1, .2, .3]));\nconst table = Table.new(i32s, f32s);\nassert(table.schema.fields[0].name === 'i32');\nassert(table.schema.fields[1].name === 'f32');\n```\n\nIf the supplied Vector or Column lengths are unequal, `Table.new` will\nextend the lengths of the shorter Columns, allocating additional bytes\nto represent the additional null slots. The memory required to allocate\nthese additional bitmaps can be computed as:\n\n```ts\nlet additionalBytes = 0;\nfor (let vec in shorter_vectors) {\n additionalBytes += (((longestLength - vec.length) + 63) & ~63) >> 3;\n}\n```\n\nFor example, an additional null bitmap for one million null values would require `125,000` bytes (`((1e6 + 63) & ~63) >> 3`), or approx. `0.11MiB`\n\n\n## Inheritance\n\n`Table` extends Chunked\n\n\n## Static Methods\n\n### Table.empty() : Table\n\nCreates an empty table\n\n### Table.from() : Table\n\nCreates an empty table\n\n### Table.from(source: RecordBatchReader): Table\n### Table.from(source: Promise<RecordBatchReader>): Promise<Table>\n### Table.from(source?: any) : Table\n### Table.fromAsync(source: import('./ipc/reader').FromArgs): Promise<Table>\n### Table.fromVectors(vectors: any[], names?: String[]) : Table\n### Table.fromStruct(struct: Vector) : Table\n\n\n### Table.new(columns: Object)\n### Table.new(...columns)\n### Table.new(vectors: Vector[], names: String[])\n\nType safe constructors. Functionally equivalent to calling `new Table()` with the same arguments, however if using Typescript using the `new` method instead will ensure that types inferred from the arguments \"flow through\" into the return Table type.\n\n\n## Members\n\n### schema (readonly)\n\nThe `Schema` of this table.\n\n\n### length : Number (readonly)\n\nThe number of rows in this table.\n\nTBD: this does not consider filters\n\n\n### chunks : RecordBatch[] \\(readonly)\n\nThe list of chunks in this table.\n\n\n### numCols : Number (readonly)\n\nThe number of columns in this table.\n\n\n## Methods\n\n### constructor(batches: RecordBatch[])\n\nThe schema will be inferred from the record batches.\n\n### constructor(...batches: RecordBatch[])\n\nThe schema will be inferred from the record batches.\n\n### constructor(schema: Schema, batches: RecordBatch[])\n\n### constructor(schema: Schema, ...batches: RecordBatch[])\n\n### constructor(...args: any[])\n\n\nCreate a new `Table` from a collection of `Columns` or `Vectors`, with an optional list of names or `Fields`.\n\nTBD\n\n### clone(chunks?:)\n\nReturns a new copy of this table.\n\n### getColumnAt(index: number): Column | null\n\nGets a column by index.\n\n### getColumn(name: String): Column | null\n\nGets a column by name\n\n### getColumnIndex(name: String) : Number | null\n\nReturns the index of the column with name `name`.\n\n### getChildAt(index: number): Column | null\n\nTBD\n\n### serialize(encoding = 'binary', stream = true) : Uint8Array\n\nReturns a `Uint8Array` that contains an encoding of all the data in the table.\n\nNote: Passing the returned data back into `Table.from()` creates a \"deep clone\" of the table.\n\n\n### count(): number\n\nTBD - Returns the number of elements.\n\n### select(...columnNames: string[]) : Table\n\nReturns a new Table with the specified subset of columns, in the specified order.\n\n### countBy(name : Col | String) : Table\n\nReturns a new Table that contains two columns (`values` and `counts`).\n","slug":"arrowjs/docs/api-reference/table","title":"Table"},{"excerpt":"Schema Sequence of arrow::Field objects describing the columns of a record batch or table data structure Accessors fields : Field…","rawMarkdownBody":"# Schema\n\nSequence of arrow::Field objects describing the columns of a record batch or table data structure\n\n\n## Accessors\n\n### fields : Field[] \\(readonly)\n\nReturn the list of fields (columns) in the schema.\n\n### metadata (readonly)\n\nThe custom key-value metadata, if any. metadata may be null.\n\n### dictionaries (readonly)\n\nTBD - List of dictionaries (each dictionary is associated with a column that is dictionary encoded).\n\n### dictionaryFields (readonly)\n\nTBD - List of fields\n\n\n## Methods\n\n### constructor(fields: Field[], metadata?: Object, dictionaries?: Object, dictionaryFields?: Object)\n\nCreates a new schema instance.\n\n\n### select(columnNames) : Schema\n\nReturns a new `Schema` with the Fields indicated by the column names.\n\n\n","slug":"arrowjs/docs/api-reference/schema","title":"Schema"},{"excerpt":"Types Objects representing types.","rawMarkdownBody":"# Types\n\nObjects representing types.\n\n","slug":"arrowjs/docs/api-reference/types","title":"Types"},{"excerpt":"Chunked Holds a \"chunked array\" that allows a number of array fragments (represented by  instances) to be treated logically as a single…","rawMarkdownBody":"# Chunked\n\nHolds a \"chunked array\" that allows a number of array fragments (represented by `Vector` instances) to be treated logically as a single vector. `Vector` instances can be concatenated into a `Chunked` without any memory being copied.\n\n\n## Usage\n\nCreate a new contiguous typed array from a `Chunked` instance (note that this creates a new typed array unless only one chunk)\n\n```js\nconst typedArray = chunked.toArray();\n```\n\nA `Chunked` array supports iteration, random element access and mutation.\n\n\n\n## Inheritance\n\nclass Chunked extends [Vector](docs-arrow/api-reference/vector.md)\n\n\n## Static Methods\n\n### Chunked.flatten(...vectors: Vector[]) : Vector\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nUtility method that flattens a number of `Vector` instances or Arrays of `Vector` instances into a single Array of `Vector` instances. If the incoming Vectors are instances of `Chunked`, the child chunks are extracted and flattened into the resulting Array. Does not mutate or copy data from the Vector instances.\n\nReturns an Array of `Vector` instances.\n\n### Chunked.concat(...chunks: Vector<T>[]): Chunked\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nConcatenates a number of `Vector` instances of the same type into a single `Chunked` Vector. Returns a new `Chunked` Vector.\n\nNote: This method extracts the inner chunks of any incoming `Chunked` instances, and flattens them into the `chunks` array of the returned `Chunked` Vector.\n\n## Members\n\n### [Symbol.iterator]() : Iterator\n\n`Chunked` arrays are iterable, allowing you to use constructs like `for (const element of chunked)` to iterate over elements. For in-order traversal, this is more performant than random-element access.\n\n### type : T\n\nReturns the DataType instance which determines the type of elements this `Chunked` instance contains. All vector chunks will have this type.\n\n### length: Number  (read-only)\n\nReturns the total number of elements in this `Chunked` instance, representing the length of of all chunks.\n\n### chunks: Vector[]  (read-only)\n\nReturns an array of the `Vector` chunks that hold the elements in this `Chunked` array.\n\n### typeId : TBD  (read-only)\n\nThe `typeId` enum value of the `type` instance\n\n### data : Data  (read-only)\n\nReturns the `Data` instance of the _first_ chunk in the list of inner Vectors.\n\n### ArrayType  (read-only)\n\nReturns the constructor of the underlying typed array for the values buffer as determined by this Vector's DataType.\n\n### numChildren  (read-only)\n\nThe number of logical Vector children for the Chunked Vector. Only applicable if the DataType of the Vector is one of the nested types (List, FixedSizeList, Struct, or Map).\n\n### stride  (read-only)\n\nThe number of elements in the underlying data buffer that constitute a single logical value for the given type. The stride for all DataTypes is 1 unless noted here:\n\n- For `Decimal` types, the stride is 4.\n- For `Date` types, the stride is 1 if the `unit` is DateUnit.DAY, else 2.\n- For `Int`, `Interval`, or `Time` types, the stride is 1 if `bitWidth <= 32`, else 2.\n- For `FixedSizeList` types, the stride is the `listSize` property of the `FixedSizeList` instance.\n- For `FixedSizeBinary` types, the stride is the `byteWidth` property of the `FixedSizeBinary` instance.\n\n### nullCount  (read-only)\n\nNumber of null values across all Vector chunks in this chunked array.\n\n### indices : ChunkedKeys<T> | null  (read-only)\n\nIf this is a dictionary encoded column, returns a `Chunked` instance of the indicies of all the inner chunks. Otherwise, returns `null`.\n\n### dictionary: ChunkedDict | null  (read-only)\n\nIf this is a dictionary encoded column, returns the Dictionary.\n\n\n## Methods\n\n### constructor(type : \\*, chunks? : Vector[] = [], offsets? : Number[])\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nCreates a new `Chunked` array instance of the given `type` and optionally initializes it with a list of `Vector` instances.\n\n* `type` - The DataType of the inner chunks\n* `chunks`= - Vectors must all be compatible with `type`.\n* `offsets`= - A Uint32Array of offsets where each inner chunk starts and ends. If not provided, offsets are automatically calculated from the list of chunks.\n\nTBD - Confirm/provide some information on how `offsets` can be used?\n\n\n### clone(chunks? : this.chunks): Chunked\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nReturns a new `Chunked` instance that is a clone of this instance. Does not copy the actual chunks, so the new `Chunked` instance will reference the same chunks.\n\n\n### concat(...others: Vector<T>[]): Chunked\n\n<p class=\"badges\">\n   <img src=\"https://img.shields.io/badge/zero-copy-green.svg?style=flat-square\" alt=\"zero-copy\" />\n</p>\n\nConcatenates a number of `Vector` instances after the chunks. Returns a new `Chunked` array.\n\nThe supplied `Vector` chunks must be the same DataType as the `Chunked` instance.\n\n### slice(begin?: Number, end?: Number): Chunked\n\nReturns a new chunked array representing the logical array containing the elements within the index range, potentially dropping some chunks at beginning and end.\n\n* `begin`=`0` - The first logical index to be included as index 0 in the new array.\n* `end` - The first logical index to be included as index 0 in the new array. Defaults to the last element in the range.\n\nReturns a zero-copy slice of this Vector. The begin and end arguments are handled the same way as JS' `Array.prototype.slice`; they are clamped between 0 and `vector.length` and wrap around when negative, e.g. `slice(-1, 5)` or `slice(5, -1)`\n\n\n### getChildAt(index : Number): Chunked | null\n\nIf this `Chunked` Vector's DataType is one of the nested types (Map or Struct), returns a `Chunked` Vector view over all the chunks for the child Vector at `index`.\n\n### search(index: Number): [number, number] | null;\n### search(index: Number, then?: SearchContinuation): ReturnType<N>;\n### search(index: Number, then?: SearchContinuation)\n\nUsing an `index` that is relative to the whole `Chunked` Vector, binary search through the list of inner chunks using supplied \"global\" `index` to find the chunk at that location. Returns the child index of the inner chunk and an element index that has been adjusted to the keyspace of the found inner chunk.\n\n`search()` can be called with only an integer index, in which case a pair of `[chunkIndex, valueIndex]` are returned as a two-element Array:\n\n```ts\nlet chunked = [\n    Int32Vector.from([0, 1, 2, 3]),\n    Int32Vector.from([4, 5, 6, 7, 8])\n].reduce((x, y) => x.concat(y));\n\nlet [chunkIndex, valueIndex] = chunked.search(6)\nassert(chunkIndex === 1)\nassert(valueIndex === 3)\n```\n\nIf `search()` is called with an integer index and a callback, the callback will be invoked with the `Chunked` instance as the first argument, then the `chunkIndex` and `valueIndex` as the second and third arguments:\n\n```ts\nlet getChildValue = (parent, childIndex, valueIndex) =>\n    chunked.chunks[childIndex].get(valueIndex);\nlet childValue = chunked.search(6, (chunked, childIndex, valueIndex) => )\n```\n\n\n### isValid(index: Number): boolean\n\nChecks if the element at `index` in the logical array is valid.\n\nChecks the null map (if present) to determine if the value in the logical `index` is included.\n\n### get(index : Number): T['TValue'] | null\n\nReturns the element at `index` in the logical array, or `null` if no such element exists (e.e.g if `index` is out of range).\n\n### set(index: Number, value: T['TValue'] | null): void\n\nWrites the given `value` at the provided `index`. If the value is null, the null bitmap is updated.\n\n### indexOf(element: Type, offset?: Number): Number\n\nReturns the index of the first occurrence of `element`, or `-1` if the value was not found.\n\n* `offset` - the index to start searching from.\n\n### toArray(): TypedArray\n\nReturns a single contiguous typed array containing data in all the chunks (effectively \"flattening\" the chunks.\n\nNotes:\n* Calling this function creates a new typed array unless there is only one chunk.\n\n\n","slug":"arrowjs/docs/api-reference/chunked","title":"Chunked"},{"excerpt":"Vector Also referred to as . An abstract base class for vector types. Can support a null map ... TBD Inheritance Fields data: Data (readonly…","rawMarkdownBody":"# Vector\n\nAlso referred to as `BaseVector`. An abstract base class for vector types.\n\n* Can support a null map\n* ...\n* TBD\n\n\n## Inheritance\n\n\n## Fields\n\n### data: Data<T> (readonly)\n\nThe underlying Data instance for this Vector.\n\n### numChildren: number (readonly)\n\nThe number of logical Vector children. Only applicable if the DataType of the Vector is one of the nested types (List, FixedSizeList, Struct, or Map).\n\n### type : T\n\nThe DataType that describes the elements in the Vector\n\n### typeId : T['typeId']\n\nThe `typeId` enum value of the `type` instance\n\n### length : number\n\nNumber of elements in the `Vector`\n\n### offset : number\n\nOffset to the first element in the underlying data.\n\n### stride : number\n\nStride between successive elements in the the underlying data.\n\nThe number of elements in the underlying data buffer that constitute a single logical value for the given type. The stride for all DataTypes is 1 unless noted here:\n\n- For `Decimal` types, the stride is 4.\n- For `Date` types, the stride is 1 if the `unit` is DateUnit.DAY, else 2.\n- For `Int`, `Interval`, or `Time` types, the stride is 1 if `bitWidth <= 32`, else 2.\n- For `FixedSizeList` types, the stride is the `listSize` property of the `FixedSizeList` instance.\n- For `FixedSizeBinary` types, the stride is the `byteWidth` property of the `FixedSizeBinary` instance.\n\n### nullCount : Number\n\nNumber of `null` values in this `Vector` instance (`null` values require a null map to be present).\n\n### VectorName : String\n\nReturns the name of the Vector\n\n### ArrayType : TypedArrayConstructor | ArrayConstructor\n\nReturns the constructor of the underlying typed array for the values buffer as determined by this Vector's DataType.\n\n### values : T['TArray']\n\nReturns the underlying data buffer of the Vector, if applicable.\n\n### typeIds : Int8Array | null\n\nReturns the underlying typeIds buffer, if the Vector DataType is Union.\n\n### nullBitmap : Uint8Array | null\n\nReturns the underlying validity bitmap buffer, if applicable.\n\nNote: Since the validity bitmap is a Uint8Array of bits, it is _not_ sliced when you call `vector.slice()`. Instead, the `vector.offset` property is updated on the returned Vector. Therefore, you must factor `vector.offset` into the bit position if you wish to slice or read the null positions manually. See the implementation of `BaseVector.isValid()` for an example of how this is done.\n\n### valueOffsets : Int32Array | null\n\nReturns the underlying valueOffsets buffer, if applicable. Only the List, Utf8, Binary, and DenseUnion DataTypes will have valueOffsets.\n\n## Methods\n\n### clone(data: Data<R>, children): Vector<R>\n\nReturns a clone of the current Vector, using the supplied Data and optional children for the new clone. Does not copy any underlying buffers.\n\n### concat(...others: Vector<T>[])\n\nReturns a `Chunked` vector that concatenates this Vector with the supplied other Vectors. Other Vectors must be the same type as this Vector.\n\n\n### slice(begin?: number, end?: number)\n\nReturns a zero-copy slice of this Vector. The begin and end arguments are handled the same way as JS' `Array.prototype.slice`; they are clamped between 0 and `vector.length` and wrap around when negative, e.g. `slice(-1, 5)` or `slice(5, -1)`\n\n### isValid(index: number): boolean\n\nReturns whether the supplied index is valid in the underlying validity bitmap.\n\n### getChildAt<R extends DataType = any>(index: number): Vector<R> | null\n\nReturns the inner Vector child if the DataType is one of the nested types (Map or Struct).\n\n### toJSON(): any\n\nReturns a dense JS Array of the Vector values, with null sentinels in-place.\n","slug":"arrowjs/docs/api-reference/vector","title":"Vector"},{"excerpt":"Types and Vectors Overview Usage Constructing new  instances is done through the static  methods Special Vectors Dictionary Arrays The…","rawMarkdownBody":"# Types and Vectors\n\n## Overview\n\n\n## Usage\n\nConstructing new `Vector` instances is done through the static `from()` methods\n\n\n## Special Vectors\n\n### Dictionary Arrays\n\nThe Dictionary type is a special array type that enables one or more record batches in a file or stream to transmit integer indices referencing a shared dictionary containing the distinct values in the logical array. Later record batches reuse indices in earlier batches and add new ones as needed.\n\nA `Dictionary` is similar to a `factor` in R or a pandas, or \"Categorical\" in Python. It is is often used with strings to save memory and improve performance.\n\n\n### StructVector\n\nHolds nested fields.\n\n\n### Bool Vectors\n\n| Bool Vectors            |\n| ---                     |\n| `BoolVector`            |\n\n\n### Binary Vectors\n\n| Binary Vectors          |\n| ---                     |\n| `BinaryVector`          |\n\n\n## FloatVectors\n\n| Float Vectors           | Backing         | Comments                  |\n| ---                     |\n| `Float16Vector`         | `Uint16Array`   | No native JS 16 bit type, additional methods available |\n| `Float32Vector`         | `Float32Array`  | Holds 32 bit floats       |\n| `Float64Vector`         | `Float64Array`  | Holds 64 bit floats       |\n\n\n### Static FloatVector Methods\n\n### FloatVector.from(data: Uint16Array): Float16Vector;\n### FloatVector.from(data: Float32Array): Float32Vector;\n### FloatVector.from(data: Float64Array): Float64Vector;\n### FloatVector16.from(data: Uint8Array | Iterable<Number>): Float16Vector;\n### FloatVector16.from(data: Uint16Array | Iterable<Number>): Float16Vector;\n### FloatVector32.from(data: Float32['TArray'] | Iterable<Number>): Float32Vector;\n### FloatVector64.from(data: Float64['TArray'] | Iterable<Number>): Float64Vector;\n\n\n## Float16Vector Methods\n\nSince JS doesn't have half floats, `Float16Vector` is backed by a `Uint16Array` integer array. To make it practical to work with these arrays in JS, some extra methods are added.\n\n### toArray() : `Uint16Array`\n\nReturns a zero-copy view of the underlying `Uint16Array` data.\n\nNote: Avoids incurring extra compute or copies if you're calling `toArray()` in order to create a buffer for something like WebGL, but makes it hard to use the returned data as floating point values in JS.\n\n### toFloat32Array() : Float32Array\n\nThis method will convert values to 32 bit floats. Allocates a new Array.\n\n### toFloat64Array() : Float64Array\n\nThis method will convert values to 64 bit floats. Allocates a new Array.\n\n\n## IntVectors\n\n| Int Vectors             | Backing         | Comments                  |\n| ---                     | ---             | ---                       |\n| `Int8Vector`            | `Int8Array`     |                           |\n| `Int16Vector`           | `Int16Array`    |                           |\n| `Int32Vector`           | `Int32Array`    |                           |\n| `Int64Vector`           | `Int32Array`    | 64-bit values stored as pairs of `lo, hi` 32-bit values for engines without BigInt support, extra methods available |\n| `Uint8Vector`           | `Uint8Array`    |                           |\n| `Uint16Vector`          | `Uint16Array `  |                           |\n| `Uint32Vector`          | `Uint32Array `  |                           |\n| `Uint64Vector`          | `Uint32Array`   | 64-bit values stored as pairs of `lo, hi` 32-bit values for engines without BigInt support, extra methods available |\n\n## Int64Vector Methods\n\n### toArray() : `Int32Array`\n\nReturns a zero-copy view of the underlying pairs of `lo, hi` 32-bit values as an `Int32Array`. This Array's length is twice the logical length of the `Int64Vector`.\n\n### toBigInt64Array(): `BigInt64Array`\n\nReturns a zero-copy view of the underlying 64-bit integers as a `BigInt64Array`. This Array has the samne length as the length of the original `Int64Vector`.\n\nNote: as of 03/2019, `BigInt64Array` is only available in v8/Chrome. In JS runtimes without support for `BigInt`, this method throws an unsupported error.\n\n## Uint64Vector Methods\n\n### toArray() : `Uint32Array`\n\nReturns a zero-copy view of the underlying pairs of `lo, hi` 32-bit values as a `Uint32Array`. This Array's length is twice the logical length of the `Uint64Vector`.\n\n### toBigUint64Array(): `BigUint64Array`\n\nReturns a zero-copy view of the underlying 64-bit integers as a `BigUint64Array`. This Array has the samne length as the length of the original `Uint64Vector`.\n\nNote: as of 03/2019, `BigUint64Array` is only available in v8/Chrome. In JS runtimes without support for `BigInt`, this method throws an unsupported error.\n\n## Static IntVector Methods\n\n### IntVector.from(data: Int8Array): Int8Vector;\n### IntVector.from(data: Int16Array): Int16Vector;\n### IntVector.from(data: Int32Array, is64?: boolean): Int32Vector | Int64Vector;\n### IntVector.from(data: Uint8Array): Uint8Vector;\n### IntVector.from(data: Uint16Array): Uint16Vector;\n### IntVector.from(data: Uint32Array, is64?: boolean): Uint32Vector | Uint64Vector;\n\n### Int8Vector.from(this: typeof Int8Vector,   data: Int8Array   | Iterable<number>): Int8Vector;\n### Int16Vector.from(this: typeof Int16Vector,  data: Int16Array  | Iterable<number>): Int16Vector;\n### Int32Vector.from(this: typeof Int32Vector,  data: Int32Array  | Iterable<number>): Int32Vector;\n### Int64Vector.from(this: typeof Int64Vector,  data: Int32Array  | Iterable<number>): Int64Vector;\n### Uint8Vector.from(this: typeof Uint8Vector,  data: Uint8Array  | Iterable<number>): Uint8Vector;\n### Uint16Vector.from(this: typeof Uint16Vector, data: Uint16Array | Iterable<number>): Uint16Vector;\n### Uint32Vector.from(this: typeof Uint32Vector, data: Uint32Array | Iterable<number>): Uint32Vector;\n### Uint64Vector.from(this: typeof Uint64Vector, data: Uint32Array | Iterable<number>): Uint64Vector;\n\n\n## Date Vectors\n\n| Date Vectors            | Backing       |                     |\n| ---                     | ---           | ---                 |\n| `DateDayVector`         | `Int32Array`  |                     |\n| `DateMillisecondVector` | `Int32Array`  | TBD - stride: 2?    |\n","slug":"arrowjs/docs/api-reference/vectors","title":"Types and Vectors"}]}}}
{"componentChunkName":"component---node-modules-gatsby-theme-ocular-src-react-templates-search-jsx","path":"/search","result":{"pageContext":{"data":[{"excerpt":"Upgrade Guide Upgrading to v3.0 Transpilation The module entry point is now only lightly transpiled for the most commonly used evergreen browsers. This change offers significant savings on bundle size. If your application needs to support older browsers such as IE 11, make sure to include  node_modules  in your babel config. Worker Concurrency Default number of worker threads for each loader has been reduced from  5  to  3  on non mobile devices and to  1  on mobile devices to reduce memory use. Generally, increasing the number of workers has diminishing returns. @loaders.gl/gltf GLTFScenegraph  is updated to provide modifying capabilities. Signatures of some methods have been changed to use named parameters (rather than positional parameters). The deprecated  GLBBuilder  class and  encodeGLTFSync  functions have now been removed. @loaders.gl/basis Module has been moved to  @loaders.gl/textures . @loaders.gl/images The texture API  loadImage ,  loadImageArray ,  loadImageCube  has been moved to the new  @loaders.gl/textures  module, and have been renamed to  loadImageTexture* . @loaders.gl/kml The  KMLLoader ,  GPXLoader , and  TCXLoader  now require a value for  options.gis.format . Previously, the lack of a value would return data in \"raw\" format, i.e. not normalized to GeoJSON. To return GeoJSON-formatted data, use  options.gis.format: 'geojson' . Other options are  binary  and  raw . The  kml.normalize  option has been deprecated. When  options.gis.format  is  geojson , coordinates will always be in longitude-latitude ordering. @loaders.gl/compression Sync transforms no longer supported (this enables dynamic library loading). Transform static members now named  run()  instead of  inflate()  and  deflate() . Zstandard transforms removed due to excessive bundle size impact. Use the new  ZstdWorker  object instead. @loaders.gl/crypto Sync hashing no longer supported (this enables dynamic library loading). Transform static members now named  run()  instead of  hash() . @loaders.gl/loader-utils createWorker()  now creates a generic worker. For loader workers use the new  createLoaderWorker()  function. Upgrading to v2.3 @loaders.gl/core : selectLoader()  is now async and returns a  Promise  that resolves to a loader. selectLoaderSync()  is available for situations when calling an async function is inconvenient. Passing  fetch  options to  load()  and  parse()  etc. should now be done via the  options.fetch  sub-options object. fetch options on the root object are now deprecated. @loaders.gl/kml : The  KMLAsGeoJsonLoader  has been removed, use  KMLLoader , with  options.gis.format: 'geojson' . Upgrading to v2.2 @loaders.gl/core selectLoader  is no longer experimental. If you were using the experimental export, replace  _selectLoader  with  selectLoader . Also note that argument order has changed and now aligns with  load  and  parse parseInBatchesSync  has been removed, all batched parsing is now performed asynchronously. Some iterator utilities that are mostly used internally have been changed. Function Replacement / Status makeChunkIterator combined into  makeIterator makeStreamIterator combined into  makeIterator textDecoderAsyncIterator makeTextDecoderIterator lineAsyncIterator makeLineIterator numberedLineAsyncIterator makeNumberedLineIterator getStreamIterator Deprecated in 2.1, now removed in 2.2 contatenateAsyncIterator Deprecated in 2.1, now removed in 2.2 @loaders.gl/csv Header auto-detection now requires  options.csv.header  to be set to  'auto'  instead of  undefined .  'auto'  is the new default value for this option, so this change is unlikely to affect applications. @loaders.gl/json The experimental  json._rootObjectBatches  option is now deprecated. Use the top-level  metadata: true  option instead. Note that the  batchType  names have also changed, see the JSONLoader docs for details. @loaders.gl/ply The experimental streaming  _PLYStreamingLoader  has been removed. Use the non-streaming  PLYLoader  instead. @loaders.gl/images The new function  getBinaryImageMetadata()  replaces  isBinaryImage() ,  getBinaryImageSize()  and  getBinaryImageMIMEType() . The old functions are now deprecated, but still available. Upgrading to v2.1 @loaders.gl/core Some iterator helper functions have been renamed, the old naming is now deprecated. Old Name New Name getStreamIterator makeStreamIterator contatenateAsyncIterator concatenateChunksAsync @loaders.gl/json Experimental exports have been removed  JSONParser ,  StreamingJSONParser ,  ClarinetParser . @loaders.gl/images The experimental ImageLoaders for individual formats introduced in 2.0 have been removed, use  ImageLoader  for all formats.\n @loaders.gl/images getImageData(image)  now returns an object with  {data, width, height}  instead of just the  data  array. This small breaking change ensures that the concept of  image data  is consistent across the API. ImageLoader :  options.image.type : The  html  and  ndarray  image types are now deprecated and replaced with  image  and  data  respectively. @loaders.gl/3d-tiles Tileset3DLoader  and  Tile3DLoader  are replaced by  Tiles3DLoader , which supports loading both a 3D tileset file and a tile. Check  loaders.gl/3d-tiles  for loaded data format. Upgrading to v2.0 Version 2.0 is a major release that consolidates functionality and APIs, and a number of deprecated functions have been removed. Some general changes: All exported loader and writer objects now expose a  mimeType  field. This field is not yet used by  @loaders.gl/core  but is available for applications (e.g. see  selectLoader ). All (non-worker) loaders are now required to expose a  parse  function (in addition to any more specialized  parseSync/parseText/parseInBatches  functions). This simplifies using loaders without  @loaders.gl/core , which can reduce footprint in small applications. @loaders.gl/core Removal Replacement TextEncoder Use global  TextEncoder  instead and  @loaders.gl/polyfills  if needed TextDecoder Use global  TextDecoder  instead and  @loaders.gl/polyfills  if needed createReadStream fetch().then(resp => resp.body) parseFile parse parseFileSync parseSync loadFile load @loaders.gl/images Removal Replacement ImageHTMLLoader ImageLoader  with  options.images.format: 'image' ImageBitmapLoader ImageLoader  with  options.images.format: 'imagebitmap' decodeImage parse(arrayBuffer, ImageLoader) isImage isBinaryImage getImageMIMEType getBinaryImageMIMEType getImageSize getBinaryImageSize getImageMetadata getBinaryImageMIMEType  +  getBinaryImageSize Loader Objects Loaders can no longer have a  loadAndParse  method. Remove it, and just make sure you define  parse  on your loaders instead. @loaders.gl/gltf The  GLTFLoader  now always uses the new v2 parser, and the original  GLTFParser  has been removed. Removal Replacement GLBParser GLBLoader GLBBuilder GLBWriter GLTFParser GLTFLoader GLTFBuilder GLTFWriter packBinaryJson N/A unpackBinaryJson N/A Note that automatic packing of binary data (aka \"packed JSON\" support) was only implemented in the v1  GLTFLoader  and has thus also been removed. Experience showed that packing of binary data for  .glb  files is best handled by applications. GLTFLoader option changes The foillowing top-level options are deprecated and will be removed in v2.0 Removed Option Replacement Descriptions gltf.parserVersion N/A No longer needs to be specied, only the new gltf parser is available. fetchLinkedResources gltf.fetchBuffers ,  gltf.fetchImages fetchImages gltf.fetchImages createImages N/A Images are now always created when fetched decompress gltf.decompressMeshes Decompress Draco compressed meshes (if DracoLoader available). DracoLoader N/A Supply  DracoLoader  to  parse , or call  registerLoaders(pDracoLoader]) postProcess gltf.postProcess Perform additional post processing before returning data. uri baseUri Auto-populated when loading from a url-equipped source fetch N/A fetch is automatically available to sub-loaders. @loaders.gl/draco Removal Replacement DracoParser DracoLoader DracoBuilder DracoWriter Loader Objects Loaders no longer have a  loadAndParse  removed. Just define  parse  on your loaders. Upgrading from v1.2 to v1.3 As with v1.1,  GLTFLoader  will no longer return a  GLTFParser  object in v2.0. A new option  options.gltf.parserVersion: 2  is provided to opt in to the new behavior now. Upgrading from v1.0 to v1.1 A couple of functions have been deprecated and will be removed in v2.0. They now emit console warnings. Start replacing your use of these functions now to remove the console warnings and ensure a smooth future upgrade to v2.0. Also, Node support now requires installing  @loaders.gl/polyfills  before use. @loaders.gl/core Removal: Node support for  fetchFile  now requires importing  @loaders.gl/polyfills  before use. Removal: Node support for  TextEncoder , and  TextDecoder  now requires importing  @loaders.gl/polyfills  before use. Deprecation:  TextEncoder  and  TextDecoder  will not be exported from  loaders.gl/core  in v2.0. @loaders.gl/images Removal: Node support for images now requires importing  @loaders.gl/polyfills  before use. @loaders.gl/gltf Deprecation:  GLBParser / GLBBuilder  - These will be merged into GLTF classes.. Deprecation:  GLTFParser / GLTFBuilder  - The new  GLTF  class can hold GLTF data and lets application access/modify it. Deprecation:  GLTFLoader  will no longer return a  GLTFParser  object in v2.0. Instead it will return a pure javascript object containing the parse json and any binary chunks. This object can be accessed through the  GLTF  class. Set  options.GLTFParser  to  false  to opt in to the new behavior now. v1.0 First official release of loaders.gl.","headings":[{"value":"Upgrade Guide","depth":1},{"value":"Upgrading to v3.0","depth":2},{"value":"Upgrading to v2.3","depth":2},{"value":"Upgrading to v2.2","depth":2},{"value":"Upgrading to v2.1","depth":2},{"value":"Upgrading to v2.0","depth":2},{"value":"@loaders.gl/core","depth":3},{"value":"@loaders.gl/images","depth":3},{"value":"Loader Objects","depth":3},{"value":"@loaders.gl/gltf","depth":3},{"value":"@loaders.gl/draco","depth":3},{"value":"Loader Objects","depth":3},{"value":"Upgrading from v1.2 to v1.3","depth":2},{"value":"Upgrading from v1.0 to v1.1","depth":2},{"value":"@loaders.gl/core","depth":3},{"value":"@loaders.gl/images","depth":3},{"value":"@loaders.gl/gltf","depth":3},{"value":"v1.0","depth":2}],"slug":"docs/upgrade-guide","title":"Upgrade Guide"},{"excerpt":"Roadmap For information planned work, see: RFCs  - technical writeups that describe proposed features. Github Tracker Issues  - \"Tracker\" issues contained detailed technical roadmaps for specific features/modules.","headings":[{"value":"Roadmap","depth":1}],"slug":"docs/roadmap","title":"Roadmap"},{"excerpt":"Introduction A Linux Foundation Project loaders.gl is part of the  vis.gl  framework ecosystem  which is under developed under open governance through the Linux Foundation and the Urban Computing Foundation. Overview loaders.gl is a is a collection of the best open source loaders and writers for file formats focused on visualization of big data, including point clouds, 3D geometries, images, geospatial formats as well as tabular data. loader.gl is packaged and published as a composable module suite with consistent APIs and features across the suite, and supports advanced features such as running loaders on workers and incremental loading (streaming) in a consistent way across the entire suite, and all loaders work in both the browser and in Node.js. loaders.gl is framework-agnostic, and all loaders and writers are designed to be usable with any JavaScript application or framework with a minimal amount of glue code. Naturally, other  vis.gl frameworks  such as  deck.gl  and  luma.gl  are designed to integrate seamlessly with loaders.gl. Loaders loaders.gl provides a wide selection of loaders organized into categories: Category Loaders Table Loaders Streaming tabular loaders for  CSV ,  JSON ,  Arrow  etc Image Loaders Loaders for  images ,  compressed textures ,  supercompressed textures (Basis) . Utilities for  mipmapped arrays ,  cubemaps ,  binary images  and more. Pointcloud and Mesh Loaders Loaders for point cloud and simple mesh formats such as  Draco ,  LAS ,  PCD ,  PLY ,  OBJ , and  Terrain . Scenegraph Loaders glTF  loader 3D Tile Loaders Loaders for 3D tile formats such as  3D Tiles ,  I3S  and potree Geospatial Loaders Loaders for geospatial formats such as  GeoJSON   KML ,  WKT/WKB ,  Mapbox Vector Tiles  etc. Code Examples loaders.gl provides a small core API module with common functions to load and save data, and a range of optional modules that provide loaders and writers for specific file formats. A minimal example using the  load  function and the  CSVLoader  to load a CSV formatted table into a JavaScript array: Streaming parsing is available using ES2018 async iterators, e.g. allowing \"larger than memory\" files to be incrementally processed: To quickly get up to speed on how the loaders.gl API works, please see  Get Started . Supported Platforms loaders.gl provides consistent support for both browsers and Node.js. The following platforms are supported: Evergreen Browsers  loaders.gl supports recent versions of the major evergreen browsers (e.g. Chrome, Firefox, Safari) on both desktop and mobile. Node.js  LTS (Long-Term Support)  releases  are also supported through the  @loaders.gl/polyfills  module. Edge and IE11  are not actively supported, however loaders.gl 2.3 is known to run on Edge and IE11. Both  @loaders.gl/polyfills  and additional appropriate polyfills (e.g. babel polyfills) need to be installed which will increase your application bundle size. For loaders.gl 3.0, additional transpilation of your  node_modules  folder may also be required. Note that because of lack of regualar testing on these older platforms, regressions can occur. Pinning your loaders.gl version is advisable. Main Design Goals Framework Agnostic  - Files are parsed into clearly documented data structures (objects + typed arrays) that can be used with any JavaScript framework. Worker Support  - Many loaders run in web workers, keeping the main thread free for other tasks while parsing completes. Streaming Support  - Several loaders can parse in batches from both node and browser  Stream s, allowing \"larger than memory\" files to be processed, and initial results to be available while the remainder of a file is still loading. Node Support  - All loaders work under Node.js and can be used when writing backend and cloud services, and when running your unit tests under Node. Loader Categories  - loaders.gl groups similar data formats into \"categories\". loaders in the same category return parsed data in \"standardized\" form, simplifying applications that want to handle multiple related formats. Format Autodection  - Applications can specify multiple loaders when parsing a file, and loaders.gl will automatically pick the right loader for a given file based on file extension, MIME type and file header. Bundle Size Optimized  - Each format is published as an independent npm module to allow applications to cherry-pick only the loaders it needs, modules are optimized for tree-shaking, large loader libraries and workers are loaded from CDN and not bundled. Modern JavaScript  - loaders.gl is written in standard ES2018 and the API emphasizes modern, portable JavaScript constructs, e.g. async iterators instead of streams,  ArrayBuffer  instead of  Buffer , etc. Binary Data Optimized  - loaders.gl is optimized for use with WebGL frameworks (e.g. by returning typed arrays whenever possible). However, there are no any actual WebGL dependencies and loaders can be used without restrictions in non-WebGL applications. Multi-Asset Loading  - Some formats like glTF, or mipmapped cube textures, can required dozens of separate loads to resolve all linked assets (external buffers, images etc). Tracking all the resulting async loads can cause complications for applications. By default, loaders.gl loads all linked assets before resolving a returned  Promise . Licenses loaders.gl contains code under several permissive open source licenses, currently MIT, BSD and Apache licenses. Additional licenses might be included in the future, however loaders.gl will never include code with non-permissive, commercial or copy-left licenses. Note that each loader module comes with its own license, so if the distinction matters to you, please check and decide accordingly. Credits and Attributions loaders.gl is maintained by a group of organizations collaborating through open governance under the Linux Foundation. loaders.gl is partly a repackaging of superb work done by many others in the open source community. We try to be as explicit as we can about the origins and attributions of each loader, both in the documentation page for each loader and in the preservation of comments relating to authorship and contributions inside forked source code. Even so, we can make mistakes, and we may not have the full history of the code we are reusing. If you think that we have missed something, or that we could do better in regard to attribution, please let us know. Primary maintainers","headings":[{"value":"Introduction","depth":1},{"value":"A Linux Foundation Project","depth":2},{"value":"Overview","depth":2},{"value":"Loaders","depth":2},{"value":"Code Examples","depth":2},{"value":"Supported Platforms","depth":2},{"value":"Main Design Goals","depth":2},{"value":"Licenses","depth":2},{"value":"Credits and Attributions","depth":2},{"value":"Primary maintainers","depth":3}],"slug":"docs","title":"Introduction"},{"excerpt":"3D Tiles Loaders The 3D tiles category is experimental. The 3D Tiles category defines a generalized representation of hierarchical geospatial data structures. 3D Tiles Category Loaders Loader Notes Tiles3DLoader CesiumIonLoader I3SLoader PotreeLoader Overview The 3D Tiles category is can represent the OGC 3D Tiles  standard OGC i3s  standard potree  format as well. Concepts Tile Header Hierarchy  - An initial, \"minimal\" set of data listing the  hierarchy of available tiles , with minimal information to allow an application to determine which tiles need to be loaded based on a certain viewing position in 3d space. Tile Header  - A minimal header describing a tiles bounding volume and a screen space error tolerance (allowing the tile to be culled if it is distant), as well as the URL to load the tile's actual content from. Tile Cache  - Since the number of tiles in big tilesets often exceed what can be loaded into available memory, it is important to have a system that releases no-longer visible tiles from memory. Tileset Traversal  - Dynamically loading and rendering 3D tiles based on current viewing position, possibly triggering loads of new tiles and unloading of older, no-longer visible tiles. Data Format Check  Tiles3DLoader ,  CesiumIonLoader  | |\nand  I3SLoader . Helper Classes Tileset Traversal Support To start loading tiles once a top-level tileset file is loaded, the application can instantiate the  Tileset3D  class and start calling  tileset3D.update(viewport) . Since 3D tiled data sets tend to be very big, the key idea is to only load the tiles actually needed to show a view from the current camera position. The  Tileset3D  allows callbacks ( onTileLoad ,  onTileUnload ) to be registered that notify the app when the set of tiles available for rendering has changed. This is important because tile loads complete asynchronously, after the  tileset3D.update(...)  call has returned. Additional Information Coordinate Systems To help applications process the  position  data in the tiles, 3D Tiles category loaders are expected to provide matrices are provided to enable tiles to be used in both fixed frame or cartographic (long/lat-relative, east-north-up / ENU) coordinate systems: cartesian  WGS84 fixed frame coordinates cartographic  tile geometry positions to ENU meter offsets from  cartographicOrigin . Position units in both cases are in meters. For cartographic coordinates, tiles come with a prechosen cartographic origin and precalculated model matrix. This cartographic origin is \"arbitrary\" (chosen based on the tiles bounding volume center). A different origin can be chosen and a transform can be calculated, e.g. using the math.gl  Ellipsoid  class.","headings":[{"value":"3D Tiles Loaders","depth":1},{"value":"3D Tiles Category Loaders","depth":2},{"value":"Overview","depth":2},{"value":"Concepts","depth":2},{"value":"Data Format","depth":2},{"value":"Helper Classes","depth":2},{"value":"Additional Information","depth":2},{"value":"Coordinate Systems","depth":3}],"slug":"docs/specifications/category-3d-tiles","title":"3D Tiles Loaders"},{"excerpt":"Geospatial Loaders The Geospatial category is experimental Several geospatial formats return data in the form of lists of lng/lat encoded geometric objects. Geospatial Category Loaders Loader Notes GPXLoader KMLLoader MVTLoader TCXLoader WKTLoader Data Format Data Structure A JavaScript object with a number of top-level array-valued fields: Field Description points A  GeoJson  FeatureCollection. lines A  GeoJson  FeatureCollection. polygons A  GeoJson  FeatureCollection. imageoverlays Urls and bounds of bitmap overlays documents folders links GeoJSON Conversion Geospatial category data can be converted to GeoJSON (sometimes with a loss of information). Most geospatial applications can consume geojson.","headings":[{"value":"Geospatial Loaders","depth":1},{"value":"Geospatial Category Loaders","depth":2},{"value":"Data Format","depth":2},{"value":"Data Structure","depth":2},{"value":"GeoJSON Conversion","depth":3}],"slug":"docs/specifications/category-gis","title":"Geospatial Loaders"},{"excerpt":"What's New v3.0 (In Development) Target Release Date: Q1 2021. (Alpha releases are available). @loaders.gl/core New  processOnWorker()  function allows applications to run certain non-loader tasks (such as compression and decompression) on workers. @loaders.gl/compression The new  ZlibWorker ,  LZ4Worker  and  ZstdWorker  exports enable compression and decompression of data to be done on worker threads using the new  processOnWorker()  function. @loaders.gl/crypto The new  CryptoWorker  export enables CRC32, CRC32c and MD5 hashing to be done on worker threads using the new  processOnWorker()  function. @loaders.gl/tile-converter  (NEW) A major new module contributed by ESRI, that implements conversion between the OGC 3D tiles and the OGC I3S tileset formats. A  tile-converter  CLI tool is avaible for automated batch conversion of multi-terabyte tilesets. A converter class API is also available for programmatic use. @loaders.gl/excel  (NEW) New table category loader for Excel spreadsheets in both binary  .xls ,  .xlsb  and XML-based  .xlsx  formats. @loaders.gl/mvt Binary output is now 2-3X faster for large datasets thanks to parsing directly from PBF to binary, rather than going through GeoJSON as an intermediate representation. Speed comparison on some example data sets (MVT tiles parsed per second): Via GeoJSON Direct Speed increase Block groups 2.86/s 5.57/s 1.94X Census layer 6.09/s 11.9/s 1.95X Counties Layer 72.5/s 141/s 1.94X Usa Zip Code Layer 8.45/s 20.3/s 2.4X Benchmarks ran using scripts on a 2012 MacBook Pro, 2.3 GHz Intel Core i7, 8 GB, measuring parsing time of MVTLoader only (network time and rendering is not included) @loaders.gl/textures  (NEW) textures  website example shows which compressed texture formats work on the current device. CompressedTextureLoader  now supports KTX2, DDS and PVR containers. BasisLoader  with latest binaries. CrunchLoader CompressedTextureWriter  is available (for Node.js only) Texture loading API for multi-image-based textures  loadImageTexture ,  loadImageTextureArray ,  loadImageTextureCube A new  NPYLoader  to parse N-dimensional arrays generated by the NumPy Python library for high bit depth data and image textures. @loaders.gl/draco Updated to  draco3d@1.4 Supports binary array fields in draco metadata. Significant performance improvements for library loading and decoding. @loaders.gl/kml New loaders:  GPXLoader  and  TCXLoader  to parse common formats for recorded GPS tracks. v2.3 Release Date: October 12, 2020 This release brings a new Shapefile loader, compression codecs (Zlib, LZ4, Zstandard), support for binary output from geospatial loaders, and a range of improvements supporting loaders.gl integration with kepler.gl, a major geospatial application. @loaders.gl/shapefile  (NEW) A new loader for the ESRI Shapefile format has been added. It loads  .SHP  and (if available)  .DBF ,  .CPG  and  .PRJ  files and returns a geojson like geometry. @loaders.gl/compression  (NEW) A new module with compression/decompression transforms for compression codecs (Zlib, LZ4, Zstandard). As always, these work reliably in both browsers and Node.js. @loaders.gl/crypto  (NEW) A new module for calculating cryptographic hashes (MD5, SHA256 etc). Provided transforms enables hashes to be calculated incrementally, e.g. on incoming binary chunks while streaming data into  parseInBatches . @loaders.gl/draco Draco3D libraries are upgraded to version 1.3.6. Draco metadata can now be encoded and decoded. Custom Draco attributes are now decoded. @loaders.gl/gltf GLBLoader  can now read older GLB v1 files in addition to GLB v2. GLTFLoader  now offers optional, partial support for reading older glTF v1 files and automatically converting them to glTF v2 format (via  options.glt.normalize ). @loaders.gl/json Binary output is now available for the  GeoJsonLoader , via  options.gis.format: 'binary' . @loaders.gl/kml Binary output is now available for the  KMLLoader , via  options.gis.format: 'binary' . @loaders.gl/las Uses a newer version of the  laz-perf  parser (1.4.4). @loaders.gl/mvt Binary output is now available for the Mapbox Vector Tiles  MVTLoader , via  options.gis.format: 'binary' . @loaders.gl/core parseInBatches()  now allows the caller to specify \"transforms\" that shoud be applied on the input data before parsing, via  options.transforms . See the new crypto and compression modules for available transforms to calculate cryptographic hashes on / decompress \"streaming\" data. parseInBatches()  can now be called on all loaders. Non-batched loaders will just return a single batch. options.fetch  ( load ,  parse  etc.) can now be used to supply a either a  fetch  options object or a custom  fetch  function. (BREAKING)  selectLoader()  is now async and returns a  Promise  that resolves to a loader. selectLoader()  can now select loaders through content sniffing of  Blob  and  File  objects. selectLoaderSync()  has been added for situations when calling an async function is not practial. @loaders.gl/polyfills fetch  polyfill: Files with  .gz  extension are automatically decompressed with gzip. The extension reported in the  fetch  response has the  .gz  extension removed. fetch  polyfill: Improved robustness and error handling in Node.js when opening unreadable or non-existent files. Underlying errors ( ENOEXIST ,  EISDIR  etc) are now caught and reported in  Response.statusText . Blob  and  File , new experimental polyfills. v2.2 Framework and loader improvements based on usage in applications. Release Date: June 18, 2020 Typescript Type Definitions Typescript type definitions ( d.ts  files) are now provided for some loaders.gl modules that export APIs (functions and classes). Loader Improvements @loaders.gl/core parseInBatches  a new  options.metadata  option adds an initial batch with metadata about what data format is being loaded. selectLoader  (and  parse  etc) now recognizes unique unregistered MIME types (e.g  application/x.ply ) for every loader. This enable applications that can set  content-type  headers to have precise control over loader selection. @loaders.gl/images The  ImageLoader  now loads images as  Imagebitmap  by default on browsers that support  ImageBitmap  (Chrome and Firefox). The performance improvements are dramatic, which can be verified in the new  benchmark example . @loaders.gl/i3s Addresses a number of compatibility issues with different I3S tilesets that have been reported by users. @loaders.gl/terrain A new  QuantizedMeshLoader  has been added to the  terrain  module to decode the  Quantized Mesh  format. @loaders.gl/video  (new loader module) An experimental new module with video loading and GIF generation support. @loaders.gl/wkt A new  WKBLoader  has been added to the  wkt  module to decode the  Well-Known Binary  format. Worker support for the  WKTLoader , designed to support future binary data improvements. @loaders.gl/json parseInBatches  now accepts  options.json.jsonpaths  to specify which array should be streamed using limited JSONPath syntax (e.g.  '$.features'  for GeoJSON). parseInBatches  returned batches now contain a  batch.bytesUsed  field to enable progress bars. parseInBatches  partial and final result batches are now generated when setting the  metadata: true  options. .geojson  files can now alternatively be parsed by a new experimental  GeoJSONLoader  (exported with an underscore as  _GeoJSONLoader ), introduced to support future binary data improvements. @loaders.gl/csv parseInBatches  now returns a  batch.bytesUsed  field to enable progress bars. Header auto-detection available via  options.csv.header: 'auto' . @loaders.gl/arrow Updated to use  apache-arrow  version  0.17.0 . @loaders.gl/3d-tiles The  Tile3DLoader  now installs the  DracoLoader . The application no longer needs to import and register the  DracoWorkerLoader . @loaders.gl/gltf The  GLTFLoader  now installs the  DracoLoader . The application no longer needs to import and register the  DracoWorkerLoader . @loaders.gl/polyfills The  fetch  and  Response  polyfills for Node.js have been significantly improved, supporting more types of input and parameters with higher fidelity The  fetch  polyfill now automatically add the  accept-encoding  header and automatically decompresses  gzip ,  brotli  and  deflate  compressed responses. v2.1 Release Date: Mar 16, 2020 This release adds a number of new geospatial format loaders New Geospatial Loaders The new loaders empowers rendering frameworks to visualize various geospatial datasets. @loaders.gl/i3s  (new loader module) A new loader module for  I3S  tiles is added to the 3D Tiles family. Checkout the San Francisco Buildings  example . This is a collaboration with ESRI and Tamrat Belayneh  @Tamrat-B @loaders.gl/mvt  (new loader module) A new loader module for loading  Mapbox Vector Tiles . Development was led by contributors from  CARTO , @loaders.gl/terrain  (new loader module) A new loader module for reconstructing mesh surfaces from height map images. Check out the  example  with deck.gl's  TerrainLayer . @loaders.gl/wkt  (new loader module) A new loader module for the Well-Known Text geometry format. Other Improvements @loaders.gl/core The  load  and  parse  functions can now read data directly from  Stream  objects both in node and browser. @loaders.gl/arrow The ArrowJS dependency has been upgraded to v0.16. The ArrowJS API documentation in the loaders.gl website has been improved. @loaders.gl/images Images can now be loaded as data: Using the  ImageLoader  with  options.image.type: 'data'  parameter will return an  image data object  with width, height and a typed array containing the image data (instead of an opaque  Image  or  ImageBitmap  instance). ImageBitmap  loading now works reliably, use  ImageLoader  with  options.image.type: 'imagebitmap' . @loaders.gl/json The streaming JSON loader now has an experimental option  _rootObjectBatches  that returns the top-level JSON object containing the JSON array being streamed, as additional first (partial) and last (complete) batches. Mesh Category Add  boundingBox  to  mesh category  header v2.0 Release Date: Dec 20, 2019 The 2.0 release brings potentially dramatic bundle size savings through dynamic loading of loaders and workers, significant overhauls to several loaders including , image loading improvements and the glTF loader, and a powerful loader composition system. Loader-Specific Options  Each loader now defines its own sub object in the options object. This makes it possible to cleanly specify options for multiple loaders at the same time. This is helpful when loaders.gl auto-selects a pre-registered loader or when passing options to a sub-loader when using a composite loader. Smaller Loaders  Big loaders such as  DracoLoader  and  BasisLoader  that use large libraries (e.g. WASM/WebAssembly or emscripten/C++ transpiled to JavaScript) now load those libraries dynamically from  unpkg.com  CDN resulting in dramatic bundle size savings. E.g the bundle size impact of the  DracoLoader  was reduced from > 1MB to just over 10KB. Worker Loaders Ease-of-use: Worker loading is provided by the main loader objects. It is not necessary to import the  ...WorkerLoader  objects to enable worker loading (but see below about bundle size) Performance: Loading on worker threads is now the default: All worker enabled loaders now run on worker threads by default (set  options.worker: false  to disable worker-thread loading and run the loader on the main thread). Debugging: Development builds of workers are now available on  unpkg.com  CDN, eabling debugging of worker loaders. Bundle size: Workers are no longer bundled, but loaded from from the  unpkg.com  CDN. Bundle size: Note that the old  ...WorkerLoader  classes are still available. Using these can save even more bundle space since during tree-shaking since they do not depend on the non-worker parser. Composite Loaders The new  composite loader  architecture enables complex loaders like  Tiles3DLoader  and  GLTFLoader  to be composed from more primitive loaders without losing the ability to run some parts on worker, pass arguments to sub-loaders etc. New Loader Modules @loaders.gl/basis  (Experimental) A new module for the basis format that enables. This module also provides a  CompressedImageLoader  for more traditional compressed images. @loaders.gl/json  (Experimental) A new streaming  JSONLoader  that supports batched (i.e. streaming) parsing from standard JSON files, e.g. geojson. No need to reformat your files as line delimited JSON. Update Loader Modules @loaders.gl/gltf  the  GLTFLoader  is now a \"composite loader\". The perhaps most important change is that  load(url, GLTFLoader)  also loads all sub-assets, including images, Draco compressed meshes, etc making the loaded data easier for applications to use. @loaders.gl/images  see below for a list of changes @loaders.gl/images Updates New ImageLoader options   options: {image: {}}  contain common options that apply across the category options.image.type , Ability to control loaded image type enabling faster  ImageBitmap  instances to be loaded via  type: 'imagebitmap . Default  auto  setting returns traditional HTML image objects. Image Decoding.  options.image.decodeHTML: true  -  ImageLoader  now ensures HTML images are completely decoded and ready to be used when the image is returned (by calling  Image.decode() ). Parsed Image API  Since the type of images returned by the  ImageLoader  depends on the  {image: {type: ...}}  option, a set of functions are provided to work portably with loaded images:  isImage() ,  getImageType() ,  getImageData() , ... Binary Image API  Separate API to work with unparsed images in binary data form:  isBinaryImage() ,  getBinaryImageType() ,  getBinaryImageSize() , ... \"Texture\" Loading API  New methods  loadImages  and  loadImageCube  can signficantly simplify loading of arrays of arrays of (mipmapped) images that are often used in 3D applications. These methods allow an entire complex of images (e.g. 6 cube faces with 10 mip images each) to be loaded using a single async call. Improved Node.js support  More image test cases are now run in both browser and Node.js and a couple of important Node.js issues were uncovered and fixed. v1.3 Release Date: Sep 13, 2019 The 1.3 release is focused on production quality 3D tiles support, maturing the v2 glTF parser, and provides some improvements to the core API. @loaders.gl/3d-tiles Tile3DLayer moved to deck.gl The  Tile3DLayer  can now be imported from  @deck.gl/geo-layers , and no longer needs to be copied from the loaders.gl  3d-tiles  example Batched 3D Model Tile Support b3dm  tiles can now be loaded and displayed by the  Tile3DLayer  (in addition to  pnts  tiles). Performance Tracking Tileset3D  now contain a  stats  object which tracks the loading process to help profile big tilesets. Easily displayed in your UI via the  @probe.gl/stats-widget  module (see 3d-tiles example). Request Scheduling The  Tileset3D  class now cancels loads for not-yet loaded tiles that are no longer in view). Scheduling dramatically improves loading performance when panning/zooming through large tilesets. @loaders.gl/gltf Version 2 Improvements Select the new glTF parser by passing  options.gltf.parserVersion: 2  to the  GLTFLoader . Many improvements to the v2 glTF parser. @loaders.gl/core Loader Selection Improvements The loader selection mechanism is now exposed to apps through the new  selectLoader  API. Loaders can now examine the first bytes of a file This complements the existing URL extension based auto detection mechanisms. Worker Thread Pool Now reuses worker threads. Performance gains by avoiding worker startup overhead. Worker threads are named, easy to track in debugger Worker based loaders can now call  parse  recursively to delegate parsing of embedded data (e.g. glTF, Draco) to other loaders v1.2 The 1.2 release is a smaller release that resolves various issues encountered while using 1.1. Release Date: Aug 8, 2019 @loaders.gl/core : File Type Auto Detection now supports binary files @loaders.gl/polyfills : Fixed  TextEncoder  warnings @loaders.gl/arrow : Improved Node 8 support @loaders.gl/images : Image file extensions now added to loader object @loaders.gl/gltf : Generate default sampler parameters if none provided in gltf file @loaders.gl/3d-tiles (EXPERIMENTAL) Support for dynamic traversal of 3D tilesets (automatically loads and unloads tiles based on viewer position and view frustum). Support for loading tilesets from Cesium ION servers. Asynchronous tileset loading Auto centering of view based on tileset bounding volumes deck.gl  Tile3DLayer  class provided in examples. v1.1 The 1.1 release addresses a number of gaps in original loaders.gl release, introduces the  GLTFLoader , and initiates work on 3DTiles support. Release Date: May 30, 2019 @loaders.gl/core fetchFile  function - Can now read browser  File  objects (from drag and drop or file selection dialogs). isImage(arrayBuffer [, mimeType])  function - can now accept a MIME type as second argument. @loaders.gl/images getImageMIMEType(arrayBuffer)  function ( EW) - returns the MIME type of the image in the supplied  ArrayBuffer . isImage(arrayBuffer [, mimeType])  function - can now accept a MIME type as second argument. @loaders.gl/gltf The glTF module has been refactored with the aim of simplifying the loaded data and orthogonalizing the API. \"Embedded' GLB data (GLBs inside other binary formats) can now be parsed (e.g. the glTF parser can now extract embedded glTF inside 3D tile files). New classes/functions: GLTFScenegraph  class (NEW) - A helper class that provides methods for structured access to and modification/creation of glTF data. postProcessGLTF  function ( EW) - Function that performs a set of transformations on loaded glTF data that simplify application processing. GLBLoader / GLBWriter  - loader/writer pair that enables loading/saving custom (non-glTF) data in the binary GLB format. GLTFLoader , letting application separately handle post-processing. @loaders.gl/3d-tiles (NEW MODULE) Support for the 3D tiles format is being developed in the new  @loaders.gl/3d-tiles  module. Loading of individual point cloud tiles, including support for Draco compression and compact color formats such as RGB565 is supported. @loaders.gl/polyfills (NEW MODULE) Node support now requires importing  @loaders.gl/polyfills  before use. This reduces the number of dependencies, bundle size and potential build complications when using other loaders.gl modules when not using Node.js support. @loaders.gl/loader-utils (NEW MODULE) Helper functions for loaders have been broken out from  @loaders.gl/core . Individual loaders no longer depend on @loaders.gl/core  but only on  @loaders.gl/loader-utils . v1.0 Release Date: April 2019 First Official Release","headings":[{"value":"What's New","depth":1},{"value":"v3.0 (In Development)","depth":2},{"value":"v2.3","depth":2},{"value":"v2.2","depth":2},{"value":"Typescript Type Definitions","depth":3},{"value":"Loader Improvements","depth":3},{"value":"v2.1","depth":2},{"value":"New Geospatial Loaders","depth":3},{"value":"Other Improvements","depth":3},{"value":"v2.0","depth":2},{"value":"New Loader Modules","depth":3},{"value":"Update Loader Modules","depth":3},{"value":"@loaders.gl/images Updates","depth":3},{"value":"v1.3","depth":2},{"value":"@loaders.gl/3d-tiles","depth":3},{"value":"@loaders.gl/gltf","depth":3},{"value":"@loaders.gl/core","depth":3},{"value":"v1.2","depth":2},{"value":"@loaders.gl/3d-tiles (EXPERIMENTAL)","depth":3},{"value":"v1.1","depth":2},{"value":"@loaders.gl/core","depth":3},{"value":"@loaders.gl/images","depth":3},{"value":"@loaders.gl/gltf","depth":3},{"value":"@loaders.gl/3d-tiles (NEW MODULE)","depth":3},{"value":"@loaders.gl/polyfills (NEW MODULE)","depth":3},{"value":"@loaders.gl/loader-utils (NEW MODULE)","depth":3},{"value":"v1.0","depth":2}],"slug":"docs/whats-new","title":"What's New"},{"excerpt":"3D Tiles and I3S Spec comparisons Specs: 3D Tiles I3S Common parts: Data organized as a tree Each data node bounding volume has a field to indicate sufficient for a certain view or not containing positions / colors / normals / texture tileset: .json metadata for the entire tileset\ntile header: .json metadata for the tile\ntile content: .bin geometries, textures Tileset.json tileset - layers/0\ntile-node file nodePages/0  node/0 - metadata\ngeometry file / texture file / feature file (optional) 3D Tiles I3S tileset file tileset.json  (token is required) i3s layer Tileset structure asset // version \"profile\": \"meshpyramids\", // tileset type geometricError // \"rootNode\": \"./nodes/root\", // relative url root // hierarchy of the tileset, tileheader for each tile node \"version\": \"1.6\", \"obb/mbs/extend\": [],// tileset boundary \"lodSelection\": { \"metricType\": \"maxScreenThreshold\", \"maxError\": 34.87550189480981 }, \"defaultGeometrySchema\": {} // how to parse content tile files tile metadata is already specified in tileset.json each tile node has >4 files for the following info tile content stored in a separate file - metadata - geometry - feature - texture Key fields geometricError  in meters maxScreenThreshold  in screenCoords geometricError is a nonnegative number that defines the error, in meters, that determines if the tileset is rendered. A per-node value for the maximum area of the projected bounding volume on screen in pixel screenSize(Bounding box) At runtime, the geometric error is used to compute Screen-Space Error (SSE), the error measured in pixels. Transform matrix ( I3SLoad  is not using it) Transform matrix refine - Add - point cloud - Replace - mesh boundingBox, mbs is bounding box boundingSphere, function traverseNodeTree(node) { region if (node mbs is not visible) { // do nothing } else if (node has no children or viewport.screenSize(mbs)< node.maxScreenThreshold) { // render the node } else { for each child in children(node) TraverseNodeTree(child); } }","headings":[],"slug":"docs/specifications/3d-tiles-and-i3s","title":""},{"excerpt":"Table Loaders The  table  category loaders supports loading tables in  row-based ,  columnar  or  batched columnar  formats. Table Category Loaders Loader Notes ArrowLoader CSVLoader JSONLoader Set  options.json.table  to  true Data Structure Field Type Contents schema Object Metadata of the table, maps name of each column to its type. data Object  or  Array Data of the table, see  table types length Number Number of rows Table Types loaders.gl deals with (and offers utilities to convert between) three different types of tables: Classic Tables (Row-Major) This is the classic JavaScript table.  data  consists of an  Array  of  Object  instances, each representing a row. Columnar Tables (Column-Major) Columnar tables are stored as one array per column. Columns that are numeric can be loaded as typed arrays which are stored in contigous memory.  data  is an  Object  that maps column names to an array or typed array. Contiguous memory has tremendous benefits: Values are adjacent in memory, the resulting cache locality can result in big performance gains Typed arrays can of course be efficiently transferred from worker threads to main thread Can be directly uploaded to the GPU for further processing. Chunked Columnar Tables (DataFrames) A problem with columnar tables is that column arrays they can get very long, causing issues with streaming, memory allication etc. A powerful solution is to worked with chunked columnar tables, where columns is are broken into matching sequences of typed arrays. The down-side is that complexity can increase quickly. Data Frames are optimized to minimize the amount of copying/moving/reallocation of data during common operations such e.g. loading and transformations, and support zero-cost filtering through smart iterators etc. Using the Arrow API it is possible to work extremely efficiently with very large (multi-gigabyte) datasets.","headings":[{"value":"Table Loaders","depth":1},{"value":"Table Category Loaders","depth":2},{"value":"Data Structure","depth":2},{"value":"Table Types","depth":2},{"value":"Classic Tables (Row-Major)","depth":3},{"value":"Columnar Tables (Column-Major)","depth":3},{"value":"Chunked Columnar Tables (DataFrames)","depth":3}],"slug":"docs/specifications/category-table","title":"Table Loaders"},{"excerpt":"Mesh and PointCloud Loaders The  mesh and pointcloud  loader category is intended for simpler mesh and point clouds formats that describe a \"single geometry primitive\" (as opposed to e.g. a scenegraph consisting of a hierarchy of multiple geometries). Mesh/PointCloud Category Loaders Loader Notes DracoLoader LASLoader OBJLoader PCDLoader PLYLoader QuantizedMeshLoader TerrainLoader Data Format A single mesh is typically defined by a set of attributes, such as  positions ,  colors ,  normals  etc, as well as a draw mode. The Pointcloud/Mesh loaders output mesh data in a common form that is optimized for use in WebGL frameworks: All attributes (and indices if present) are stored as typed arrays of the proper type. All attributes (and indices if present) are wrapped into glTF-style \"accessor objects\", e.g.  {size: 1-4, value: typedArray} . Attribute names are mapped to glTF attribute names (on a best-effort basis). An  indices  field is added (only if present in the loaded geometry). A primitive drawing  mode  value is added (the numeric value matches WebGL constants, e.g  GL.TRIANGLES ). Field Type Contents loaderData Object  (Optional) Loader and format specific data header Object See  Header mode Number See  Mode attributes Object Keys are  glTF attribute names  and values are  accessor  objects. indices Object  (Optional) If present, describes the indices (elements) of the geometry as an  accessor  object. Header The  header  fields are only recommended at this point, applications can not assume they will be present: header  Field Type Contents vertexCount Number boundingBox Array [[minX, minY, minZ], [maxX, maxY, maxZ]] Mode Primitive modes are aligned with  OpenGL/glTF primitive types Value Primitive Mode Comment 0 POINTS Used for point cloud category data 1 LINES Lines are rarely used due to limitations in GPU-based rendering 2 LINE_LOOP - 3 LINE_STRIP - 4 TRIANGLES Used for most meshes. Indices attributes are often used to reuse vertex data in remaining attributes 5 TRIANGLE_STRIP - 6 TRIANGLE_FAN - Accessor attributes  and  indices  are represented by glTF \"accessor objects\" with the binary data for that attribute resolved into a typed array of the proper type. Accessors Fields glTF? Type Contents value No TypedArray Contains the typed array (corresponds to  bufferView ). The type of the array will match the GL constant in  componentType . size No Number Number of components,  1 - 4 . byteOffset Yes Number Starting offset into the bufferView. count Yes Number The number of elements/vertices in the attribute data. originalName No String  (Optional) If this was a named attribute in the original file, the original name (before substitution with glTF attribute names) will be made available here. glTF Attribute Name Mapping To help applications manage attribute name differences between various formats, mesh loaders map known attribute names to  glTF 2.0 standard attribute names  a best-effort basis. When a loader can map an attribute name, it will replace ir with the glTF equivalent. This allows applications to use common code to handle meshes and point clouds from different formats. Name Accessor Type(s) Component Type(s) Description POSITION \"VEC3\" 5126  (FLOAT) XYZ vertex positions NORMAL \"VEC3\" 5126  (FLOAT) Normalized XYZ vertex normals TANGENT \"VEC4\" 5126  (FLOAT) XYZW vertex tangents where the  w  component is a sign value (-1 or +1) indicating handedness of the tangent basis TEXCOORD_0 \"VEC2\" 5126  (FLOAT),  5121  (UNSIGNED_BYTE) normalized,  5123  (UNSIGNED_SHORT) normalized UV texture coordinates for the first set TEXCOORD_1 \"VEC2\" 5126  (FLOAT),  5121  (UNSIGNED_BYTE) normalized,  5123  (UNSIGNED_SHORT) normalized UV texture coordinates for the second set COLOR_0 \"VEC3\" ,  \"VEC4\" 5126  (FLOAT),  5121  (UNSIGNED_BYTE) normalized,  5123  (UNSIGNED_SHORT) normalized RGB or RGBA vertex color JOINTS_0 \"VEC4\" 5121  (UNSIGNED_BYTE),  5123  (UNSIGNED_SHORT) WEIGHTS_0 \"VEC4\" 5126  (FLOAT),  5121  (UNSIGNED_BYTE) normalized,  5123  (UNSIGNED_SHORT) normalized Note that for efficiency reasons, mesh loaders are not required to convert the format of an attribute's binary data to match the glTF specifications (i.e. if normals were encoded using BYTES then that is what will be returned even though glTF calls out for FLOAT32). Any such alignment needs to be done by the application as a second step. Limitations Scenegraph support For more complex, scenegraph-type formats (i.e. formats that contain multiple geometric primitives), loaders.gl provides glTF 2.0 support via the  GLTFLoader . Material support Material support is provided by some mesh formats (e.g. OBJ/MTL) and is currently not implemented by loaders.gl, however the glTF loader has full support for PBR (Physically-Based Rendering) materials.","headings":[{"value":"Mesh and PointCloud Loaders","depth":1},{"value":"Mesh/PointCloud Category Loaders","depth":2},{"value":"Data Format","depth":2},{"value":"Header","depth":3},{"value":"Mode","depth":3},{"value":"Accessor","depth":3},{"value":"glTF Attribute Name Mapping","depth":3},{"value":"Limitations","depth":2},{"value":"Scenegraph support","depth":3},{"value":"Material support","depth":3}],"slug":"docs/specifications/category-mesh","title":"Mesh and PointCloud Loaders"},{"excerpt":"Image Loaders The image loader category documents a common data format, options, conventions and utilities for loader and writers for images that follow loaders.gl conventions. Image Category Loaders Loader Notes ImageLoader Loads compressed images (PNG, JPG, etc) CompressedTextureLoader Parses compressed textures to image data mipmap array BasisLoader Transpiles into supported compressed texture format Core image category support is provided by the  @loaders.gl/images  module: Usage Individual loaders for specific image formats can be imported for  @loaders.gl/images : However since each image loader is quite small (in terms of code size and bundle size impact), most applications will just install all image loaders in one go: Image Types Images can be loaded as image data or as opaque image objects ( Image  or  ImageBitmap ), and the image  type  option can be used to control the type of image object produced by the  ImageLoader . A loaded image can always be returned as an  image data  object (an object containing a  Uint8Array  with the pixel data, and metadata like  width  and  height , and in Node.js images are always loaded as image data objects). In the browser, the  ImageLoader  uses the browser's native image loading functionality, and if direct access to the image data is not required, it is more efficient to load data into an opaque image object. The  ImageLoader  prefers  ImageBitmap  when supported, falling back to  Image  (aka  HTMLImageElement ) on older browsers. Note that  type  is independent of the  format  of the image (see below). Image Type Class Availability Workers Description data Object with  {width: Number, height: Number, data: Uint8Array, ...} Node.js and browsers No Compatible with headless gl. imagebitmap ImageBitmap Chrome/Firefox Yes:  transferrable A newer JavaScript class designed for efficient loading of images, optimized for use in worker threads and with WebGL image Image  (aka  HTMLImageElement ) All browsers No The traditional HTML/JavaScript class used for image loading into DOM trees. WebGL compatible. Image Data Image data objects are images loaded as data, represented by an object that contains a typed array with the pixel data, size, and possibly additional metadata  {width: Number, height: Number, data: Uint8Array, ...} To get an image data object from a loaded  Image  or  ImageBitmap , call  getImageData(image) . To load an image data object directly, set the  image.type: 'data'  option when loading the image. Image Formats The  format  of the image describes how the memory is laid out. It is mainly important when working with  data   type  images. The default format / memory layout for image data is  RGBA  and  UNSIGNED_BYTE  i.e. four components per pixel, each a byte. Some loaders may add additional fields to the image data structure to describe the data format. Currently the image category does not provide any documentation for how to describe alternate formats/memory layouts, however a preliminary recommendation is to follow OpenGL/WebGL conventions. Compressed Images Compressed images are always returned as image data objects. They will have an additional field,  compressed: true , indicating that the typed array in the  data  field contains compressed pixels and is not directly indexable. Applications that use e.g. the  CompressedTextureLoader  and/or the  BasisLoader  together with the  ImageLoader  can check this flag before attempting to access the image data. Options The image category support some generic options (specified using  options.image.<option-name> ), that are applicable to all (or most) image loaders. Option Default Type Availability Description options.image.type 'auto' string See table One of  auto ,  data ,  imagebitmap ,  image options.image.decode true boolean No: Edge, IE11 Wait for HTMLImages to be fully decoded. Notes About worker loading Worker loading is only supported for the  data  and  imagebitmap  formats. Since image worker loading is only available on some browsers (Chrome and Firefox), the  ImageLoader  dynamically determines if worker loading is available. Use  options.worker: false  to disable worker loading of images. Image API The image category also provides a few utilities: Detecting (\"sniffing\") mime type and size of image files before parsing them Getting image data (arrays of pixels) from an image without knowing which type was loaded (TBA) Remarks ImageData Image data objects return by image category loaders have the same fields ( width ,  height ,  data ) as the browser's built-in  ImageData  class, but are not actual instances of  ImageData . However, should you need it, it is easy to create an  ImageData  instance from an image data object:","headings":[{"value":"Image Loaders","depth":1},{"value":"Image Category Loaders","depth":2},{"value":"Usage","depth":2},{"value":"Image Types","depth":2},{"value":"Image Data","depth":2},{"value":"Image Formats","depth":3},{"value":"Compressed Images","depth":2},{"value":"Options","depth":2},{"value":"Notes","depth":2},{"value":"About worker loading","depth":3},{"value":"Image API","depth":2},{"value":"Remarks","depth":2},{"value":"ImageData","depth":3}],"slug":"docs/specifications/category-image","title":"Image Loaders"},{"excerpt":"Scenegraph Loaders The Scenegraph category is intended to represent glTF scenegraphs. Loaders Loader Notes GLTFLoader GLBLoader Data Format The data format is fairly raw, close to the unpacked glTF/GLB data structure, it is described by: a parsed JSON object (with top level arrays for  scenes ,  nodes  etc) a list of  ArrayBuffer s representing binary blocks (into which  bufferViews  and  images  in the JSON point). Data Structure A JSON object with the following top-level fields: Field Type Default Description magic Number glTF The first four bytes of the file version Number 2 The version number json Object {} The JSON chunk buffers ArrayBuffer[] [] (glTF) The BIN chunk plus any base64 or BIN file buffers Buffers can be objects in the shape of  {buffer, byteOffset, byteLength} . Helper Classes To simplify higher-level processing of the loaded, raw glTF data, several helper classes are provided in the  @loaders.gl/gltf  module, these can: unpack and remove certain glTF extensions extract typed array views from the JSON objects into the binary buffers create HTML images from image buffers etc Non-glTF Scenegraphs The scenegraph \"category\" was created specifically for the  glTF  format, and there are no plans to support other scenegraph formats in loaders.gl (as such formats tend to have large and complex specifications with many edge cases). Therefore, the current recommendation is to first convert scenegraph files in other formats to glTF with external tools before loading them using loaders.gl. That said, hypothetical new loaders for other scenegraph formats (e.g. a COLLADA loader) could potentially choose to belong to the Scenegraph category by \"converting\" loaded data to the format described on this page. It would thus enable interoperability with applications that are already designed to use the  GLTFLoader ).","headings":[{"value":"Scenegraph Loaders","depth":1},{"value":"Loaders","depth":2},{"value":"Data Format","depth":2},{"value":"Data Structure","depth":2},{"value":"Helper Classes","depth":2},{"value":"Non-glTF Scenegraphs","depth":2}],"slug":"docs/specifications/category-scenegraph","title":"Scenegraph Loaders"},{"excerpt":"Loader Object To be compatible with the parsing/loading functions in  @loaders.gl/core  such as  parse  and  load , a parser needs to be described by a \"loader object\" conforming to the following specification. Loader Object Format v1.0 Common Fields Field Type Default Description name String Required Short name of the loader ('OBJ', 'PLY' etc) extension String Required Three letter (typically) extension used by files of this format extensions String[] Required Array of file extension strings supported by this loader category String Optional Indicates the type/shape of data parse   |   worker Function null Every non-worker loader should expose a  parse  function. Note: Only one of  extension  or  extensions  is required. If both are supplied,  extensions  will be used. Test Function Field Type Default Description test Function String String[] null Guesses if a binary format file is of this format by examining the first bytes in the file. If the test is specified as a string or array of strings, the initial bytes are expected to be \"magic bytes\" matching one of the provided strings. testText Function null Guesses if a text format file is of this format by examining the first characters in the file Parser Functions Each (non-worker) loader should define a  parse  function. Additional parsing functions can be exposed depending on the loaders capabilities, to optimize for text parsing, synchronous parsing, streaming parsing, etc: Parser function field Type Default Description parse Function null Asynchronously parses binary data (e.g. file contents) asynchronously ( ArrayBuffer ). parseInBatches  (Experimental) Function null Parses binary data chunks ( ArrayBuffer ) to output data \"batches\" parseSync Function null Atomically and synchronously parses binary data (e.g. file contents) ( ArrayBuffer ) parseTextSync Function null Atomically and synchronously parses a text file ( String ) Synchronous parsers are more flexible as they can support synchronous parsing which can simplify application logic and debugging, and iterator-based parsers are more flexible as they can support batched loading of large data sets in addition to atomic loading. You are encouraged to provide the most capable parser function you can (e.g.  parseSync  or  parseToIterator  if possible). Unless you are writing a completely new loader from scratch, the appropriate choice often depends on the capabilities of an existing external \"loader\" that you are working with. Parser Function Signatures async parse(data : ArrayBuffer, options : Object, context : Object) : Object parseSync(data : ArrayBuffer, options : Object, context : Object) : Object parseInBatches(data : AsyncIterator, options : Object, context : Object) : AsyncIterator The  context  parameter will contain the foolowing fields parse  or  parseSync url  if available","headings":[{"value":"Loader Object","depth":1},{"value":"Loader Object Format v1.0","depth":2},{"value":"Common Fields","depth":3},{"value":"Test Function","depth":3},{"value":"Parser Functions","depth":3},{"value":"Parser Function Signatures","depth":3}],"slug":"docs/specifications/loader-object-format","title":"Loader Object"},{"excerpt":"Creating New Loaders and Writers See the a detailed specification of the  loader object format API reference . Overview Applications can also create new loader objects. E.g. if you have existing JavaScript parsing functionality that you would like to use with the loaders.gl core utility functions. Creating a Loader Object You would give a name to the loader object, define what file extension(s) it uses, and define a parser function. Field Type Default Description name String Required Short name of the loader ('OBJ', 'PLY' etc) extension String Required Three letter (typically) extension used by files of this format testText Function null Guesses if a file is of this format by examining the first characters in the file A loader must define a parser function for the format, a function that takes the loaded data and converts it into a parsed object. Depending on how the underlying loader works (whether it is synchronous or asynchronous and whether it expects text or binary data), the loader object can expose the parser in a couple of different ways, specified by provided one of the parser function fields. Dependency Management In general, it is recommended that loaders are \"standalone\" and avoid importing  @loaders.gl/core .  @loaders.gl/loader-utils  provides a small set of shared loader utilities. Creating Composite Loaders loaders.gl enables loaders to call other loaders (referred to as \"sub-loaders\" in this section). This enables loaders for \"composite formats\" to be \"composed\" out of loaders for the primitive parts. Good examples of sub-loaders are the  GLTFLoader  which can delegate Draco mesh decoding to the  DracoLoader  and image decoding to the various  ImageLoaders  and the  BasisLoader . Naturally, Composite loaders can call other composite loaders, which is for instance used by the  Tiles3DLoader  which uses the  GLTFLoader  to parse embedded glTF data in certain tiles. Calling loaders inside loaders To call another loader, a loader should use the appropriate  parse  function provided in the  context  parameter. A conceptual example of a 3D Tiles loader calling the  GLTFLoader  with some additional options. Remarks: While a loader could potentially import  parse  from  @loaders.gl/core  to invoke a sub-loader, it is discouraged, not only from a dependency management reasons, but it prevents loaders.gl from properly handling parameters and allow worker-loaders to call other loaders.","headings":[{"value":"Creating New Loaders and Writers","depth":1},{"value":"Overview","depth":2},{"value":"Creating a Loader Object","depth":2},{"value":"Dependency Management","depth":2},{"value":"Creating Composite Loaders","depth":2},{"value":"Calling loaders inside loaders","depth":2}],"slug":"docs/developer-guide/creating-loaders-and-writers","title":"Creating New Loaders and Writers"},{"excerpt":"Writer Object To be compatible with  @loaders.gl/core  functions such as  encode , writer objects need to conform to the following specification: Common Fields Field Type Default Description name String Required Short name of the loader ('OBJ', 'PLY' etc) extension String Required Three letter (typically) extension used by files of this format category String Optional Indicates the type/shape of data Encoder Function Field Type Default Description encodeSync Function null Encodes synchronously encode Function null Encodes asynchronously encodeInBatches  (Experimental) Function null Encodes and releases batches through an async iterator Note: The format of the input data to the encoders depends on the loader. Several loader categories are defined to provided standardized data formats for similar loaders.","headings":[{"value":"Writer Object","depth":1},{"value":"Common Fields","depth":3},{"value":"Encoder Function","depth":3}],"slug":"docs/specifications/writer-object-format","title":"Writer Object"},{"excerpt":"Get Started Installing Install loaders.gl core and loader for any modules you would like to use. Each format is published as a separate npm module. Usage You can import a loader and use it directly with  parse . Note that  parse  can accept a  fetch  response object as the source of data to be parsed: You can register loaders after importing them Then, in the same file (or some other file in the same app) that needs to load CSV, you no longer need to supply the loader to  parse . It will autodetect the pre-registered loader: Building You can use your bundler of choice such as webpack or rollup. See the  get-started  examples for minimal working examples of how to bundle loaders.gl. Supporting Older Browsers loaders.gl is designed to leverage modern JavaScript (ES2018) and to optimize functionality and performance on evergreen browsers. However, the default distribution is completely transpiled to ES5 so using loaders.gl with older or \"slower moving\" browsers such as IE11 and Edge is possible, assuming that the appropriate polyfills are installed. To build on Edge and IE11,  TextEncoder  and  TextDecoder  must be polyfilled. There are several polyfills available on  npm , but you can also use the polyfills provided by loaders.gl: Supporting Node.js A number of polyfills for  fetch ,  TextEncoder  etc are available to make loaders.gl work under Node.js, just install the  @loaders.gl/polyfills module  as described above.","headings":[{"value":"Get Started","depth":1},{"value":"Installing","depth":2},{"value":"Usage","depth":2},{"value":"Building","depth":2},{"value":"Supporting Older Browsers","depth":2},{"value":"Supporting Node.js","depth":2}],"slug":"docs/developer-guide/get-started","title":"Get Started"},{"excerpt":"Managing Dependencies This section is work in progress, not all options are implemented/finalized Parsers and encoders for some formats are quite complex and can be quite big in terms of code size. Loading Dependencies from Alternate CDN By default, loaders.gl loads pre-built workers and a number of bigger external libraries from the  https://unpkg.com/  CDN. It is possible to specify other CDNs using  options.cdn . Keep in mind that it is typically not sufficient to point to a server that just serves the data of the files in question. Browsers do a number of security checks on cross-origin content and requires certain response headers to be properly set, and unfortunately, error messages are not always helpful. To determine your candidate CDN service is doing what is needed, check with  curl -u <url>  and look for headers like: Loading Dependencies from Your Own Server By setting  options.cdn: false  and doing some extra setup, you can load dependencies from your own server. This removes the impact of a potentially flaky CDN. Options: Load from  node_modules/@loaders.gl/<module>/dist/libs/... Load from a modules directory  libs/... Load from unique locations -  options.modules[<dependency name>]  can be set to url strings. Bundling Dependencies It is also possible to include dependencies in your application bundle PRO: Doesn't require copying/configuring/serving supporting modules. CON: Increases the size of your application bundle options.modules  will let your application  import  or  require  dependencies (thus bundling them) and supply them to loaders.gl. See each loader module for information on its dependencies. Example: bundling the entire  draco3d  library:","headings":[{"value":"Managing Dependencies","depth":1},{"value":"Loading Dependencies from Alternate CDN","depth":3},{"value":"Loading Dependencies from Your Own Server","depth":3},{"value":"Bundling Dependencies","depth":3}],"slug":"docs/developer-guide/dependencies","title":"Managing Dependencies"},{"excerpt":"Development Environment The  master  branch is the active development branch. Building loaders.gl locally from the source requires node.js  >=10 .\nWe use  yarn  to manage the dependencies. Running Tests yarn bootstrap : Install and build workers etc. Run every time you pull a new branch. yarn lint : Check coding standards and formatting yarn lint fix : Fix errors with formatting yarn test node : Quick test run under Node.js yarn test browser : Test run under browser, good for interactive debugging yarn test : Run lint, node test, browser tests (in headless mode) Environment Setup Note that our primary development environment is MacOS, but it is also possible to build loaders.gl on Linux and Windows. Develop on Windows It is possible to build loaders.gl on Windows 10, but not directly in the Windows command prompt. You will need to install a Linux command line environment. First, install  WSL (Windows Subsystem for Linux)  on Windows 10, and follow the  Linux  directions. Note that you may also need to make some decisions on where to place your code and whether to link the linux subsystem to your windows drives. Once this is done, follow the instructions for developing on Linux. Develop on Linux On Linux systems, the following packages are necessary for running webgl-based headless render tests. mesa-utils xvfb libgl1-mesa-dri libglapi-mesa libosmesa6 libxi-dev To get the headless tests working:  export DISPLAY=:99.0; sh -e /etc/init.d/xvfb start Appendix: Installing JavaScript Development Tools You will of course need to install the basic JavaScript development tools. Unless you are new to JavaScript development you most likely already have these in place. The following should work on a linux system. Install Node and NPM Option: Install NVM https://www.liquidweb.com/kb/how-to-install-nvm-node-version-manager-for-node-js-on-ubuntu-12-04-lts/ https://github.com/nvm-sh/nvm/releases Install yarn https://www.hostinger.com/tutorials/how-to-install-yarn-on-ubuntu/ Install jq","headings":[{"value":"Development Environment","depth":1},{"value":"Running Tests","depth":2},{"value":"Environment Setup","depth":2},{"value":"Develop on Windows","depth":3},{"value":"Develop on Linux","depth":3},{"value":"Appendix: Installing JavaScript Development Tools","depth":2},{"value":"Install Node and NPM","depth":3},{"value":"Option: Install NVM","depth":3},{"value":"Install yarn","depth":3},{"value":"Install jq","depth":3}],"slug":"docs/developer-guide/dev-env","title":"Development Environment"},{"excerpt":"Loader Categories To simplify working with multiple similar formats, loaders and writers in loaders.gl are grouped into  categories . The idea is that many loaders return very similar data (e.g. point clouds loaders), which makes it possible to represent the loaded data in the same data structure, letting applications handle the output from multiple loaders without When a loader is documented as belonging to a specifc category, it converts the parsed data into the common format for that category. This allows an application to support multiple formats with a single code path, since all the loaders will return similar data structures. Categories and Loader Registration The fact that loaders belong to categories enable applications to flexibly register new loaders in the same category. For instance, once an application has added support for one loader in a category, other loaders in the same category can be registered during application startup. Original code Now support for additional point cloud formats can be added to the application without touching the original code: Data Format Each category documents the returned data format. loaders and writers reference the category documentation. Writers and Categories Writers for a format that belongs to a category accept data objects with fields described by the documentation for that category. Accessing Format-Specific Data Sometimes, not all the properties provided by a certain file format can be mapped to common properties defined by the corresponding loader category. To access format-specific properties, use the  loaderData  field in data object returned by the loader. Available Categories Categories are described in the specifications section. Some currently defined categories are: Table PointCloud/Mesh Scenegraph GIS","headings":[{"value":"Loader Categories","depth":1},{"value":"Categories and Loader Registration","depth":2},{"value":"Data Format","depth":2},{"value":"Writers and Categories","depth":2},{"value":"Accessing Format-Specific Data","depth":2},{"value":"Available Categories","depth":2}],"slug":"docs/developer-guide/loader-categories","title":"Loader Categories"},{"excerpt":"Error Handling Applications typically want to provide solid error handling when loading and saving data. Ideally the applications wants to use a simple clean API for the loading, and yet have the confidence that errors are caught and meaningful messages are presented to the user. Types of Errors There are three main types of errors that arise when attempting to load a data resource: There is some kind of network/resource access error, preventing the request for data from being issued A request is sent to a server, but the server is unable to service the request due to some error condition (often illegal access tokens or request parameters) and sends an error response. The server returns data, but the parser is unable to parse it (perhaps due to the data being malformatted, or formatted according to an unsupported version of that format). loaders.gl can detect all of these error conditions and report the resulting errors in a unified way (the errors will be available as exceptions or rejected promises depending on your async programming style, see below). Error Messages loaders.gl aims to produce concise, easy-to-understand error messages that can be presented directly to the end user. When the fetch call fails, the genereted exception is passed to the user, and the same is true when a loader fails. For server error responses, some basic information about the error is compiled into an error message (using e.g.  response.status ,  response.url  and occasionally  response.text ). Note that while servers often send some information about errors in  response.text()  when setting HTTP error codes, there are no universally adhered-to conventions for how servers format those error messages. The data is often a set of key-value pairs that are JSON or XML encoded, but even then the exact key names are usually server-specific. At the moment loaders.gl does not provide any error formatting plugins, so if you know how your specific service formats errors and want to extract these in a way that you can present to the user, you may want to take control of the fetch  Response  status checking, see below. parse Error Handling parse  accepts fetch  Response  objects, and  parse  will check the status of the  Response  before attempting to parse, and generate an exception if appropriate. Handling Errors from Async Functions Note that  parse  is an async function, and in JavaScript, errors generated by async functions will be reported either as an exception or as a rejected promise, depending on how the async funtion was called (using promises or the  await  keyword): When using  await , errors are reported as exceptions A rejected promise is generated when using  Promise.then . Also note that the Javascript runtime seamlessly converts errors between exceptions and promises in mixed code. fetch Error Handling loaders.gl is designed around the use of the modern JavaScript  fetch  API, so for additional context, it may help to review of how the JavaSctipt  fetch  function handles errors. fetch  separates between \"network errors\" that can be detected directly (these cause the  fetch  to throw an exception) and server side errors that are reported asynchronously with HTTP status codes (in this case the  Response  object offers accessors that must be called to check if the operation was successful before accessing data). Example: \"manually\" checking separately for fetch network errors and server errors: Note that servers often sends a message providing some detail about what went wrong, and that message can be accessed using the standard (asynchronous)  response.text()  or  response.json()  methods.","headings":[{"value":"Error Handling","depth":1},{"value":"Types of Errors","depth":2},{"value":"Error Messages","depth":3},{"value":"parse Error Handling","depth":2},{"value":"Handling Errors from Async Functions","depth":2},{"value":"fetch Error Handling","depth":2}],"slug":"docs/developer-guide/error-handling","title":"Error Handling"},{"excerpt":"Using Loaders loaders.gl has parser functions that use so called \"loaders\" to convert the raw data loaded from files into parsed objects. Each loader encapsulates a parsing function for one file format (or a group of related file formats) together with some metadata (like the loader name, common file extensions for the format etc). Installing loaders loaders.gl provides a suite of pre-built loader objects packaged as scoped npm modules. The intention is that applications will install and import loaders only for the formats they need. Using Loaders Loaders are passed into utility functions in the loaders.gl core API to enable parsing of the chosen format. Specifying and Registering Loaders As seen above can be specified directly in a call to  load  or any of the  parse  functions: Loaders can also be registered globally. To register a loader, use  registerLoaders : Selecting Loaders The loader selection algorithm is exposed to applications via  selectLoader : Note: Selection works on urls and/or data Loader Options load ,  parse  and other core functions accept loader options in the form of an options object. Such loader options objects are organized into nested sub objects, with one sub-object per loader or loader category. This provides a structured way to pass options to multiple loaders. An advantage of this design is that since the core functions can select a loader from a list of multiple candidate loaders, or invoke sub-loaders, the nested options system allows separate specification of options to each loader in a single options object. Loader options are merged with default options using a deep, two-level merge. Any object-valued key on the top level will be merged with the corresponding key value in the default options object. Using Composite Loaders loaders.gl enables the creation of  composite loaders  that call other loaders (referred to as \"sub-loaders\" in this section). This enables loaders for \"composite formats\" to be quickly composed out of loaders for the primitive parts. Composite Loader usage is designed to be conceptually simple for applications (loaders.gl handles a number of subtleties under the hood). A composite loader is called just like any other loader, however there are some additional Parameter Passing between Loaders Loaders and parameters are passed through to sub loaders and are merged so that applications can override them: In this example: the passed in loaders would override any loaders specified inside the sub-loaders as well as any globally registered loaders. The options will be passed through to the sub-loaders, so that the  GLTFLoader  will receive the  gltf  options, merged with any  gltf  options set by the  Tiles3DLoader . This override system makes it easy for applications to test alternate sub-loaders or parameter options without having to modify any existing loader code.","headings":[{"value":"Using Loaders","depth":1},{"value":"Installing loaders","depth":2},{"value":"Using Loaders","depth":2},{"value":"Specifying and Registering Loaders","depth":2},{"value":"Selecting Loaders","depth":2},{"value":"Loader Options","depth":2},{"value":"Using Composite Loaders","depth":2},{"value":"Parameter Passing between Loaders","depth":3}],"slug":"docs/developer-guide/using-loaders","title":"Using Loaders"},{"excerpt":"Polyfills Older browsers (mainly Edge and IE11) as well as Node.js do not provide certain APIs ( TextEncoder ,  fetch  etc) that loaders.gl depends on. The good news is that these APIs can be provided by the application using the  polyfill  technique. While there are many good polyfill modules for these classes available on  npm , to make the search for a version that is guaranteed to work with loaders.gl a little easier, the  @loaders.gl/polyfills  module is provided. To install these polyfills, just  import  the polyfills module before start using loaders.gl. Combining with other Polyfills loaders.gl only installs polyfills if the corresponding global symbol is  undefined . This means that if another polyfill is already installed when  @loaders.gl/polyfills  is imported, the other polyfill will remain in effect. Since most polyfill libraries work this way, applications can mix and match polyfills by ordering the polyfill import statements appropriately (but see the remarks below for a possible caveat). Provided Polyfills See  API Reference . Remarks Applications should typically only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, they will still be included in your application bundle, i.e. importing the polyfill module will increase your application's bundle size. When importing polyfills for the same symbol from different libraries, the import can depend on how the other polyfill is written. to control the order of installation, you may want to use  require  rather than  import  when importing  @loaders.gl/polyfills . As a general rule,  import  statements execute before  require  statments.","headings":[{"value":"Polyfills","depth":1},{"value":"Combining with other Polyfills","depth":2},{"value":"Provided Polyfills","depth":2},{"value":"Remarks","depth":2}],"slug":"docs/developer-guide/polyfills","title":"Polyfills"},{"excerpt":"Using Writers Writers and the  encode  functions are available for use, however they are considere experimental. They rae still in development, and may still have issues. Writers allow applications to generate properly formatted data for a number of the formats supported by loaders.gl. Not all formats have writers. For a detailed specification of the writer object format see the  API reference . Usage As an example, to Draco-compress a mesh using the  DracoWriter : Input Data Writers  accept the same format of data that is produced by the corresponding loaders. This format is documented either in each loader or usually as part of the documentation for that loader category. If applications have data in a different format, they will need to first transform the data to the format expected by the  writer .","headings":[{"value":"Using Writers","depth":1},{"value":"Usage","depth":2},{"value":"Input Data","depth":2}],"slug":"docs/developer-guide/using-writers","title":"Using Writers"},{"excerpt":"Using Batched Loaders A major feature of loaders.gl is the availability of a number of batched (or streaming) loaders. The advantages and characteristics of streaming are descriped in more detail in the  streaming  concepts section, but the highlights are: Ability to parse large data sources that exceed browser memory limits (maximum allocation limits for a single  string  or  ArrayBuffer  etc tends to be less tha 1GB in most browsers). While parsing is done on smaller chunks and does not freeze the main thread. data can be processed (and displayed) as it arrives over the network, rather than at the end of a long request, leading to a more interactive experience. transforms can be applied incrementally to the incoming data, e.g. to cryptographically hash or decrypt data Batches: Async Iterator based Streaming The loaders.gl streaming architecture is built around ES2018 async iterators rather than the more traditional  Stream s. Async iterators are arguably easier to work with than streams, are consistent across browsers and Node.js, and enable a \"callback-less\" programming style supported by built-in JavaScript language features, such as  for await (... of ...)  and  async function * . Note:  Stream  input sources is still accepted by loaders.gl functions, however internally processing is done via async iterators and the output of a batched parsing operation is an async iterator that yields \"batches\" of parsed data. The JSONLoader supports streaming JSON parsing, in which case it will yield \"batches\" of rows from the first array it encounters in the JSON. To e.g. parse a stream of GeoJSON: Streaming Data Sources While the primary input for  parseInBatches  is an async iterator many input types are supported: AsyncIterable<ArrayBuffer>  (i.e. the iterator must yield  ArrayBuffer  chunks). Stream  instances can be used as input to  parseInBatches . An async iterator will automatically be created from the stream. Response  objects can also be used as input (the  Response.body  stream will be used). In addition, note that applications can easily wrap many data types in a  Response  object (e.g.  FormData ,  Blob ,  File ,  string ,  ArrayBuffer  etc), which makes it possible to do streaming loads from almost any data source. Applying Transforms Example of using a transform to calculate a cryptographic hash: Note that by using a transform, the hash is calculated incrementally as batches are loaded and parsed, and does not require having the entire data source loaded into memory. It also distributes the potentially heavy hash calculation over the batches, keeping the main thread responsive.","headings":[{"value":"Using Batched Loaders","depth":1},{"value":"Batches: Async Iterator based Streaming","depth":2},{"value":"Streaming Data Sources","depth":2},{"value":"Applying Transforms","depth":2}],"slug":"docs/developer-guide/using-streaming-loaders","title":"Using Batched Loaders"},{"excerpt":"Using Workers Most loaders.gl loaders can perform parsing on JavaScript worker threads.\nThis means that the main thread will not block during parsing and can continue\nto respond to user interactions or do parallel processing. Worker threads can also run in parallel, increasing your application's performance\nwhen loading parsing many files in parallel. Note that worker thread loading is not always the best choice since the transfer of\ndata between workers and the main thread is only efficient if the data is predominantly\nbinary. When worker thread loading is not offered in a specific loader it is usually\nbecause it would not provide any performance benefits. Another advantage when using pure worker loaders is that the code required to\nparse a format is not bundled into the application but loaded on demand. This is\nparticularly useful when adding loaders that are only used occasionally by your\napplication. More details on advantages and complications with worker thread based loading the  Worker Threads  article in the concepts secion. Processing Data on Workers The  processOnWorker  function in  @loaders.gl/worker-utils  is used with worker objects\nexported by modules like  @loaders.gl/compression  and  @loaders.gl/crypto  to move\nprocessing intensive tasks to workers. Parsing data on Workers Loading Files in Parallel using Worker Loaders The  DracoLoader  is an example of a worker enabled loader. It parses data on worker threads by default. To load two Draco encoded meshes  in parallel  on worker threads, just use the  DracoLoader  as follows: Disabling Worker Loaders Applications can use the  worker: false  option to disable worker loaders, for instance to simplify debugging of parsing issues: Disabling Reuse of Workers Applications reuse already created workers by default. To avoid  enlarge memory arrays  error it is really nesessary to disable it if you need to load multiple datasets in a sequence.\nThis functionality can be disabled by  reuseWorkers: false  option: Concurrency Level and Worker Reuse Concurrency - The  options.maxConcurrency  and  option.maxMobileConcurrency  options can be adjusted to define how many worker instances should be created for each format. Note that setting this higher than roughly the number CPU cores on your current machine will not provide much benefit and may create extra overhead. Worker reuse - Workers threads can occupy memoery and ArrayBuffer Neutering Be aware that when calling worker loaders, binary data is transferred from the calling thread to the worker thread. This means that if you are using  parse , any  ArrayBuffer  parameter you pass in to the will be \"neutered\" and no longer be accessible in the calling thread. Most applications will not need to do further processing on the raw binary data after it has been parsed so this is rarely an issue, but if you do, you may need to copy the data before parsing, or disable worker loading (see above). Specifying Worker Script URLs (Advanced) In JavaScript, worker threads are loaded from separate scripts files and are typically not part of the main application bundle. For ease-of-use, loaders.gl provides a default set of pre-built worker threads which are published on loaders.gl npm distribution from  unpck.com  CDN (Content Delivery Network). As an advanced option, it is possible to for application to specify alternate URLs for loading a pre-built worker loader instance. This can be useful e.g. when building applications that cannot access CDNs or when creating highly customized application builds, or doing in-depth debugging. Composite Loaders and Workers (Advanced) loaders.gl supports sub-loader invocation from worker loaders. This is somewhat experimental A worker loader starts a seperate thread with a javascript bundle that only contains the code for that loader, so a worker loader needs to call the main thread (and indirectly, potentially another worker thread with another worrker loader) to parse using a sub-loader, properly transferring data into and back from the other thread. Debugging Worker Loaders (Advanced) Debugging worker loaders is tricky. While it is always possible to specify  options.worker: false  which helps in many situations, there are cases where the worker loader itself must be debugged. TBA - There is an ambition to provide better support for debugging worker loaders: Pre-build non-minified versions of workers, and provide option to easily select those. Let loaders.gl developers easily switch between CDN and locally built workers. ...","headings":[{"value":"Using Workers","depth":2},{"value":"Processing Data on Workers","depth":2},{"value":"Parsing data on Workers","depth":2},{"value":"Loading Files in Parallel using Worker Loaders","depth":2},{"value":"Disabling Worker Loaders","depth":2},{"value":"Disabling Reuse of Workers","depth":2},{"value":"Concurrency Level and Worker Reuse","depth":2},{"value":"ArrayBuffer Neutering","depth":2},{"value":"Specifying Worker Script URLs (Advanced)","depth":2},{"value":"Composite Loaders and Workers (Advanced)","depth":2},{"value":"Debugging Worker Loaders (Advanced)","depth":2}],"slug":"docs/developer-guide/using-worker-loaders","title":"Using Workers"},{"excerpt":"Worker Threads On modern browsers, many loaders.gl loaders are set up to run on JavaScript worker threads. (Refer the documentation of each loader to see if it supports worker thread loading). Loading and parsing of data on worker threads can bring significant advantages Avoid blocking the browser main thread  - when parsing longer files, the main thread can become blocked, effectively \"freezing\" the application's user interface until parsing completes. Parallel parsing on multi-core CPUs  - when parsing multiple files on machines that have multiple cores (essentially all machines, even modern mobile phones tend to have at least two cores), worker threads enables multiple files to be parsed in parallel which can dramatically reduce the total load times. Hoever, there are a number of considerations when loading and parsing data on JavaScript worker threads: Serialization/deserializion overhead  when transferring resuls back to main thread can more than defeat gains from loading on a separate thread. Choice of Data Types  - Due to data transfer issues there are constraints on what data types are appropriate Build configuration  - Workers can require complex build system setup/configuration. Message Passing  - Parsing on workers requires message passing between threads. While simple it can add clutter to application code. Debugging  - Worker based code tends to be harder to debug. Being able to easily switch back to main thread parsing (or an alternate worker build) can be very helpful. Startup Times  - Worker startup times can defeat speed gains from parsing on workers. Data Transfer Threads cannot share non-binary data structures and these have to be serialized/deserialized. This is a big issue for worker thread based loading as the purpose of loaders is typically to load and parse big datastructures, and main thread deserialization times are often comparable to or even exceed the time required to parse the data in the first place, defeating the value of moving parsing to a worker thread. The solution is usually to use data types that support ownership transfer (see next section) as much as possible and minimize the amount of non-binary data returned from the parser. Data Types JavaScript ArrayBuffers and Typed Arrays can be passed with minimal overhead (ownership transfer) and the value of worker based parsing usually depends on whether the loaded data can (mostly) be stored in these types. Message Passing loaders.gl will handle message passing behind the scenes. Loading on a worker thread returns a promise that completes when the worker is done and the data has been transferred back to the main thread. Build Configuration All worker enabled loaders come with a pre-built, minimal worker \"executable\" to enable zero-configuration use in applications. Bundle size concerns All worker enabled loaders provide separate loader objects to ensure that tree-shaking bundlers will be able to remove the code for the unused case. Debugging and Benchmarking Loaders.gl offers loader objects for main thread and worker threads. A simple switch lets you move your loading back to the main thread for easier debugging and benchmarking (comparing speeds to ensure you are gaining the benefits you expect from worker thread based loading).","headings":[{"value":"Worker Threads","depth":1},{"value":"Data Transfer","depth":2},{"value":"Data Types","depth":2},{"value":"Message Passing","depth":2},{"value":"Build Configuration","depth":2},{"value":"Bundle size concerns","depth":2},{"value":"Debugging and Benchmarking","depth":2}],"slug":"docs/developer-guide/concepts/worker-threads","title":"Worker Threads"},{"excerpt":"Binary Data The loaders.gl API consistently uses  ArrayBuffer s to represent and transport binary data. Why ArrayBuffers? One of the design goals of loaders.gl is to provide applications with a single, consistent API that works across (reasonably modern) browsers, worker threads and Node.js. One of the characteristics of this API is how binary data is represented. loaders.gl \"standardizes\" on ArrayBuffers for a number of reasons: ArrayBuffers are the \"canonical\" input format for the WebGL API, allowing efficient uploads of large binary data sets to the GPU. ArrayBuffers allow ownership to be transferred between threads (Browser Main Thread and WebWorkers), massively improving performance when sending data back from loaders running on web worker to the application/main thread. ArrayBuffers are used to transport raw data in most newer JavaScript APIs, including WebSockets, Web Intents, XMLHttpRequest version 2 etc. ArrayBuffers are well supported by recent Node.js versions, in fact the traditional Node.js  Buffer  class is now backed by an  ArrayBuffer . ArrayBuffers and Typed Arrays Recall that typed arrays (e.g.  Float32Array ) are just views into array buffers. Every typed array has a  buffer  reference. Many loaders.gl functions directly accept typed arrays, which essentially means they accept the associated ArrayBuffer. However, be aware that typed arrays can represent partial views (i.e. they can have offsets) that sometimes need special handling in the application. Converting between ArrayBuffers and Strings We use the  TextEncoder  and  TextDecoder  classes in the JavaScript  string encoding/decoding library . Since these classes are central to using ArrayBuffers correctly, loaders.gl provides polyfills for them under Node.js. Binary Types in JavaScript Binary data types in JS: ArrayBuffer Uint8Array  and other typed arrays, plus DataView Blob Buffer  nodejs Examples of \"semi-binary\" data types in JS: Array : Array of bytes (elements are numbers between 0 and 255). String  (binary): string in binary form, 1 byte per char (2 bytes). String  (base64): string containing the binary data encoded in a base64 form. Converting between ArrayBuffers and other Binary Formats. Standardizing on ArrayBuffers helps streamline the loaders.gl API. But occasionally applications need to interface with APIs that accept other binary data types/formats. To support this case, loaders.gl provides a small set of utilities (non-exhaustive) for converting from and to other binary JavaScript types/formats, e.g.  toArrayBuffer :","headings":[{"value":"Binary Data","depth":1},{"value":"Why ArrayBuffers?","depth":2},{"value":"ArrayBuffers and Typed Arrays","depth":2},{"value":"Converting between ArrayBuffers and Strings","depth":2},{"value":"Binary Types in JavaScript","depth":2},{"value":"Converting between ArrayBuffers and other Binary Formats.","depth":2}],"slug":"docs/developer-guide/concepts/binary-data","title":"Binary Data"},{"excerpt":"AsyncIterators Streaming functionality in loaders.gl is built on the ES2018  AsyncIterator  concept. This page gives some background on AsyncIterator since it is a recently introduced concept (at least as part of the JavaScript standard). Availability AsyncIterator  is a standard JavaScript ES2018 feature and is well supported by recent evergreen browsers and Node.js versions. The  for await of  iteration syntax is supported as well as the babel transpiler. Batched Parsing and Endcoding using AsyncIterators The input and output from streaming loaders and writers can both be expressed in terms of async iterators. Using AsyncIterator Remember tyhat an async iterator can be consumed (iterated over) via the for-await construct: Using Streams as AsyncIterators With a little effort, streams in JavaScript can be treated as AsyncIterators. As the section about  Javascript Streams  explains, instead of registering callbacks on the stream, you can now work with streams in this way: Creating AsyncIterators Remember that any object in JavaScript that implements the  [Symbol.asyncIterator]()  method is an  AsyncIterable . And the async generator syntax can be used to generate new async iterators","headings":[{"value":"AsyncIterators","depth":1},{"value":"Availability","depth":2},{"value":"Batched Parsing and Endcoding using AsyncIterators","depth":2},{"value":"Using AsyncIterator","depth":2},{"value":"Using Streams as AsyncIterators","depth":2},{"value":"Creating AsyncIterators","depth":2}],"slug":"docs/developer-guide/concepts/async-iterators","title":"AsyncIterators"},{"excerpt":"Streaming Streaming support in loaders.gl is a work-in-progress. The ambition is that many loaders would support streaming from both Node and DOM streams, through a consistent API and set of conventions (for both applications and loader/writer objects). Streaming Loads Incremental Parsing Some loaders offer incremental parsing (chunks of incomplete data can be parsed, and updates will be sent after a certain batch size has been exceeded). In many cases, parsing is fast compared to loading of data, so incremental parsing on its own may not provide a lot of value for applications. Incremental Loading Incremental parsing becomes more interesting when it can be powered by incremental loading, whether through request updates or streams (see below). Streamed Loading Streamed loading means that the entire data does not need to be loaded. This is particularly advantageous when: loading files with sizes that exceed browser limits (e.g. 1GB in Chrome) doing local processing to files (tranforming one row at a time), this allows pipe constructions that can process files that far exceed internal memory. Batched Updates For incemental loading and parsing to be really effective, the application needs to be able to deal efficiently with partial batches as they arrive. Each loader category (or loader) may define a batch update conventions that are appropriate for the format being loaded. Streaming Writes TBA Node Streams vs DOM Streams Stream support is finally arriving in browsers, however DOM Streams have a slightly different API than Node streams and the support across browsers is still spotty. Polyfills Stream support across browsers can be somewhat improved with polyfills. TBA Stream Utilities Stream to memory, ... Automatically create stream if loader/writer only supports streaming ...","headings":[{"value":"Streaming","depth":1},{"value":"Streaming Loads","depth":2},{"value":"Incremental Parsing","depth":3},{"value":"Incremental Loading","depth":3},{"value":"Streamed Loading","depth":3},{"value":"Batched Updates","depth":2},{"value":"Streaming Writes","depth":2},{"value":"Node Streams vs DOM Streams","depth":2},{"value":"Polyfills","depth":2},{"value":"Stream Utilities","depth":2}],"slug":"docs/developer-guide/concepts/streaming","title":"Streaming"},{"excerpt":"Overview The  @loaders.gl/zip  module handles compressing and decompressing of the  ZIP  and  TAR  format. Installation Attributions ZipLoader is a wrapper around the  JSZip module . JSZip has extensive documentation on options (and more functionality than this loader object can expose). TarBuilder uses a modified version of  tar-js , which is under MIT license, for tar archive construction.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Attributions","depth":2}],"slug":"modules/zip/docs","title":"Overview"},{"excerpt":"@loaders.gl/wkt This module contains a geometry loader for the Well-Known Text (WKT) and Well-Known Binary (WKB) formats. loaders.gl  is a collection of framework-independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/wkt","depth":1}],"slug":"modules/wkt","title":"@loaders.gl/wkt"},{"excerpt":"ZipLoader Decodes a Zip Archive into a file map. Loader Characteristic File Extension .zip File Type Binary File Format ZIP Archive Data Format \"File Map\" Decoder Type Asynchronous Worker Thread No Streaming No Usage Data Format The file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either  ArrayBuffer  or  String ). Options Options are forwarded to  JSZip.loadAsync .","headings":[{"value":"ZipLoader","depth":1},{"value":"Usage","depth":2},{"value":"Data Format","depth":2},{"value":"Options","depth":2}],"slug":"modules/zip/docs/api-reference/zip-loader","title":"ZipLoader"},{"excerpt":"@loaders.gl/worker-utils This module contains shared utilities for loaders.gl, a collection of framework-independent 3D and geospatial loaders (parsers). For documentation please visit the  website .","headings":[{"value":"@loaders.gl/worker-utils","depth":1}],"slug":"modules/worker-utils","title":"@loaders.gl/worker-utils"},{"excerpt":"ZipWriter Encodes a filemap into a Zip Archive. Returns an  ArrayBuffer  that is a valid Zip Archive and can be written to file. Loader Characteristic File Extension .zip File Type Binary Data Format \"File Map\" File Format ZIP Archive Encoder Type Asynchronous Worker Thread No Streaming No Usage File Format The file map is an object with keys representing file names or relative paths in the zip file, and values being the contents of each sub file (either  ArrayBuffer  or  String ). Options Options are forwarded to  JSZip.generateAsync , however type is always set to  arraybuffer  to ensure compatibility with writer driver functions in  @loaders.gl/core .","headings":[{"value":"ZipWriter","depth":1},{"value":"Usage","depth":2},{"value":"File Format","depth":2},{"value":"Options","depth":2}],"slug":"modules/zip/docs/api-reference/zip-writer","title":"ZipWriter"},{"excerpt":"@loaders.gl/video loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loader and writers for video that follow loaders.gl conventions and work under both node and browser. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/video","depth":1}],"slug":"modules/video","title":"@loaders.gl/video"},{"excerpt":"Overview The  @loaders.gl/wkt  module handles the  Well Known Text (WKT)  format, an ASCII format that defines geospatial geometries; and the  Well Known Binary  format, WKT's binary equivalent. Installation Loaders and Writers Loader WKBLoader WKTLoader WKTWriter Attribution The  WKTLoader  is based on a fork of the Mapbox  wellknown  module under the ISC license (MIT/BSD 2-clause equivalent).","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/wkt/docs","title":"Overview"},{"excerpt":"WKTLoader Loader and writer for the  Well-known text  format for representation of geometry. Loader Characteristic File Extension .wkt , File Type Text File Format Well Known Text Data Format Geometry Supported APIs load ,  parse ,  parseSync Decoder Type Synchronous Worker Thread Support Yes  Usage Options N/A Attribution The  WKTLoader  is based on a fork of the Mapbox  wellknown  module under the ISC license (MIT/BSD 2-clause equivalent).","headings":[{"value":"WKTLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/wkt/docs/api-reference/wkt-loader","title":"WKTLoader"},{"excerpt":"WKBLoader Loader for the  Well-known binary  format for representation of geometry. Loader Characteristic File Extension .wkb , File Type Binary File Format Well Known Binary Data Format Geometry Supported APIs load ,  parse ,  parseSync Decoder Type Synchronous Worker Thread Support Yes Usage Options N/A Format Summary Well-known binary (WKB) is a binary geometry encoding to store geometries (it\ndoesn't store attributes). It's used in databases such as PostGIS and as the\ninternal storage format of Shapefiles. It's also being discussed as the internal\nstorage format for a  \"GeoArrow\" \nspecification. WKB is defined starting on page 62 of the  OGC Simple Features\nspecification . It's essentially a binary representation of WKT. For common geospatial types\nincluding (Multi)  Point ,  Line , and  Polygon , there's a 1:1 correspondence\nbetween WKT/WKB and GeoJSON. WKT and WKB also support extended geometry types,\nsuch as  Curve ,  Surface , and  TIN , which don't have a correspondence to\nGeoJSON. Coordinates can be 2-4 dimensions and are interleaved. Positions stored as double precision","headings":[{"value":"WKBLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Format Summary","depth":2}],"slug":"modules/wkt/docs/api-reference/wkb-loader","title":"WKBLoader"},{"excerpt":"WKTWriter Writer for the  Well-known text  format for representation of geometry. Loader Characteristic File Extension .wkt , File Type Text File Format Well Known Text Data Format Geometry Supported APIs encode ,  encodeSync Usage Options N/A Attribution The  WKTWriter  is based on a fork of the Mapbox  wellknown  module under the ISC license (MIT/BSD 2-clause equivalent).","headings":[{"value":"WKTWriter","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/wkt/docs/api-reference/wkt-writer","title":"WKTWriter"},{"excerpt":"Overview The  @loaders.gl/video  module contains loader and writers for images that follow loaders.gl conventions. Video support is still experimental, and does not work in Node.js. Installation API Loader Description VideoLoader","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"API","depth":2}],"slug":"modules/video/docs","title":"Overview"},{"excerpt":"GIFBuilder This  GIFBuilder  is currenly highly experimental and may change significantly in minor releases and ev en patch releases. Pin down your loaders.gl version if you wish to use it. The  GIFBuilder  class creates a base64 encoded GIF image from either: a series of images a series of image URLs a video URL or by capturing the webcam. The  GIFBuilder  only works in the browser, and many features are experimental. Usage Build a GIF from images Build a GIF from image URLs (Experimental) Build a GIF from image URLs, with frame-specific Text (Experimental) Build a GIF from the webcam (Experimental) Methods constructor(options: object) Creates a new  GIFBuilder  instance. options  See the Options section below. add(file: Image | string | object) images  --  Image  objects can be added.(Note:  ImageBitmap  is not currently supported). Experimentally, tha following types can currently be added (may be removed in upcoming release) string URLs for images  If this option is used, then a GIF will be created using these images e.g. ,.' http://i.imgur.com/2OO33vX.jpg' , ' http://i.imgur.com/qOwVaSN.png' , ' http://i.imgur.com/Vo5mFZJ.gif' a video  a GIF will be created using the first supplied video that is supported by the current browser's video codecs. E.g. 'example.mp4', 'example.ogv'. Also note that a mix of types is not supported. All added elements must be of the same type (images, image URLs, video URLs). build(): string The build method will actually build the GIF. It returns a base 64 encoded GIF. Note: After calling  build()  this builder instance is not intended to be used further. Create new  GLTBuilder  instances to build additional GIFs. Options Option Type Default Description source string 'images' Either  'images' ,  'video'  or  'webcam' width number 200 Desired width of the generated GIF image height number 200 Desired height of the generated GIF image crossOrigin string CORS attribute for requesting image or video URLs. 'Anonymous' 'Anonymous', 'use-credentials', or '' (to not set). QUALITY SETTINGS sampleInterval 10 pixels to skip when creating the palette. Default is 10. Less is better, but slower. numWorkers 2 how many web workers to use to process the animated GIF frames. Default is 2. interval 0.1 The amount of time (in seconds) to wait between each frame capture offset null The amount of time (in seconds) to start capturing the GIF (only for HTML5 videos) numFrames 10 The number of frames to use to create the animated GIF. Each frame is captured every 100 milliseconds of a video and every ms for existing images frameDuration 1 The amount of time (10 = 1s) to stay on each frame Notes: By adjusting the sample interval, you can either produce extremely high-quality images slowly, or produce good images in reasonable times. With a sampleInterval of 1, the entire image is used in the learning phase, while with an interval of 10, a pseudo-random subset of 1/10 of the pixels are used in the learning phase. A sampling factor of 10 gives a substantial speed-up, with a small quality penalty. Experimental Options These options are forwarded directly to the underlying  gifshot  module. They are not officially supported by loaders.gl, but can still be useful. In case things are unclear it is recommended to search the documentation and issues in that module. Option Type Default Description when the current image is completed CSS FILTER OPTIONS filter `'', // CSS filter that will be applied to the image (eg. blur(5px)) WATERMARK OPTIONS waterMark null If an image is given here, it will be stamped on top of the GIF frames waterMarkHeight null ,// Height of the waterMark waterMarkWidth null Height of the waterMark waterMarkXCoordinate 1 The X (horizontal) Coordinate of the watermark image waterMarkYCoordinate 1 The Y (vertical) Coordinate of the watermark image | TEXT OPTIONS\n|  text  |  '', // The text that covers the animated GIF | | showFrameText | true | If frame-specific text is supplied with the image array, you can force the |frame-specific text to not be displayed by making this option 'false'.\n|  fontWeight  |  'normal' | The font weight of the text that covers the animated GIF | | fontSize | '16px' | The font size of the text that covers the animated GIF |\n|  minFontSize  |  '10px' | The minimum font size of the text that covers the animated GIF (Note  |  This |option is only applied if the text being applied is cut off) | resizeFont | false | Whether or not the animated GIF text will be resized to fit within the GIF |container\n|  fontFamily  |  'sans-serif' | The font family of the text that covers the animated GIF | | fontColor | '#ffffff' | The font color of the text that covers the animated GIF |\n|  textAlign  |  'center' | The horizontal text alignment of the text that covers the animated GIF | | textBaseline | 'bottom' | The vertical text alignment of the text that covers the animated GIF |\n|  textXCoordinate  |  null | The X (horizontal) Coordinate of the text that covers the animated GIF (only |use this if the default textAlign and textBaseline options don't work for you) | textYCoordinate | null | The Y (vertical) Coordinate of the text that covers the animated GIF (only |use this if the default textAlign and textBaseline options don't work for you) Remarks Make sure these image resources are CORS enabled to prevent any cross-origin JavaScript errors You may also pass a NodeList of existing image elements on the page Attribution GIFBuilder  is based on Yahoo's awesome  gifshot  module, and is MIT licensed.","headings":[{"value":"GIFBuilder","depth":1},{"value":"Usage","depth":2},{"value":"Methods","depth":2},{"value":"constructor(options: object)","depth":3},{"value":"add(file: Image | string | object)","depth":3},{"value":"build(): string","depth":3},{"value":"Options","depth":2},{"value":"Experimental Options","depth":3},{"value":"Remarks","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/video/docs/api-reference/gif-builder","title":"GIFBuilder"},{"excerpt":"@loaders.gl/tiles (Experimental) This module contains the common components for tiles loaders, i.e.  3D tiles . loaders.gl  is a collection of loaders for big data visualizations. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/tiles (Experimental)","depth":1}],"slug":"modules/tiles","title":"@loaders.gl/tiles (Experimental)"},{"excerpt":"Overview @loaders/tiles  exposes handy classes  Tileset3D  and  Tile3D  which can understand the loaded data from tile loaders ( @loaders.gl/3d-tiles ,  @loaders.gl/i3s , etc.), and provide useful functions for dynamically selecting tiles for rendering under a viewport. Concepts OGC 3D Tiles  standard OGC i3s  standard Tile Header Hierarchy  - An initial, \"minimal\" set of data listing the  hierarchy of available tiles , with minimal information to allow an application to determine which tiles need to be loaded based on a certain viewing position in 3d space. Tile Header  - A minimal header describing a tiles bounding volume and a screen space error tolerance (allowing the tile to be culled if it is distant), as well as the URL to load the tile's actual content from. Tile Content  - The actual payload of the tile. Tile Cache  - Since the number of tiles in big tilesets often exceed what can be loaded into available memory, it is important to have a system that releases no-longer visible tiles from memory. Tileset Traversal  - Dynamically loading and rendering 3D tiles based on current viewing position, possibly triggering loads of new tiles and unloading of older, no-longer visible tiles. Tileset3D Class Properties boundingVolume  ( BoundingVolume ): The root tile's bounding volume, which is also the bouding volume of the entire tileset. Check  Tile3DHeader#boundingVolume cartesianCenter  ( Number[3] ): Center of tileset in fixed frame coordinates. cartographicCenter  ( Number[3] ): Center of tileset in cartographic coordinates  [long, lat, elevation] ellipsoid  ( Ellipsoid ): Gets an ellipsoid describing the shape of the globe. maximumMemoryUsage  ( Number ): If tiles sized more than  maximumMemoryUsage  are needed to for the current view, when these tiles go out of view, they will be unloaded. maximumMemoryUsage  must be greater than or equal to zero. modelMatrix  (`Matrix4: A  Matrix4  instance (4x4 transformation matrix) that transforms the entire tileset. root  ( Tile3DHeader ): The root tile header. tiles : ( Array<Tile3DHeader> ): All the tiles that have been traversed. stats  ( Stats )): An instance of a probe.gl  Stats  object that contains information on how many tiles have been loaded etc. Easy to display using a probe.gl  StatsWidget . tileset  ( Object ): The original tileset data this object instanced from. tilesLoaded  ( Boolean ): When  true , all tiles that meet the screen space error this frame are loaded. The tileset is completely loaded for this view. gpuMemoryUsageInBytes  ( Number ): The total amount of GPU memory in bytes used by the tileset. This value is estimated from geometry, texture, and batch table textures of loaded tiles. For point clouds, this value also includes per-point metadata. url  ( String ): The url to a tileset JSON file. zoom  ( Number[3] ): A web mercator zoom level that displays the entire tile set bounding volume Methods constructor(tileset : Object, url : String [, options : Object]) tileset : The loaded tileset (parsed JSON). See  Tileset Object Format . options : Options object, but not limited to\nParameters: modelMatrix = Matrix4.IDENTITY  ( Matrix4 ) - A 4x4 transformation matrix that transforms the tileset's root tile. maximumMemoryUsage = 512 ] ( Number ) - The maximum amount of memory in MB that can be used by the tileset. ellipsoid = Ellipsoid.WGS84  ( Ellipsoid ) - The ellipsoid determining the size and shape of the globe.\nCallbacks: onTileLoad  ( (tileHeader : Tile3DHeader) : void ) - callback when a tile node is fully loaded during the tileset traversal. onTileUnload  ( (tileHeader : Tile3DHeader) : void ) - callback when a tile node is unloaded during the tileset traversal. onTileError  ( (tileHeader : Tile3DHeader, message : String) : void ) - callback when a tile faile to load during the tileset traversal. update(viewport: WebMercatorViewport) : Number : Execute traversal under current viewport and fetch tiles needed for current viewport and update  selectedTiles . Return  frameNumber  of this update frame. destroy() : void : Destroys the WebGL resources held by this object, and destroy all the tiles' resources by recursively traversing the tileset tree. Tile3D Class Properties boundingVolume  ( BoundingVolume ): A bounding volume that encloses a tile or its content. Exactly one box, region, or sphere property is required. ( Reference ) id  ( Number | String ): A unique number for the tile in the tileset. Default to the url of the tile. contentState  ( String ): Indicate of the tile content state. Available options UNLOADED : Has never been requested or has been destroyed. LOADING : Is waiting on a pending request. PROCESSING : Contents are being processed for rendering. Depending on the content, it might make its own requests for external data. READY : All the resources are loaded and decoded. FAILED : Request failed. contentType  ( String ): One of empty : does not have any content to render render : has content to render tileset : tileset tile depth  ( Number ): The depth of the tile in the tileset tree. content  ( Object ): The tile's content.This represents the actual tile's payload. type  ( String ): One of  scenegraph ,  pointcloud ,  mesh parent  ( Tile3DHeader ): Parent of this tile. refine  ( String ): Specifies the type of refine that is used when traversing this tile for rendering.  Reference ADD : high-resolution children tiles should be rendered in addition to lower-resolution parent tiles when level of details of parent tiles are not sufficient for current view. REPLACEMENT : high-resolution children tiles should replace parent tiles when lower-resolution parent tiles are not sufficient for current view. selected  ( Boolean ): Whether this tile is selected for rendering in current update frame and viewport. A selected tile should has its content loaded and satifies current viewport. tileset  ( Tileset3D ): The  Tileset3D  instance containing this tile. header  ( Object ): The unprocessed tile header object passed in. Methods constructor(tileset : Object, header : Object, parentHeader : Object) tileset : The loaded tileset (parsed JSON) header : The loaded tile header file. See  Tile Object Format . parentHeader : The loaded parent file. destroy() : Destroy the tile node, including destroy all the metadata and unload content. loadContent() : Load a content of the tile. unloadContent() : Unload a content of the tile. Data Format This section specifies the unified data formats from tileset loader and tile loader. Tileset Object The following fields are guaranteed. But different tileset loaders may have different extra fields. Field Type Contents root Object The root tile header object url Object The root tile header object type String Indicate the type of tileset specification,  3d-tiles ,  i3s ,  potree , etc. Tile Object The following fields are guaranteed. But different tile loaders may have different extra fields. Field Type Contents boundingVolume Object A bounding volume that encloses a tile or its content. Exactly one box, region, or sphere property is required. ( Reference ) children Array An array of objects that define child tiles. Each child tile content is fully enclosed by its parent tile's bounding volume and, generally, has more details than parent. for leaf tiles, the length of this array is zero, and children may not be defined. content String The actual payload of the tile or the url point to the actual payload. id String Identifier of the tile, unique in a tileset lodSelection Object Used for deciding if this tile is sufficient given current viewport. Cesium tile use  geometricError ,  i3s  uses  metricType  and  maxError refine String Refinement type of the tile,  ADD  or  REPLACE type String Type of the tile, one of  pointcloud ,  scenegraph ,  mesh transformMatrix Number[16] A matrix that transforms from the tile's local coordinate system to the parent tile's coordinate systemor the tileset's coordinate system in the case of the root tile Tile Content After content is loaded, the following fields are guaranteed. But different tiles may have different extra content fields. Field Type Contents cartesianOrigin Number[3] \"Center\" of tile geometry in WGS84 fixed frame coordinates cartographicOrigin Number[3] \"Origin\" in lng/lat (center of tile's bounding volume) modelMatrix Number[16] Transforms tile geometry positions to fixed frame coordinates attributes Object Each attribute follows luma.gl  accessor  properties attributes  contains following fields Field Type Contents attributes.positions Object {value, type, size, normalized} attributes.normals Object {value, type, size, normalized} attributes.colors Object {value, type, size, normalized} PointCloud Fields Field Type Contents pointCount Number Number of points color Number[3]  or  Number[4] Color of the tile when there are not  attributes.colors Scenegraph Fields Field Type Contents gltf Object check  GLTFLoader  for detailed spec SimpleMesh Fields Field Type Contents texture URL url of tile's texture Additional Information Coordinate Systems To help applications process the  position  data in the tiles, 3D Tiles category loaders are expected to provide matrices are provided to enable tiles to be used in both fixed frame or cartographic (long/lat-relative, east-north-up / ENU) coordinate systems: cartesian  WGS84 fixed frame coordinates cartographic  tile geometry positions to ENU meter offsets from  cartographicOrigin . Position units in both cases are in meters. For cartographic coordinates, tiles come with a prechosen cartographic origin and precalculated model matrix. This cartographic origin is \"arbitrary\" (chosen based on the tiles bounding volume center). A different origin can be chosen and a transform can be calculated, e.g. using the math.gl  Ellipsoid  class.","headings":[{"value":"Overview","depth":1},{"value":"Concepts","depth":2},{"value":"Tileset3D Class","depth":3},{"value":"Properties","depth":4},{"value":"Methods","depth":4},{"value":"Tile3D Class","depth":3},{"value":"Properties","depth":4},{"value":"Methods","depth":4},{"value":"Data Format","depth":2},{"value":"Tileset Object","depth":3},{"value":"Tile Object","depth":3},{"value":"Tile Content","depth":3},{"value":"Additional Information","depth":2},{"value":"Coordinate Systems","depth":3}],"slug":"modules/tiles/docs","title":"Overview"},{"excerpt":"VideoLoader The  VideoLoader  is experimental. A basic Video element loader. Only works in the browser. Loader Characteristic File Extension .mp4 File Type Binary File Format Image Data Format Video  (browsers) (Not currently supported on node.js) Supported APIs load ,  parse Usage Options Option Type Default Description","headings":[{"value":"VideoLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/video/docs/api-reference/video-loader","title":"VideoLoader"},{"excerpt":"Tile3D The  Tile3D  class is used internally by  loaders.gl/tiles   Tileset3D  class to manage loading/unloading tiles. Constructor Paremeters: tileset  (Tileset3D) -  Tileset3D  instance which contains this tile header  (Tile3D) -  Tile3D  instance parentHeader  (Tile3D) -  Tile3D  instance of parent tile Properties boundingVolume  (BoundingVolume) A bounding volume that encloses a tile or its content. Exactly one box, region, or sphere property is required. ( Reference ) id  (Number | String) A unique number for the tile in the tileset. Default to the url of the tile. contentState  (String) Indicate of the tile content state. Available options UNLOADED : Has never been requested or has been destroyed. LOADING : Is waiting on a pending request. PROCESSING : Contents are being processed for rendering. Depending on the content, it might make its own requests for external data. READY : All the resources are loaded and decoded. FAILED : Request failed. contentType  (String) One of empty : does not have any content to render render : has content to render tileset : tileset tile depth  (Number) The depth of the tile in the tileset tree. content  (Object) The tile's content.This represents the actual tile's payload. type  (String) One of  scenegraph ,  pointcloud ,  mesh parent  (Tile3DHeader) Parent of this tile. refine  (String) Specifies the type of refine that is used when traversing this tile for rendering.  Reference ADD : high-resolution children tiles should be rendered in addition to lower-resolution parent tiles when level of details of parent tiles are not sufficient for current view. REPLACEMENT : high-resolution children tiles should replace parent tiles when lower-resolution parent tiles are not sufficient for current view. selected  (Boolean) Whether this tile is selected for rendering in current update frame and viewport. A selected tile should has its content loaded and satifies current viewport. tileset  (Tileset3D) The  Tileset3D  instance containing this tile. header  (Object) The unprocessed tile header object passed in. Methods destroy() Destroy the tile node, including destroy all the metadata and unload content. loadContent() Load a content of the tile. unloadContent() Unload a content of the tile.","headings":[{"value":"Tile3D","depth":1},{"value":"Constructor","depth":2},{"value":"Properties","depth":4},{"value":"boundingVolume (BoundingVolume)","depth":6},{"value":"id (Number|String)","depth":6},{"value":"contentState (String)","depth":6},{"value":"contentType (String)","depth":6},{"value":"depth (Number)","depth":6},{"value":"content (Object)","depth":6},{"value":"type (String)","depth":6},{"value":"parent (Tile3DHeader)","depth":6},{"value":"refine (String)","depth":6},{"value":"selected (Boolean)","depth":6},{"value":"tileset (Tileset3D)","depth":6},{"value":"header (Object)","depth":6},{"value":"Methods","depth":4},{"value":"destroy()","depth":5},{"value":"loadContent()","depth":5},{"value":"unloadContent()","depth":5}],"slug":"modules/tiles/docs/api-reference/tile-3d","title":"Tile3D"},{"excerpt":"Tileset3D The  Tileset3D  class is being generalized to handle more use cases. Since this may require modifying some APIs, this class should be considered experiemental. The  Tileset3D  class can be instantiated with tileset data formatted according to the  3D Tiles Category , which is supported by the  Tiles3DLoader . References 3D Tiles . I3S Tiles . Usage Loading a tileset and instantiating a  Tileset3D  instance. Loading a tileset and dynamically load/unload with viewport. Since  Tileset3D's update  is a synchronized call, which selects the tiles qualified for rendering based on current viewport and available tiles, user can trigger another  update  when new tiles are loaded. Constructor Parameters: json : loaded tileset json object, should follow the format  tiles format options : options.ellipsoid = Ellipsoid.WGS84  ( Ellipsoid ) - The ellipsoid determining the size and shape of the globe. options.throttleRequests = true  ( Boolean ) - Determines whether or not to throttle tile fetching requests. options.modelMatrix = Matrix4.IDENTITY  ( Matrix4 ) - A 4x4 transformation matrix this transforms the entire tileset. options.maximumMemoryUsage = 512 ] ( Number ) - The maximum amount of memory in MB that can be used by the tileset. options.fetchOptions  - fetchOptions, i.e. headers, used to load tiles from tiling server Callbacks: onTileLoad  ( (tileHeader : Tile3DHeader) : void ) - callback when a tile node is fully loaded during the tileset traversal. onTileUnload  ( (tileHeader : Tile3DHeader) : void ) - callback when a tile node is unloaded during the tileset traversal. onTileError  ( (tileHeader : Tile3DHeader, message : String) : void ) - callback when a tile faile to load during the tileset traversal. The  Tileset3D  allows callbacks ( onTileLoad ,  onTileUnload ) to be registered that notify the app when the set of tiles available for rendering has changed. This is important because tile loads complete asynchronously, after the  tileset3D.update(...)  call has returned. Cesium 3D tiles specific options: options.maximumScreenSpaceError = 16 ] ( Number ) - The maximum screen space error used to drive level of detail refinement. Properties boundingVolume  (BoundingVolume) The root tile's bounding volume, which is also the bouding volume of the entire tileset. Check  Tile3DHeader#boundingVolume cartesianCenter  (Number 3 ) Center of tileset in fixed frame coordinates. cartographicCenter  (Number 3 ) Center of tileset in cartographic coordinates  [long, lat, elevation] ellipsoid  ( Ellipsoid ) Gets an ellipsoid describing the shape of the globe. modelMatrix  (Matrix4) A  Matrix4  instance (4x4 transformation matrix) that transforms the entire tileset. root  (Tile3DHeader) The root tile header. tiles  (Tile3DHeader[]) All the tiles that have been traversed. stats  ( Stats ) An instance of a probe.gl  Stats  object that contains information on how many tiles have been loaded etc. Easy to display using a probe.gl  StatsWidget . tileset  (Object) The original tileset data this object instanced from. tilesLoaded  (Boolean) When  true , all tiles that meet the screen space error this frame are loaded. The tileset is completely loaded for this view. gpuMemoryUsageInBytes  (Number) The total amount of GPU memory in bytes used by the tileset. This value is estimated from geometry, texture, and batch table textures of loaded tiles. For point clouds, this value also includes per-point metadata. url  (String) The url to a tileset JSON file. zoom  (Number 3 ) A web mercator zoom level that displays the entire tile set bounding volume tilesLoaded  : boolean When  true , all tiles that meet the screen space error this frame are loaded. The tileset is\ncompletely loaded for this view. See Tileset3D#allTilesLoaded Cesium 3D Tiles properties asset : Object Gets the tileset's asset object property, which contains metadata about the tileset. See the  asset schema reference  in the 3D Tiles spec for the full set of properties. properties : Object Gets the tileset's properties dictionary object, which contains metadata about per-feature properties. See the  properties schema reference  in the 3D Tiles spec for the full set of properties. maximumScreenSpaceError : Number The maximum screen space error used to drive level of detail refinement. This value helps determine when a tile refines to its descendants, and therefore plays a major role in balancing performance with visual quality. A tile's screen space error is roughly equivalent to the number of pixels wide that would be drawn if a sphere with a\nradius equal to the tile's  geometric error  were rendered at the tile's position. If this value exceeds\n maximumScreenSpaceError  the tile refines to its descendants. Depending on the tileset,  maximumScreenSpaceError  may need to be tweaked to achieve the right balance. Higher values provide better performance but lower visual quality.  * maximumMemoryUsage : Number ^default 16  * \n^exception  maximumScreenSpaceError  must be greater than or equal to zero. The maximum amount of GPU memory (in MB) that may be used to cache tiles. This value is estimated from\ngeometry, textures, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata. Tiles not in view are unloaded to enforce this. If decreasing this value results in unloading tiles, the tiles are unloaded the next frame. If tiles sized more than  maximumMemoryUsage  are needed\nto meet the desired screen space error, determined by  Tileset3D.maximumScreenSpaceError ,\nfor the current view, then the memory usage of the tiles loaded will exceed\n maximumMemoryUsage . For example, if the maximum is 256 MB, but\n300 MB of tiles are needed to meet the screen space error, then 300 MB of tiles may be loaded. When\nthese tiles go out of view, they will be unloaded. ^default 512  * \n^exception  maximumMemoryUsage  must be greater than or equal to zero.\n^see Tileset3D#gpuMemoryUsageInBytes root : Tile3DHeader The root tile header. boundingSphere : BoundingSphere The tileset's bounding sphere. modelMatrix : Matrix4 A 4x4 transformation matrix that transforms the entire tileset. maximumMemoryUsage : Number gpuMemoryUsageInBytes : Number The total amount of GPU memory in bytes used by the tileset. This value is estimated from\ngeometry, texture, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata. stats : Stats An instance of a probe.gl  Stats  object that contains information on how many tiles have been loaded etc. Easy to display using a probe.gl  StatsWidget . ellipsoid : Ellipsoid Gets an ellipsoid describing the shape of the globe. Returns the  extras  property at the top-level of the tileset JSON, which contains application specific metadata.\nReturns  undefined  if  extras  does not exist. Exception The tileset is not loaded. Use Tileset3D.readyPromise or wait for Tileset3D.ready to be true. See  Extras  in the 3D Tiles specification.} unloadTileset Unloads all tiles that weren't selected the previous frame. This can be used to\nexplicitly manage the tile cache and reduce the total number of tiles loaded below\n Tileset3D.maximumMemoryUsage . Tile unloads occur at the next frame to keep all the WebGL delete calls\nwithin the render loop. isDestroyed() : Boolean Returns true if this object was destroyed; otherwise, false. If this object was destroyed, it should not be used; calling any function other than\n isDestroyed  will result in an exception. ^returns  Boolean :  true  if this object was destroyed; otherwise,  false . destroy() Destroys the WebGL resources held by this object. Destroying an object allows for deterministic\nrelease of WebGL resources, instead of relying on the garbage collector to destroy this object. Once an object is destroyed, it should not be used; calling any function other than  isDestroyed  will result in an exception. Therefore, assign the return value  undefined  to the object as done in the example. Wxception This object was destroyed, i.e., destroy() was called. Methods update update(viewport: WebMercatorViewport) : Number : Parameters: viewport : a  WebMercatorViewport Execute traversal under current viewport and fetch tiles needed for current viewport and update  selectedTiles . Return  frameNumber  of this update frame.","headings":[{"value":"Tileset3D","depth":1},{"value":"Usage","depth":2},{"value":"Constructor","depth":2},{"value":"Properties","depth":2},{"value":"boundingVolume (BoundingVolume)","depth":6},{"value":"cartesianCenter (Number3)","depth":6},{"value":"cartographicCenter (Number3)","depth":6},{"value":"ellipsoid (Ellipsoid)","depth":6},{"value":"modelMatrix (Matrix4)","depth":5},{"value":"root (Tile3DHeader)","depth":6},{"value":"tiles (Tile3DHeader[])","depth":6},{"value":"stats (Stats)","depth":6},{"value":"tileset (Object)","depth":6},{"value":"tilesLoaded (Boolean)","depth":6},{"value":"gpuMemoryUsageInBytes (Number)","depth":6},{"value":"url (String)","depth":6},{"value":"zoom (Number3)","depth":6},{"value":"tilesLoaded : boolean","depth":5},{"value":"Cesium 3D Tiles properties","depth":3},{"value":"asset : Object","depth":3},{"value":"properties : Object","depth":3},{"value":"maximumScreenSpaceError : Number","depth":3},{"value":"maximumMemoryUsage : Number","depth":3},{"value":"root : Tile3DHeader","depth":3},{"value":"boundingSphere : BoundingSphere","depth":3},{"value":"modelMatrix : Matrix4","depth":3},{"value":"maximumMemoryUsage : Number","depth":3},{"value":"gpuMemoryUsageInBytes : Number","depth":3},{"value":"stats : Stats","depth":3},{"value":"ellipsoid : Ellipsoid","depth":3},{"value":"unloadTileset","depth":3},{"value":"isDestroyed() : Boolean","depth":3},{"value":"destroy()","depth":3},{"value":"Methods","depth":2},{"value":"update","depth":5}],"slug":"modules/tiles/docs/api-reference/tileset-3d","title":"Tileset3D"},{"excerpt":"@loaders.gl/zip loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loaders and writers for the Zip Archive format. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/zip","depth":1}],"slug":"modules/zip","title":"@loaders.gl/zip"},{"excerpt":"@loaders.gl/tile-converter loaders.gl  is a collection of framework independent 3D and geospatial parsers and encoders. This module contains command line scripts and JavaScript APIs for converting between formats, for instance betwen 3D Tiles and I3S tilesets. For documentation please visit the  website . Installation","headings":[{"value":"@loaders.gl/tile-converter","depth":1},{"value":"Installation","depth":2}],"slug":"modules/tile-converter","title":"@loaders.gl/tile-converter"},{"excerpt":"Tiles3DConverter class The  Tiles3DConverter  class converts an I3S layer. It converts between the OGC I3S formats and the 3D Tiles. Usage The converted tiles are written to the specified output path. Methods constructor() Constructs a new  Tiles3DConverter  instance. convert(options: object): object Converts a tileset to 3DTiles format options.inputUrl  the url to read the tileset from. Required options.outputPath  the output path options.tilesetName  the output name of the tileset options.egmFilePath  location of  * .pgm file to convert heights from ellipsoidal to gravity-related format options.maxDepth  The max tree depth of conversion","headings":[{"value":"Tiles3DConverter class","depth":1},{"value":"Usage","depth":2},{"value":"Methods","depth":2},{"value":"constructor()","depth":3},{"value":"convert(options: object): object","depth":3}],"slug":"modules/tile-converter/docs/api-reference/3d-tiles-converter","title":"Tiles3DConverter class"},{"excerpt":"Build Instructions Following steps are for running converter right on a repository branch. It might be helpful if latest changes from any repository branch are needed. E.g. if latest  master  branch has some important updates in  tile-converter  module or other module that  tile converter  module depends on, a user can run it. It doesn't need to wait for new release. Using  yarn  instead of npm is recommended because it is general practice in vis.gl repos; Clone the repository Build modules Install default Earth Gravity Model dependency: You can use custom Earth Gravity Model using  --egm  option. Convert some tileset \nExamples: Notice \"--max-depth\" option. It means that the converter will load and convert only first 'n' (10 in example) levels of tiles. Use it for big tilesets when full conversion could take a lot of time. If you want to convert all the tileset, omit this option. I3S layers can be used only as http service. There is local server to handle i3s layer After conversion there are new i3s layers in output (default: \"data\") directory. Run it with the local web server: Show converted layer on a map. Advanced A. To show converted layer in a locally built loaders.gl example. Run the front-end application from examples B. To run a custom layer in a web-browser manually","headings":[{"value":"Build Instructions","depth":1},{"value":"Advanced","depth":3}],"slug":"modules/tile-converter/docs/api-reference/build-instructions","title":"Build Instructions"},{"excerpt":"@loaders.gl/textures loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loaders for compressed textures and  basis universal textures . For documentation please visit the  website .","headings":[{"value":"@loaders.gl/textures","depth":1}],"slug":"modules/textures","title":"@loaders.gl/textures"},{"excerpt":"Tile converter The  tile-converter  is a command line utility (CLI) for two-way batch conversion between the OGC 3D Tiles and the I3S formats. It can load the tileset to be converted directly from an URL. Installation Installing  @loaders.gl/tile-converter  makes the  converter  command line tool available. It can be run using  npx . Options Option 3DTiles to I3S conversion I3S to 3DTiles conversion Description install-dependencies Run the script for installing dependencies. Run this options separate from others. Now \" * .pgm\" file installation is implemented input-type * * \"I3S\" - for I3S to 3DTiles conversion, \"3DTILES\" for 3DTiles to I3S conversion tileset * * \"tileset.json\" file (3DTiles) / \"http://..../SceneServer/layers/0\" resource (I3S) output * * Output folder. This folder will be created by converter if doesn't exist. It is relative to the converter path. Default: \"data\" folder name * * Tileset name. This option is used for naming in resulting json resouces and for resulting path/ * .slpk file naming max-depth * * Maximal depth of the hierarchical tiles tree traversal, default: infinite slpk * Whether the converter generate  * .slpk (Scene Layer Packages) I3S output file 7zExe * location of 7z.exe archiver to create slpk on Windows OS, default: \"C: \\ Program Files \\ 7-Zip \\ 7z.exe\" egm * * location of the Earth Gravity Model ( * .pgm) file to convert heights from ellipsoidal to gravity-related format, default: \"./deps/egm2008-5.pgm\". A model file can be loaded from GeographicLib  https://geographiclib.sourceforge.io/html/geoid.html token * Token for Cesium ION tilesets authentication no-draco * Disable draco compression for geometry. Default: not set help * * Show the converter tool options list Running local server to handle i3s layer. After conversion there are new i3s layers in output (\"data\" in example) directory. Run it with the local web server from project directory: Show converted layer on a map. open  https://loaders.gl/examples/i3s?url=http://localhost/SceneServer/layers/0","headings":[{"value":"Tile converter","depth":2},{"value":"Installation","depth":2},{"value":"Options","depth":2},{"value":"Running local server to handle i3s layer.","depth":2},{"value":"Show converted layer on a map.","depth":2}],"slug":"modules/tile-converter/docs/cli-reference/tile-converter","title":"Tile converter"},{"excerpt":"Overview Installation For CLI For API Command Line Utilities tile-converter  - the npx tool for launch conversion API A JavaScript API is also available: I3SConverter  class that converts 3DTiles to I3S Tiles3DConverter  class that converts I3S to 3DTiles Note: the command line tools are implemented using this API and offer the same functions. References The  @loaders.gl/i3s  module supports loading and traversing Indexed 3d Scene Layer (I3S). The  @loaders.gl/3d-tiles  module supports loading and traversing 3D Tiles. I3S Indexed Scene Layer Specification  - The living specification. 3D Tiles Specification  - The living specification. OGC I3S Indexed Scene Layer Standard  - The official standard from  OGC , the Open Geospatial Consortium. OGC 3D Tiles Standard  - The official standard from  OGC , the Open Geospatial Consortium. Additional build instructions There are additional ways to perform conversion: Tile converter can be run right on a repository branch. It might be helpful if some updates are needed which last release doesn't contain  Build instructions . An autonomous bundle script can be built. It is entire converter in just 1 file. This file can be destributed directly to interested but not experienced user.  See instructions . Attribution The tile-converter module represents a major development effort and was funded and contributed to loaders.gl by Esri. MIT License.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Command Line Utilities","depth":2},{"value":"API","depth":2},{"value":"References","depth":2},{"value":"Additional build instructions","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/tile-converter/docs","title":"Overview"},{"excerpt":"Tile converter bundle script The converter can be run with autonomous script. It might be helpful for users which are not experienced in npm/yarn tools. All what they have to have to run conversion is a built script (converter.min.js) and NodeJS. If making  * .slpk is necessary, a zip archiver is \"must have\" as well.\nThe \"bundle script\" is good options for fast and easy destribution of the converter. Create bundle: This command generates bundle into \"modules/cli/dist/converter.min.js\" Take \"converter.min.js\". It can be run on Ubuntu and Windows as autonomous script; Install default Earth Gravity Model (egm2008-5): You can use custom Earth Gravity Model using  --egm  option. Check out cli options:  node converter.min.js --help Example: Requirements: NodeJs ; External archiver (for slpk mode): Ubuntu:  apt install zip Windows:  7-Zip . Default 7-zip location is \"C:\\Program Files\\7-Zip\\7z.exe\" but there is option \"--7zExe\" that can be used for setup \"7z.exe\" location manualy.","headings":[{"value":"Tile converter bundle script","depth":2}],"slug":"modules/tile-converter/docs/api-reference/tile-converter-bundle","title":"Tile converter bundle script"},{"excerpt":"I3SConverter class The  I3SConverter  class converts a 3D Tiles tileset to I3S layer. Usage Methods constructor() Constructs a new  I3SConverter  instance. convert(options: object): object Converts a tileset to I3S format options.inputUrl  the url to read the tileset from options.outputPath  the output filename options.tilesetName  the output name of the tileset options.maxDepth  The max tree depth of conversion options.slpk  Whether the resulting layer be saved as \" * .slpk\" package options.sevenZipExe  Windows only. The path of 7-zip archiver tool for creating \" * .slpk\" file options.egmFilePath  location of  * .pgm file to convert heights from ellipsoidal to gravity-related format. A model file can be loaded from GeographicLib  https://geographiclib.sourceforge.io/html/geoid.html options.token  ION token of input tileset options.draco  Whether the converter create DRACO compressed geometry in path \"layers/0/nodes/xxx/geometries/1\" along with non-compressed geometry in path \"layers/0/nodes/xxx/geometries/0\" options.validateBoundingVolumes  Enable/Disable Bounding Volumes Validation (Check if child bounding volume inside parent bounding volume)","headings":[{"value":"I3SConverter class","depth":1},{"value":"Usage","depth":2},{"value":"Methods","depth":2},{"value":"constructor()","depth":3},{"value":"convert(options: object): object","depth":3}],"slug":"modules/tile-converter/docs/api-reference/i3s-converter","title":"I3SConverter class"},{"excerpt":"CompressedTextureLoader Loader for compressed textures in the PVR file format Loader Characteristic File Format PVR ,  DDS File Extension .dds ,  .pvr File Type Binary Data Format Array of compressed image data objects Supported APIs load ,  parse Usage Data Format Returns an array of image data objects representing mip levels. {compressed: true, format, width, height, data: ..., levelSize} Options Option Type Default Description N/A","headings":[{"value":"CompressedTextureLoader","depth":1},{"value":"Usage","depth":2},{"value":"Data Format","depth":2},{"value":"Options","depth":2}],"slug":"modules/textures/docs/api-reference/compressed-texture-loader","title":"CompressedTextureLoader"},{"excerpt":"CompressedTextureWriter The experimental  CompressedTextureWriter  class can encode a binary encoded image into a compressed texture. Loader Characteristic File Extension File Type Binary Data Format File Format Encoder Type Asynchronous Worker Thread No (but may run on separate native thread in browsers) Streaming No Usage Data Format TBA Options Option Type Default Description Remarks For more information, see  texture-compressor .","headings":[{"value":"CompressedTextureWriter","depth":1},{"value":"Usage","depth":2},{"value":"Data Format","depth":2},{"value":"Options","depth":2},{"value":"Remarks","depth":2}],"slug":"modules/textures/docs/api-reference/compressed-texture-writer","title":"CompressedTextureWriter"},{"excerpt":"Overview The  @loaders.gl/textures  module contains loaders for compressed textures. More specifically it contains loaders and writers for compressed texture  container  formats, including KTX, DDS and PVR. It also supports supercompressed Basis textures. Note that a texture is more complex than an image. A texture typically has many subimages. A texture can represent a single logical image but can also be a texture cube, a texture array etc representing many logical images. In addition, each \"image\" typically has many mipmap levels. In addition, in compressed textures each mipmap image is compressed opaquely into a format that can only be understood by certain GPUs. Basis encoded textures are super compressed. A more recent addition, they can be efficiently transcoded on the client into actual compressed texture formats appropriate for each device and are therefore quite convenient to use. Installation API Loader Description BasisLoader Compressed Texture API A set of functions that can extract information from \"unparsed\" binary memory representation of certain compressed texture image formats. These functions are intended to be called on raw  ArrayBuffer  data, before the  BasisLoader  parses it and converts it to a parsed image type. TBA Function Description Return Types The  BasisLoader  returns Array of Array of ArrayBuffer See  BasisLoader  for more details on options etc. Attributions The  CompressedTextureLoader  was forked from  PicoGL , Copyright (c) 2017 Tarek Sherif, The MIT License (MIT) The  CompressedTextureWriter  is a wrapper around @TimvanScherpenzeel's  texture-compressor  utility (MIT licensed).","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"API","depth":2},{"value":"Compressed Texture API","depth":3},{"value":"Return Types","depth":2},{"value":"Attributions","depth":2}],"slug":"modules/textures/docs","title":"Overview"},{"excerpt":"BasisLoader A loader for Basis Universal \"supercompressed\" GPU textures. Extracts supercompressed textures from the basis container and efficiently \"transpiles\" them into the specified compressed texture format. Loader Characteristic File Format Basis Universal File Extension .basis File Type Binary Data Format Array of compressed image data objects Supported APIs load ,  parse Usage Options Option Type Default Description basis.format String 'auto' Set to one of the supported compressed texture formats. Compressed Texture Formats The  BasisLoader  can transpile into the following compressed (and uncompressed) texture formats. Format Description etc1 etc2 bc1 bc3 bc4 bc5 bc7-m6-opaque-only bc7-m5 pvrtc1-4-rgb pvrtc1-4-rgba astc-4x4 atc-rgb atc-rgba-interpolated-alpha rgba32 rgb565 bgr565 rgba4444","headings":[{"value":"BasisLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Compressed Texture Formats","depth":2}],"slug":"modules/textures/docs/api-reference/basis-loader","title":"BasisLoader"},{"excerpt":"Worker Loader for compressed textures in the Crunch file format Loader Characteristic File Format CRN File Extension .crn File Type Binary Data Format Array of compressed image data objects Supported APIs load ,  parse Usage Data Format Returns an array of image data objects representing mip levels. {compressed: true, format, width, height, data: ..., levelSize} Options Option Type Default Description N/A","headings":[{"value":"Worker","depth":1},{"value":"Usage","depth":2},{"value":"Data Format","depth":2},{"value":"Options","depth":2}],"slug":"modules/textures/docs/api-reference/crunch-loader","title":"Worker"},{"excerpt":"NPYLoader The  NPYLoader  parses an array from the  NPY format , a lightweight encoding of multidimensional arrays used by the Python NumPy library. Loader Characteristic File Extension .npy File Type Binary File Format Array Data Format Array Supported APIs load ,  parse ,  parseSync Decoder Type Synchronous Worker Thread Support Yes Streaming Support No Usage data  is a TypedArray containing the array's data. header  is an object with three keys: descr : a string describing the data type. E.g.  |u1  refers to  uint8  and  <u2  refers to little-endian  uint16 . Full details are available in the  NumPy documentation . fortran_order : a boolean that is  true  if the array is stored in Fortran order instead of C order. shape : an array of integers that describes the shape of the array. The length of the array corresponds to the number of dimensions of the array. Options Currently no options are supported for this loader. Option Type Default Description","headings":[{"value":"NPYLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/textures/docs/api-reference/npy-loader","title":"NPYLoader"},{"excerpt":"About Compressed Textures Compressed textures  are different from compressed images in that they do not have to be decompressed, they can be used directly by a supporting GPU. However, a compressed texture typically consists of a collection of compressed subimages, representing mipmaps etc. These compressed subimages are stored as an array of \"binary blobs\" in a container file. Only the container file is parsed, extracting metadata and the binary buffers representing subimages. The binary subimages can then be passed directly to a GPU that understands how to read pixels directly from them without decompressing them first. Supercompressed textures  are an intermediate format whose subimages are compressed in a common format. This format can be cheaply transcoded on to a real compressed texture format supported on the current client, without decompressing and recompressing the texture. This allows a single supercompressed texture to be portably used on multiple platforms even though those platforms do not support the same compressed texture formats. Performance Considerations Advantages: Compressed textures can allow a lot more textures (~4x) to be stored in the same amount of GPU memory, which can make a big difference, decreasing memory bandwidth use, or allowing more detail, also mobile devices tend to crash when memory fills up. Compressed textures do not need to be decoded before use which reduces CPU load, noticable when many textures are loaded. Compressed textures include mipmaps, further reducing CPU load by avoiding mipmap generation step, noticable when many textures are loaded. On the downside: Compressed textures can be somewhat bigger and slower than JPEGs to load over the network. Actual number should be verified but as an example, compressed texture formats might achieve about 4-6x compression, compared to say 15x compression for JPEG. Compression tends to be relatively slow. In combination with some IP issues this usually makes it impractical to create GPU compressed textures on the fly. Since different devices have different GPUs that support different compressed texture formats, one typically has to provide compressed textures in multiple formats and decide which ones to load at runtime (although basis avoids this problem). Container Formats Texture Containers  This section is based on the information in Dave Evan's helpful Texture Containers article, please refer to it for additional details. Non-texture image formats do not support storing mipmap chains. When loading a JPG or a PNG, mipmaps must be generated by resizing the original image repeatedly for each required mipmap level. In contrast, a single texture container can store all the data required for an entire texture, mipmaps, array layers or cubemap faces. Generating mipmaps offline is important if you use compressed textures, as its generally impractical to generate compressed textures at runtime. The main container formats for compressed textures are the Khronos Texture format (KTX) and Microsoft's DirectDraw Surface (DDS). KTX, being a standard, is better specified and therefore recommended. KTX (Khronos Texture) The KTX format is a Khronos Group standard for storing textures. It can store 1D, 2D, 3D, Cubemaps and Array Textures, along with any number of mipmaps for these textures. This makes it ideal for storing almost any kind of texture you could want. The fields in the KTX header are directly compatible with other Khronos standards such as WebGL. The texture data is described in the  glType ,  glFormat ,  glInternalFormat , and  glBaseInternalFormat  header fields. These should match up with the parameters to the  gl[Compressed]Tex[Sub]Image*  calls used to submit each texture mipmap levels data. Khronos KTX Specification Khronos KTX Software DDS (DirectDraw Surface) The DDS format is in common use for storing textures (despite DirectDraw being long deprecated). Originally only 2D textures were supported, but the D3D10 header extension added support for texture arrays and D3D10+ features. The format is partially documented on MSDN. MSDN: Programming Guide for DDS PVR (PowerVR) The PVR texture compression format defines its own container http://cdn.imgtec.com/sdk-documentation/PVR+File+Format.Specification.pdf Compression Formats As mentioned the actual compressed subimages are not parsed or modified by loaders.gl, however loaders.gl attempts to identify the formats using metadata and return the appropiate format fields to facilitate use in WebGL and WebGPU. The following is the typical list of compressed texture formats, which loaders.gl can properly tag: Format Description `S3TC S3 texture compression formats `S3TC_SRGB S3 SRGB texture compression formats `PVRTC PowerVR texture compression formats `ETC1 texture compression formats `ETC texture compression formats ASTC texture compression formats `ATC AMD texture compression formats Recommnended Formats The following could be a starting point for choosing texture formats Desktop: BC3 ( DXT5 ) - transparent textures with full alpha range BC1 ( DXT1 ) - opaque textures iOS: PVR4  - transparent textures with alpha PVR2  - opaque textures Android: ASTC_4x4 ,  ASTC8x8  - transparent textures with full alpha range ETC1  - opaque textures Using Compressed Textures Compressed textures are designed to be directly uploaded to GPUs that have the required decoding support implemented in hardware. Using compressed textures in JS loaders.gl currently does not provide CPU-side decoding capabilities for compressed textures, meaning that they can only be uploaded directly to supporting GPUs. Use a WebGL context and read back the rendered texture to the client. Using Compressed Textures in luma.gl While loaders.gl itself is framework-independent, luma.gl (and other vis.gl frameworks like deck.gl) are designed to seamless consume data loaded by loaders.gl. Data returned by any loaders.gl \"image\" category loader (including texture loaders) can be passed directly to luma.gl  Texture2D  class. Using Compressed Textures in raw WebGL To use compressed textures in WebGL WebGL Extensions Used to query if the GPU supports specific proprietary compressed texture formats. Extension Enables [ WEBGL_compressed_texture_s3tc ( https://www.khronos.org/registry/webgl/extensions/WEBGL_compressed_texture_s3tc ) S3 texture compression formats [ WEBGL_compressed_texture_s3tc_srgb ( https://www.khronos.org/registry/webgl/extensions/WEBGL_compressed_texture_s3tc_srgb ) S3 SRGB texture compression formats [ WEBGL_compressed_texture_atc ( https://www.khronos.org/registry/webgl/extensions/WEBGL_compressed_texture_atc ) AMD texture compression formats [ WEBGL_compressed_texture_pvrtc ( https://www.khronos.org/registry/webgl/extensions/WEBGL_compressed_texture_pvrtc ) PowerVR texture compression formats [ WEBGL_compressed_texture_etc1 ( https://www.khronos.org/registry/webgl/extensions/WEBGL_compressed_texture_etc1 ) texture compression formats [ WEBGL_compressed_texture_etc ( https://www.khronos.org/registry/webgl/extensions/WEBGL_compressed_texture_etc ) texture compression formats [ WEBGL_compressed_texture_astc ( https://www.khronos.org/registry/webgl/extensions/WEBGL_compressed_texture_astc ) texture compression formats Using Compressed Textures in WebGPU Support for compressed textures is a work in progress in the  WebGPU standard . At the time of writing, only S3 texture compression has been specified: Creating Compressed Textures Texture compression code is usually not readily available, particulary not in JavaScript. Compression is typically done by binary programs, e.g.  PVRTexTool . The loaders.gl  CompressedTextureWriter  can compress textures (under Node.js only) by executing a binary with the appropriate command line, and then loading back the output. IP and Patent Considerations An issue with compressed texture formats is that they tend to be highly propietary and patent-encumbered, and while it is usually no longer an issue, there can be cases where e.g. royalty requirements come into play when using them. To side-step patent issues when using these formats an application would typically: Generate compressed textures in external applications (which should already have licensed any required formats and libraries). Load them in binary form without touching the content. Pass them directly to a texture, so that they are processed inside the GPU driver (which should also habe licensed the supported formats and libraries).","headings":[{"value":"About Compressed Textures","depth":1},{"value":"Performance Considerations","depth":2},{"value":"Container Formats","depth":2},{"value":"KTX (Khronos Texture)","depth":3},{"value":"DDS (DirectDraw Surface)","depth":3},{"value":"PVR (PowerVR)","depth":3},{"value":"Compression Formats","depth":2},{"value":"Recommnended Formats","depth":3},{"value":"Using Compressed Textures","depth":2},{"value":"Using compressed textures in JS","depth":3},{"value":"Using Compressed Textures in luma.gl","depth":3},{"value":"Using Compressed Textures in raw WebGL","depth":3},{"value":"WebGL Extensions","depth":3},{"value":"Using Compressed Textures in WebGPU","depth":3},{"value":"Creating Compressed Textures","depth":2},{"value":"IP and Patent Considerations","depth":2}],"slug":"modules/textures/docs/using-compressed-textures","title":"About Compressed Textures"},{"excerpt":"@loaders.gl/terrain loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module reconstructs mesh surfaces from height map images, e.g.  Mapzen Terrain Tiles , which encodes elevation into R,G,B values. For documentation please visit the  website . @loaders.gl/terrain  uses  MARTINI  for mesh reconstruction. ISC License Copyright (c) 2019, Mapbox Permission to use, copy, modify, and/or distribute this software for any purpose\nwith or without fee is hereby granted, provided that the above copyright notice\nand this permission notice appear in all copies. THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH\nREGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND\nFITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, DIRECT,\nINDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS\nOF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER\nTORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF\nTHIS SOFTWARE.","headings":[{"value":"@loaders.gl/terrain","depth":1}],"slug":"modules/terrain","title":"@loaders.gl/terrain"},{"excerpt":"Overview The  @loaders.gl/terrain  module reconstructs mesh surfaces from either height\nmap images--e.g.  Mapzen Terrain Tiles --which encode\nelevation into R,G,B values or the  quantized mesh  format. Installation Attribution The  QuantizedMeshLoader  is a fork of\n quantized-mesh-decoder \nfrom HERE under the MIT license to decode quantized mesh. The  TerrainLoader  uses  MARTINI  for mesh\nreconstruction under the ISC License.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/terrain/docs","title":"Overview"},{"excerpt":"QuantizedMeshLoader The  QuantizedMeshLoader  module reconstructs mesh surfaces from the  quantized\nmesh  format. Loader Characteristic File Extension .terrain File Type Binary File Format Encoded mesh Data Format Mesh Supported APIs load ,  parse ,  parseSync Decoder Type Synchronous Worker Thread Support Yes Streaming Support No Usage Options Option Type Default Description quantized-mesh.bounds array<number> [0, 0, 1, 1] Bounds of the image to fit x,y coordinates into. In  [minX, minY, maxX, maxY] . Remarks Future Work Skirting. The Quantized Mesh format includes data on which vertices are on each edge, which should assist in creating a skirt. Use optional Quantized Mesh extensions, such as vertex normals. Closer integration into tile culling. Quantized Mesh headers, the first 88 bytes, describe a tile's bounding volume and min/max elevations. Just the headers could be parsed while deciding whether the tile is in view. Upon verifying visibility, the rest of the tile's data can be parsed.","headings":[{"value":"QuantizedMeshLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Remarks","depth":2},{"value":"Future Work","depth":3}],"slug":"modules/terrain/docs/api-reference/quantized-mesh-loader","title":"QuantizedMeshLoader"},{"excerpt":"TerrainLoader The  TerrainLoader  reconstructs mesh surfaces from height map images, e.g.  Mapzen Terrain Tiles , which encodes elevation into R,G,B values. Loader Characteristic File Extension .png ,  .pngraw File Type Binary File Format Encoded height map Data Format Mesh Supported APIs load ,  parse Decoder Type Asynchronous Worker Thread Support Yes Streaming Support No Usage Options Option Type Default Description terrain.meshMaxError number 10 Mesh error in meters. The output mesh is in higher resolution (more vertices) if the error is smaller. terrain.bounds array<number> null Bounds of the image to fit x,y coordinates into. In  [minX, minY, maxX, maxY] . If not supplied, x and y are in pixels relative to the image. terrain.elevationDecoder object See below See below elevationDecoder Parameters used to convert a pixel to elevation in meters.\nAn object containing the following fields: rScale : Multiplier of the red channel. gScale : Multiplier of the green channel. bScale : Multiplier of the blue channel. offset : Translation of the sum. Each color channel (r, g, and b) is a number between  [0, 255] . For example, the Mapbox terrain service's elevation is  encoded as follows : The corresponding  elevationDecoder  is: The default value of  elevationDecoder  decodes a grayscale image:","headings":[{"value":"TerrainLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"elevationDecoder","depth":3}],"slug":"modules/terrain/docs/api-reference/terrain-loader","title":"TerrainLoader"},{"excerpt":"Working with Tables The loaders.gl table category provides support for working interchangably with row-oriented and columnar tables.","headings":[{"value":"Working with Tables","depth":1}],"slug":"modules/tables/docs/table-guide","title":"Working with Tables"},{"excerpt":"@loaders.gl/tables This module contains: Classes and APIs for manipulating tabular data output from loaders.gl table category loaders (CSV, JSON, ...). loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. Please visit the  website .","headings":[{"value":"@loaders.gl/tables","depth":1}],"slug":"modules/tables","title":"@loaders.gl/tables"},{"excerpt":"TableBatch RowTableBatch ColumnarTableBatch ArrowTableBatch","headings":[{"value":"TableBatch","depth":1}],"slug":"modules/tables/docs/api-reference/table-batch","title":"TableBatch"},{"excerpt":"Table","headings":[{"value":"Table","depth":1}],"slug":"modules/tables/docs/api-reference/table","title":"Table"},{"excerpt":"TableSchema","headings":[{"value":"TableSchema","depth":1}],"slug":"modules/tables/docs/api-reference/table-schema","title":"TableSchema"},{"excerpt":"@loaders.gl/tables Table Table APIs The table API is modelled after a subset of the Apache Arrow API: Class Arrow Counterpart Description Table Table Table TableSchema Schema Table schema TableBatch RecordBatch Table batch Micro-Loaders Loaders with limited functionality but with minimal bundle size impact: Loader Description JSONLoader A minimal non-streaming JSON loader that uses the built-in  JSON.parse  function XMLLoader A non-streaming, browser-only XML loader that uses the browser's built-in DOM parser.","headings":[{"value":"@loaders.gl/tables","depth":1},{"value":"Table APIs","depth":2},{"value":"Micro-Loaders","depth":2}],"slug":"modules/tables/docs","title":"@loaders.gl/tables"},{"excerpt":"@loaders.gl/shapefile This module contains a geometry loader for the ESRI Shapefile format. loaders.gl  is a collection of framework-independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/shapefile","depth":1}],"slug":"modules/shapefile","title":"@loaders.gl/shapefile"},{"excerpt":"DBFLoader A sub loader for the  .dbf  (attributes/properties) file component of a shapefile. This is essentially a loader for the legacy dBase 7 database format. Note: Most applications will want to use the  ShapefileLoader  instead of this loader. Loader Characteristic File Extension .dbf , File Type Binary File Format Shapefiles Data Format Table Supported APIs load ,  parse ,  parseSync Decoder Type Synchronous Worker Thread Support Yes Usage The  DBFLoader  parses feature attributes from the Shapefile format. Options encoding : text encoding of DBF file: usually either  utf8 , or  ascii / windows-1252 . For Shapefiles, there's often a  .cpg  file designating the encoding used. Format Summary ESRI Shapefiles are a popular file format for storing geospatial vector data.\nThe format consists of a number of files that must be stored together and with\nthe same file name. Files with extensions  .shp ,  .shx ,  .dbf  must exist;\nadditional files with other extensions such as  .prj  and  .cpg  may exist. References: https://www.clicketyclick.dk/databases/xbase/format/data_types.html http://www.dbase.com/Knowledgebase/INT/db7_file_fmt.htm http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Geoprocessing_considerations_for_shapefile_output https://www.loc.gov/preservation/digital/formats/fdd/fdd000326.shtml https://support.esri.com/en/technical-article/000013192","headings":[{"value":"DBFLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Format Summary","depth":2},{"value":"References:","depth":2}],"slug":"modules/shapefile/docs/api-reference/dbf-loader","title":"DBFLoader"},{"excerpt":"Overview The  @loaders.gl/shapefile  module handles the Shapefile format, a widely used binary format. Installation Loaders and Writers Loader SHPLoader Attribution","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/shapefile/docs","title":"Overview"},{"excerpt":"@loaders.gl/potree (Experimental) This module contains loaders for the  potree  format. loaders.gl  is a collection of framework-independent 3D and geospatial loaders (parsers). For documentation please visit the  website .","headings":[{"value":"@loaders.gl/potree (Experimental)","depth":1}],"slug":"modules/potree","title":"@loaders.gl/potree (Experimental)"},{"excerpt":"SHPLoader A \"sub loader\" for the  .shp  (geometries) file component of a shapefile. Note: Most applications will want to use the  ShapefileLoader  instead of this loader. Loader Characteristic File Extension .shp File Type Binary File Format Shapefiles Data Format Geometry Supported APIs load ,  parse ,  parseSync Decoder Type Synchronous Worker Thread Support Yes Usage Options Option Type Default Description shp. _ maxDimensions Integer 4 Shapefiles can hold up to 4 dimensions (XYZM). By default all dimensions are parsed; when set to  2  only the X and Y dimensions are parsed. Note that for some Shapefiles, the third dimension is M, not Z.  header.type  in the output designates the stored dimensions. Output The  ShapefileLoader 's output looks like the following.  geometries  holds an\narray of features in loaders.gl's binary geometry format.  prj  contains the\nShapefile's projection string.  header  contains the Shapefile's header values,\nincluding a bounding box of the data and the file's geometry type. Consult the\n Shapefile specification  for the meaning of the numeric types. Format Summary ESRI Shapefiles are a popular file format for storing geospatial vector data.\nThe format consists of a number of files that must be stored together and with\nthe same file name. Files with extensions  .shp ,  .shx ,  .dbf  must exist;\nadditional files with other extensions such as  .prj  and  .cpg  may exist.","headings":[{"value":"SHPLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Output","depth":2},{"value":"Format Summary","depth":2}],"slug":"modules/shapefile/docs/api-reference/shp-loader","title":"SHPLoader"},{"excerpt":"@loaders.gl/polyfills loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains polyfills for running on older browsers (mainly Edge and IE11) as well as Node. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/polyfills","depth":1}],"slug":"modules/polyfills","title":"@loaders.gl/polyfills"},{"excerpt":"@loaders.gl/potree The potree loaders are still under development and are not yet considered ready for use. Support for loading and traversing  potree  format point clouds. Installation Usage Intended usage only, not yet working! API This modules provides the following exports: PotreeHierarchyChunkLoader  for the hierarchy indices Roadmap The plan is to provide the following loaders/writers: PotreeLoader  for individual tiles PotreeLoader  is intended to work with the 3d tileset classes in the  @loaders.gl/3d-tiles  module. Tileset3D  class will be generalized to accept loaded potree tilesets. Attribution The  PotreeLoader  is a fork of Markus Schuetz' potree code ( https://github.com/potree/potree ) under BSD-2 clause license.","headings":[{"value":"@loaders.gl/potree","depth":1},{"value":"Installation","depth":2},{"value":"Usage","depth":2},{"value":"API","depth":2},{"value":"Roadmap","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/potree/docs","title":"@loaders.gl/potree"},{"excerpt":"@loaders.gl/ply loaders.gl  is a collection of loaders for big data visualizations. This module contains loaders for the PLY format. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/ply","depth":1}],"slug":"modules/ply","title":"@loaders.gl/ply"},{"excerpt":"Overview The  @loaders.gl/ply  module handles the the  Polygon file format , or the Stanford Trangle Format, a file format for 3D graphical objects described as a collection of polygons. Installation Attribution PLYLoader is a fork of the THREE.js PLYLoader under MIT License. The THREE.js source files contained the following attributions: @author Wei Meng /  http://about.me/menway","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/ply/docs","title":"Overview"},{"excerpt":"Overview The optional  @loaders.gl/polyfills  module installs support for Node.js and older browsers. loaders.gl is based on the HTML5 API provided by modern, evergreen browsers. Older browsers (mainly Edge and IE11) as well as versions of Node.js prior to v12 do not provide certain classes that loaders.gl depends on. Note that while  @loaders.gl/polyfills  is designed to work seamlessly with other loaders.gl modules, using it is not a requirement. There are other good polyfill modules available on  npm  that can be used in its place. Installation Usage Just import  @loaders.gl/polyfills  before you start using other loaders.gl modules. To use the experimental  Blob  and  File  polyfills Included Polyfills Polyfill Node Browser Comments TextEncoder / TextDecoder Node.js < 11 Yes (Older browsers) Only UTF8 is guaranteed to be supported atob / btoa All versions No Note: these functions are  not unicode safe , but OK to use for test cases. fetch All versions No A subset of the fetch API is supported, see below. Response All versions No A subset of the  Response  API is supported, see below. Headers All versions No A subset of the fetch API is supported, see below. Blob  (Experimental) All versions No A subset of the fetch API is supported, see below. File  (Experimental) All versions No A subset of the fetch API is supported, see below. FileReader  (Experimental) All versions No A subset of the fetch API is supported, see below. ReadableStream  (Experimental) All versions No A subset of the ReadableStream API is supported. fetch Polyfill The Node.js  fetch ,  Response  and  Headers  polyfills supports a large subset of the browser fetch API, including: Response.text() ,  Response.arrayBuffer() ,  Response.json() Response.body  stream headers ,  status ,  statusText  etc. data uri / base64 decoding automatic gzip, brotli and deflate decompression support for responses with  content-encoding  headers. Files ending with  .gz  are automatically decompressed with gzip decompression (this is only done on Node.js, in the browser the content-encoding header must be set). The Node.js  fetch  is able to follow 30X redirect: if  Response  has status 300-399 and  location  header is set, the  fetch  polyfill re-requests data from  location . TextEncoder and TextDecoder Polyfills TextEncoder  and  TextDecoder  polyfills are provided to ensure these APIs are always available. In modern browsers these will evaluate to the built-in objects of the same name, however under Node.js polyfills are transparently installed. Note: The provided polyfills only guarantee UTF8 support. Remarks Applications should only install this module if they need to run under older environments. While the polyfills are only installed at runtime if the platform does not already support them, importing this module will increase the application's bundle size. Refer to browser documentation for the usage of these classes, e.g. MDN. In the browser, overhead of using these imports is not as high, as most polyfills are only bundled under Node.js. If working under older browsers, e.g. IE11, you may need to install your own TextEncoder/TextDecoder polyfills before loading this library Attribution The  Header  polyfill (for Node.js  fetch ) is a fork of the implementation in  https://github.com/github/fetch  (MIT license). The  Blob  and  File  polyfills are forks of @gozala's  web-blob  and  web-file  modules respectively, under MIT license.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Usage","depth":2},{"value":"Included Polyfills","depth":2},{"value":"fetch Polyfill","depth":2},{"value":"TextEncoder and TextDecoder Polyfills","depth":1},{"value":"Remarks","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/polyfills/docs/api-reference","title":"Overview"},{"excerpt":"PLYLoader The  PLYLoader  parses simple meshes in the Polygon File Format or the Stanford Triangle Format. Loader Characteristic File Extension .ply File Type Binary/Text File Format PLY Data Format Mesh Decoder Type Synchronous Worker Thread Support Yes Streaming Support No Usage Options Option Type Default Description","headings":[{"value":"PLYLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/ply/docs/api-reference/ply-loader","title":"PLYLoader"},{"excerpt":"@loaders.gl/pcd loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loaders for the PCD format. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/pcd","depth":1}],"slug":"modules/pcd","title":"@loaders.gl/pcd"},{"excerpt":"ShapefileLoader Shapefile loader Loader Characteristic File Extension .shp , File Type Binary, Multi-File File Format Shapefile Data Format Geometry Supported APIs load ,  parse ,  parseSync Decoder Type Synchronous Worker Thread Support Yes, For Some Loaders Usage Options Option Type Default Description shp. _ maxDimensions Integer 4 Shapefiles can hold up to 4 dimensions (XYZM). By default all dimensions are parsed; when set to  2  only the X and Y dimensions are parsed. Note that for some Shapefiles, the third dimension is M, not Z.  header.type  in the output designates the stored dimensions. Output The  ShapefileLoader 's output looks like the following.  data  holds an array\nof GeoJSON  Feature s.  prj  contains the Shapefile's projection string.\n header  contains the Shapefile's header values, including a bounding box of the\ndata and the file's geometry type. Consult the  Shapefile\nspecification  for the meaning of the numeric types. Format Summary ESRI Shapefiles are a popular file format for storing geospatial vector data.\nThe format consists of a number of files that must be stored together and with\nthe same file name. Files with extensions  .shp ,  .shx ,  .dbf  must exist;\nadditional files with other extensions such as  .prj  and  .cpg  may exist. File Type Contents .shp Binary The geometry, i.e. the geometry column in the resulting table. .dbf Binary The attributes, i.e. the data columns in the resulting table. .shx Binary The index (technically required, however it is sometimes possible to open shapefiles without the index) .prj Text A small usually single line text file containing a WKT-CRS style projection. WGS84 is assumed if not present. .cpg Text A small text file containing a text encoding name for the DBF text fields.  latin1  is assumed if not present.","headings":[{"value":"ShapefileLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Output","depth":2},{"value":"Format Summary","depth":2}],"slug":"modules/shapefile/docs/api-reference/shapefile-loader","title":"ShapefileLoader"},{"excerpt":"Overview The  @loaders.gl/pcd  module handles the the  Point Cloud Data format , which encodes point cloud data for use inside Point Cloud Library (PCL). Installation Attribution PCDLoader is a fork of the THREE.js PCDLoader under MIT License. The THREE.js source files contained the following attributions: @author Filipe Caixeta /  http://filipecaixeta.com.br @author Mugen87 /  https://github.com/Mugen87","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/pcd/docs","title":"Overview"},{"excerpt":"@loaders.gl/obj loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loader for the OBJ format. For documentation please visit the  website . Note: The OBJ parser in this module is a port of  three.js 's OBJLoader. The MIT License Copyright  2010-2019 three.js authors Permission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.","headings":[{"value":"@loaders.gl/obj","depth":1}],"slug":"modules/obj","title":"@loaders.gl/obj"},{"excerpt":"PCDLoader The  PCDLoader  loads point cloud in the Point Cloud Data (PCD) format. Loader Characteristic File Extension .pcd File Type Text/Binary File Format Point Cloud Data Data Format PointCloud Decoder Type Synchronous Worker Thread Support Yes Streaming Support No Note: Currently only  ascii  and  binary  subformats are supported. Compressed binary files are currently not supported. Usage Options Option Type Default Description","headings":[{"value":"PCDLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/pcd/docs/api-reference/pcd-loader","title":"PCDLoader"},{"excerpt":"Overview The  @loaders.gl/obj  module handles the the  Wavefront OBJ format , a simple ASCII format that defines 3D geometries as vertices, normals and faces. Installation Attribution OBJLoader is a port of  three.js 's OBJLoader under MIT License.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/obj/docs","title":"Overview"},{"excerpt":"OBJLoader The  OBJLoader  parses the OBJ half of the classic Wavefront OBJ/MTL format. Loader Characteristic File Extension .obj File Type Text File Format Wavefront OBJ file Data Format Mesh Decoder Type Synchronous Worker Thread Support Yes Streaming Support No Usage Options Option Type Default Description","headings":[{"value":"OBJLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/obj/docs/api-reference/obj-loader","title":"OBJLoader"},{"excerpt":"Overview The  @loaders.gl/mvt  module handles the  Mapbox Vector Tile  format, a protobuf encoded format that defines geospatial geometries. Installation Loaders and Writers Loader MVTLoader Attribution The  MVTLoader  uses  @mapbox/vector-tile  module under the BSD-3-Clause.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/mvt/docs","title":"Overview"},{"excerpt":"@loaders.gl/mvt This module contains a geometry loader for Mapbox Vector Tiles (MVT). loaders.gl  is a collection of framework-independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/mvt","depth":1}],"slug":"modules/mvt","title":"@loaders.gl/mvt"},{"excerpt":"@loaders.gl/math (Experimental, Temporary) loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains math utilities for the  @loaders.gl3d-tiles  module. As they mature, these will likely be moved to a math framework (e.g. math.gl). This code is a fork of a subset of the Cesium math library whcih is Apache 2 licensed. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/math (Experimental, Temporary)","depth":1}],"slug":"modules/math","title":"@loaders.gl/math (Experimental, Temporary)"},{"excerpt":"Overview The  @loaders.gl/math  library is being developed to support 3D tiles and will be moved to the math.gl repository when it stabilizes. Classes and utilities to help working with geometries (arrays of vertices) stored in typed arrays according to WebGL/OpenGL layout rules. Usage Examples Framework Independence Like all non-core math.gl modules, this library can be used without the math.gl core classes. Any input vectors can be supplied as length 3 JavaScript  Array  instances. Any result vectors can be treated as length 3 JavaScript  Array  instances (they may be math.gl  Vector3 ). The core math.gl classes inherit from JavaScript  Array  and can be used directly as input.","headings":[{"value":"Overview","depth":1},{"value":"Usage Examples","depth":2},{"value":"Framework Independence","depth":2}],"slug":"modules/math/docs","title":"Overview"},{"excerpt":"MVTLoader Loader for the  Mapbox Vector Tile  format for representation of geometry. Loader Characteristic File Extension .mvt , File Type Binary File Format Mapbox Vector Tile Data Format Geometry Supported APIs load ,  parse ,  parseSync Usage Outputs GeoJSON The parser will return an array of  GeoJSON objects  with WGS84 coordinates and feature properties from MVT if  coordinates  property is set to  wgs84  and  tileIndex  properties are present. GeoJSON with local coordinates The parser will return an array of GeoJSON objects with local coordinates in a range from 0 to 1 and feature properties from MVT by default. Even though tile coordinates go from 0 to 1, there can be some negative (or greater than one) coordinates because of buffer cells within MVT to handle geometry clipping. That difference can be as much as  bufferSize / tileExtent  depending on MVT creation parameters. Note that local coordinates are relative to tile origin, which is in the top left. Options Option Type Default Description mvt.coordinates String local When set to  wgs84 , the parser will return a flat array of GeoJSON objects with coordinates in longitude, latitude decoded from the provided tile index. When set to  local , the parser will return a flat array of GeoJSON objects with local coordinates decoded from tile origin. mvt.layerProperty String layerName When non- null , the layer name of each feature is added to  feature.properties[layerProperty] . (A  feature.properties  object is created if the feature has no existing properties). If set to  null , a layer name property will not be added. mvt.layers String[] null Optional list of layer names. If not  null , only features belonging to the named layers will be included in the output. If  null , features from all layers are returned. mvt.tileIndex Object ( {x: number, y: number, z: number} ) null Mandatory with  wgs84  coordinates option. An object containing tile index values ( x ,  y ,  z ) to reproject features' coordinates into WGS84. If you want to know more about how geometries are encoded into MVT tiles, please read  this documentation section . Attribution The  MVTLoader  uses  @mapbox/vector-tile  module under the BSD-3-Clause.","headings":[{"value":"MVTLoader","depth":1},{"value":"Usage","depth":2},{"value":"Outputs","depth":2},{"value":"GeoJSON","depth":3},{"value":"GeoJSON with local coordinates","depth":3},{"value":"Options","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/mvt/docs/api-reference/mvt-loader","title":"MVTLoader"},{"excerpt":"GLType Helper functions to work with WebGL data type constants. WebGL type constant JavaScript Typed Array Notes GL.FLOAT Float32Array GL.DOUBLE Float64Array Not yet directly usable in WebGL/GLSL GL.UNSIGNED_SHORT Uint16Array GL.UNSIGNED_INT Uint32Array GL.UNSIGNED_BYTE Uint8Array GL.UNSIGNED_BYTE Uint8ClampedArray GL.BYTE Int8Array GL.SHORT Int16Array GL.INT Int32Array Usage Static Methods GLType.fromTypedArray(typedArray: Typed Array | Function) : Number Returns the size, in bytes, of the corresponding datatype. glType  The component datatype to get the size of. Returns The size in bytes. Throws glType is not a valid value. Gets the {@link ComponentDatatype} for the provided TypedArray instance. array The typed array. Returns The ComponentDatatype for the provided array, or undefined if the array is not a TypedArray. GLType.getArrayType(glType: Number) : Function returns the constructor of the array static GLType.getByteSize(glType: Number) : Number Returns the size in bytes of one element of the provided WebGL type. Equivalent to  GLType.getArrayType(glType).BYTES_PER_ELEMENT . static GLType.validate(glType) : Boolean Returns  true  if  glType  is a valid WebGL data type. static GLType.createTypedArray(glType : Number, buffer : ArrayBuffer [, byteOffset : Number  , length : Number ]) : TypedArray Creates a typed view of an array of bytes. glType  The type of typed array (ArrayBuffer view) to create. buffer  The buffer storage to use for the view. byteOffset = 0  The offset, in bytes, to the first element in the view. length = The number of elements in the view. Defaults to buffer length. Returns Int8Array | Uint8Array | Int16Array | Uint16Array | Int32Array | Uint32Array | Float32Array | Float64Array  A typed array view of the buffer. Throws glType  is not a valid value.","headings":[{"value":"GLType","depth":1},{"value":"Usage","depth":2},{"value":"Static Methods","depth":2},{"value":"GLType.fromTypedArray(typedArray: Typed Array | Function) : Number","depth":3},{"value":"GLType.getArrayType(glType: Number) : Function","depth":3},{"value":"static GLType.getByteSize(glType: Number) : Number","depth":3},{"value":"static GLType.validate(glType) : Boolean","depth":3},{"value":"static GLType.createTypedArray(glType : Number, buffer : ArrayBuffer [, byteOffset : Number , length : Number]) : TypedArray","depth":3}],"slug":"modules/math/docs/api-reference/gl-type","title":"GLType"},{"excerpt":"Request Scheduler The request scheduler enables an application to \"issue\" a large number of requests without flooding the browser's limited request queue. A getPriority callback is called on all outstanding requests whenever a slot frees up, allowing the application to reprioritize or even cancel \"issued\" requests if the application state has changed. Note: The request scheduler does not actually issue requests, it just lets apps know when the request can be issued without overwhelming the connection and the server. A primary use case is to let the app reprioritize or cancel requests if circumstances change before the request can be scheduled. Some information on browser  request throttling Usage To schedule a request so that it can be issued at a time when it can be immediately processed. Methods constructor(options?: object) id ?: string; throttleRequests ?: boolean; maxRequests ?: number; scheduleRequest(handle: any, getPriority?: () => number): Promise<{done: () => any)}>; Called by an application that wants to issue a request, without having it deeply queued by the browser When the returned promise resolved, it is OK for the application to issue a request.\nThe promise resolves to an object that contains a  done  method.\nWhen the application's request has completed (or failed), the application must call the  done  function Parameters handle  an arbitrary handle to identify the request, e.g. a URL string getPriority  will be called when request \"slots\" open up,\nallowing the caller to update priority or cancel the request\nHighest priority executes first, priority < 0 cancels the request Returns a promise that resolves to an object (with a  done  field) when the request can be issued without queueing. The application should issue the request and call  done()  when completed. resolves to  null  if the request has been cancelled (by the callback return < 0).\nIn this case the application should not issue the request. About Request Priorities The  getPriority  callback controls priority of requests and also cancellation of outstanding requests.","headings":[{"value":"Request Scheduler","depth":1},{"value":"Usage","depth":2},{"value":"Methods","depth":2},{"value":"constructor(options?: object)","depth":3},{"value":"scheduleRequest(handle: any, getPriority?: () => number): Promise<{done: () => any)}>;","depth":3},{"value":"About Request Priorities","depth":2}],"slug":"modules/loader-utils/docs/api-reference/request-scheduler","title":"Request Scheduler"},{"excerpt":"Overview The  @loaders.gl/las  module handles the  LASER file format  (LAS) or its compressed version (LAZ), a public format for the interchange of 3-dimensional point cloud data data, developed for LIDAR mapping purposes. Installation Attribution LASLoader is a fork of Uday Verma and Howard Butler's  plasio  under MIT License.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/las/docs","title":"Overview"},{"excerpt":"@loaders.gl/loader-utils This module contains shared utilities for loaders.gl, a collection of framework-independent 3D and geospatial loaders (parsers). For documentation please visit the  website .","headings":[{"value":"@loaders.gl/loader-utils","depth":1}],"slug":"modules/loader-utils","title":"@loaders.gl/loader-utils"},{"excerpt":"@loaders.gl/las loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loaders and writers for the LAS and LAZ formats. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/las","depth":1}],"slug":"modules/las","title":"@loaders.gl/las"},{"excerpt":"LASLoader The  LASLoader  parses a point cloud in the LASER file format. Loader Characteristic File Extension .las ,  .laz File Type Binary File Format LASER file format Data Format PointCloud Decoder Type Synchronous Worker Thread Support Yes Streaming Support No Usage Options Option Type Default Description options.las.skip Number 1 Read one from every  n  points. options.las.fp64 Number false If  true , positions are stored in 64-bit floats instead of 32-bit. options.las.colorDepth Number or string 8 Whether colors encoded using 8 or 16 bits? Can be set to  'auto' . Note: LAS specification recommends 16 bits. options.onProgress Function - Callback when a new chunk of data is read. Only works on the main thread.","headings":[{"value":"LASLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/las/docs/api-reference/las-loader","title":"LASLoader"},{"excerpt":"@loaders.gl/kml loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loaders for the KML format. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/kml","depth":1}],"slug":"modules/kml","title":"@loaders.gl/kml"},{"excerpt":"Overview The  @loaders.gl/kml  module supports the KML, GPX, and TCX formats. KML (Keyhole Markup Language) is an XML-based file format used to display geographic data in an Earth browser such as Google Earth (originally named \"Keyhole Earth Viewer\"). It can be used with any 2D or 3D maps. GPX (GPS Exchange Format) is an XML-based file format commonly used by GPS tracking software. TCX (Training Center XML) is an XML-based file format commonly used by fitness watches or similar GPS tracking software. References: Keyhole Markup Language - Wikipedia KML Tutorial - Google GPX - Wikipedia TCX - Wikipedia Installation Attribution The three loaders use  @tmcw/togeojson  under the BSD-2-Clause license.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/kml/docs","title":"Overview"},{"excerpt":"KMLLoader The  KMLLoader  parses  KML files  into GeoJSON. From Wikipedia: Keyhole Markup Language (KML) is an XML notation for expressing geographic\nannotation and visualization within two-dimensional maps and three-dimensional\nEarth browsers. KML is now an  Open Geospatial Consortium standard . Loader Characteristic File Extension .kml File Type Text File Format KML Data Format GIS Decoder Type Synchronous Worker Thread Support No Streaming Support No Usage Options Option Type Default Description gis.format string 'geojson' Can be set to  'raw' ,  'geojson'  or  'binary' . Limitations In Node.JS, applications must import  @loaders.gl/polyfills  for the  DOMParser  polyfill.","headings":[{"value":"KMLLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Limitations","depth":2}],"slug":"modules/kml/docs/api-reference/kml-loader","title":"KMLLoader"},{"excerpt":"GPXLoader The  GPXLoader  parses  GPX files  into GeoJSON. From Wikipedia: GPX, or GPS Exchange Format, is an XML schema designed as a common GPS data\nformat for software applications. It can be used to describe waypoints,\ntracks, and routes. ... Location data (and optionally elevation, time, and\nother information) is stored in tags and can be interchanged between GPS\ndevices and software. Loader Characteristic File Extension .gpx File Type Text File Format GPX Data Format GIS Decoder Type Synchronous Worker Thread Support No Streaming Support No Usage Options Option Type Default Description gis.format string 'geojson' Can be set to  'raw' ,  'geojson'  or  'binary' . Limitations In Node.JS, applications must import  @loaders.gl/polyfills  for the  DOMParser  polyfill.","headings":[{"value":"GPXLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Limitations","depth":2}],"slug":"modules/kml/docs/api-reference/gpx-loader","title":"GPXLoader"},{"excerpt":"Overview The  @loaders.gl/json  module handles tabular data stored in the  JSON file format . Installation Loaders and Writers Loader JSONLoader Additional APIs See table category. Module Roadmap General Improvements Error messages:  JSON.parse  tends to have unhelpful error messages Support Streaming JSON Formats Overview of  JSON Streaming Formats  (Wikipedia). Line-delimited JSON  (LDJSON) (aka JSON lines) (JSONL). NewLine delimited JSON Autodetection of streaming JSON A number of hints can be used to determine if the data is formatted using a streaming JSON format if the filename extension is  .jsonl if the MIMETYPE is  application/json-seq if the first value in the file is a number, assume the file is length prefixed. For data in non-streaming JSON format, the presence of a top-level array will start streaming of objects. For embedded arrays, a path specifier may need to be supplied (or could look for first array). MIME Types and File Extensions Format Extension MIME Media Type  RFC4288 Standard JSON .json application/json Line-delimited JSON .jsonl - NewLine delimited JSON .ndjson application/x-ndjson Record separator-delimited JSON - application/json-seq","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Additional APIs","depth":2},{"value":"Module Roadmap","depth":2},{"value":"General Improvements","depth":3},{"value":"Support Streaming JSON Formats","depth":3},{"value":"Autodetection of streaming JSON","depth":3},{"value":"MIME Types and File Extensions","depth":3}],"slug":"modules/json/docs","title":"Overview"},{"excerpt":"TCXLoader The  TCXLoader  parses  TCX files  into GeoJSON. From Wikipedia: Training Center XML (TCX) is a data exchange format introduced in 2007 as part\nof Garmin's Training Center product. The XML is similar to GPX since it\nexchanges GPS tracks, but treats a track as an Activity rather than simply a\nseries of GPS points. TCX provides standards for transferring heart rate,\nrunning cadence, bicycle cadence, calories in the detailed track. It also\nprovides summary data in the form of laps. Loader Characteristic File Extension .tcx File Type Text File Format TCX Data Format GIS Decoder Type Synchronous Worker Thread Support No Streaming Support No Usage Options Option Type Default Description gis.format string 'geojson' Can be set to  'raw' ,  'geojson'  or  'binary' . Limitations In Node.JS, applications must import  @loaders.gl/polyfills  for the  DOMParser  polyfill.","headings":[{"value":"TCXLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Limitations","depth":2}],"slug":"modules/kml/docs/api-reference/tcx-loader","title":"TCXLoader"},{"excerpt":"JSONLoader Streaming loader for JSON encoded files. Loader Characteristic File Extension .json , File Type Text File Format JSON Data Format Classic Table Supported APIs load ,  parse ,  parseSync ,  parseInBatches Usage The JSONLoader supports streaming JSON parsing, in which case it will yield \"batches\" of rows from one array. To e.g. parse a stream of GeoJSON, the user can specify the  options.json.jsonpaths  to stream the  features  array. If no JSONPath is specified the loader will stream the first array it encounters in the JSON payload. When batch parsing an embedded JSON array as a table, it is possible to get access to the containing object using the  {json: {_rootObjectBatches: true}}  option. The loader will yield an initial and a final batch with  batch.container  providing the container object and  batch.batchType  set to  root-object-batch-partial  and  root-object-batch-complete  respectively. Data Format Parsed batches are of the format Options Supports table category options such as  batchType  and  batchSize . Option From Type Default Description json.table [ ] boolean false Parses non-streaming JSON as table, i.e. return the first embedded array in the JSON. Always  true  during batched/streaming parsing. json.jsonpaths [ ] string[] [] A list of JSON paths (see below) indicating the array that can be streamed. metadata  (top level) [ ] boolean If  true , yields an initial and final batch containing the partial and final result (i.e. the root object, excluding the array being streamed). JSONPaths A minimal subset of the JSONPath syntax is supported, to specify which array in a JSON object should be streamed as batchs. $.component1.component2.component3 No support for wildcards, brackets etc. Only paths starting with  $  (JSON root) are supported. Regardless of the paths provided, only arrays will be streamed. Attribution This loader is based on a fork of dscape's  clarinet  under BSD 2-clause license.","headings":[{"value":"JSONLoader","depth":1},{"value":"Usage","depth":2},{"value":"Data Format","depth":2},{"value":"Options","depth":2},{"value":"JSONPaths","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/json/docs/api-reference/json-loader","title":"JSONLoader"},{"excerpt":"@loaders.gl/images loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loader and writers for images that follow loaders.gl conventions and work under both node and browser. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/images","depth":1}],"slug":"modules/images","title":"@loaders.gl/images"},{"excerpt":"Binary Image Utilities Utilities to extract metadata such as image format (MIME type) and size (dimensions) from binary images without parsing the full image. Looks for format-specific headers in the encoded binary data (e.g. encoded JPEG or PNG images). The format is reported using MIME types strings. Supported binary formats and their MIME types are: Format MIME Type PNG image/png JPEG image/jpeg BMP image/bmp GIF image/gif Usage Functions getBinaryImageMetadata(imageData: ArrayBuffer | DataView): object | null Parameters: imageData : Binary encoded image data. Returns a metadata object describing the image. Returns  null  if the binary data does not represent a known binary image format. If  mimeType  is supplied, assumes the image is of that type. If not supplied, first attempts to auto deduce the image format (see  getImageMIMEType ).","headings":[{"value":"Binary Image Utilities","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"getBinaryImageMetadata(imageData: ArrayBuffer | DataView): object | null","depth":3}],"slug":"modules/images/docs/api-reference/binary-image-api","title":"Binary Image Utilities"},{"excerpt":"Overview The  @loaders.gl/images  module contains loader and writers for images that follow loaders.gl conventions and work under both node and browser. Installation API Loader Description ImageLoader ImageWriter Binary Image API A set of functions that can extract information from \"unparsed\" binary memory representation of certain image formats. These functions are intended to be called on raw  ArrayBuffer  data, before the  ImageLoader  parses it and converts it to a parsed image type. These functions are used internally to autodetect if image loader can be used to parse a certain  ArrayBuffer , but are also available to applications. Function Description isBinaryImage(imageData : ArrayBuffer [, mimeType : String]) : Boolean `getBinaryImageMIMEType(imageData : ArrayBuffer) : String null` getBinaryImageSize(imageData : ArrayBuffer [, mimeType : String]) : Object Parsed Image API A set of functions to work with parsed images returned by the  ImageLoader . Function Description isImageTypeSupported(type : string) : boolean Check if type is supported by current run-time environment getDefaultImageType() : string Returns the image type selected by default (  options.image.type: 'auto'  in current run-time environment isImage(image : any) : boolean Checks any JavaScript value to see if it is an image of a type that loaders.gl can work with getImageType(image : any) : string Returns the type name for this image. getImageData(image : any) : object Returns an image data object with a  data  array representing the pixels of an image Image Loading API for WebGL Textures The images API also offers functions to load \"composite\" images for WebGL textures, cube textures and image mip levels. These functions take a  getUrl  parameter that enables the app to supply the url for each \"sub-image\", and return a single promise enabling applications to for instance load all the faces of a cube texture, with one image for each mip level for each face in a single async operation. Function Description loadImage Load a single image loadImageArray Load an array of images, e.g. for a  Texture2DArray  or  Texture3D loadImageCube Load a map of 6 images for the faces of a cube map, or a map of 6 arrays of images for the mip levels of the 6 faces. As with all loaders.gl functions, while these functions are intended for use in WebGL applications, they do not call any WebGL functions, and do not actually create any WebGL textures.. Image Types To support image loading on older browsers and Node.js, the  ImageLoader  can return different types, i.e. different representations of the parsed image. ImageBitmap  - An  ImageBitmap  object represents a bitmap image that can be performantly painted to a canvas (\"without undue latency\"). Due to the signficant performance advantages, and the fact that  ImageBitmap  instances can be transferred efficiently between threads,  ImageBitmap  is the preferred parsed image representation in browsers, when available. Currently only available in Chrome and Firefox. Image  (aka  HTMLImageElement ) - The traditional HTML image class. Available in all browsers. data  - Raw binary memory representing the image pixels, typically in RGBA  Uint8Array  format. JavaScript computations can be done on this data. Also, Node.js texture creation functions in headless gl accept  data  images. and browser  ImageData  objects can be initialized with this data. See  ImageLoader  for more details on options etc.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"API","depth":2},{"value":"Binary Image API","depth":3},{"value":"Parsed Image API","depth":3},{"value":"Image Loading API for WebGL Textures","depth":3},{"value":"Image Types","depth":2}],"slug":"modules/images/docs","title":"Overview"},{"excerpt":"@loaders.gl/json This module contains a table loader for the JSON and line delimited JSON formats. loaders.gl  is a collection of framework-independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/json","depth":1}],"slug":"modules/json","title":"@loaders.gl/json"},{"excerpt":"ImageWriter The  ImageWriter  class can encode an image into  ArrayBuffer  both under browser and Node.js Loader Characteristic File Extension .png ,  .jpg ,  .jpeg File Type Binary Data Format ImageBitmap ,  Image  or \"image data\" File Format JPEG, PNG, ... Encoder Type Asynchronous Worker Thread No (but may run on separate native thread in browsers) Streaming No Usage Data Format The  ImageWriter  can encode three different in-memory Image representations into binary image representation (such as JPEG or PNG images) that can then be be saved or uploaded, The supported image types are: ImageBitmap - Optimized image class on modern browsers. Image  - Works on all browsers, less performant. \"image data object\" - an  ImageData  like object that hold the raw decoded bytes of an image. Works in both browsers and Node.js. Options Option Type Default Description image.mimeType string 'image/png' image output format image.jpegQuality `number null` image/jpeg: 0.92 ,  image/webp: 0.80 Image quality, between  0-1 . Only applies to  image/jpeg  and  image/webp . Remarks Supported image types (MIME types) depend on the browser / environment. As a rule,  image/png  and  image/jpeg  are always supported. (Unfortunately it is not currently clear how to determine what formats are available on any given browser, other than \"trial and error\"). jpegQuality  argument is not supported on all platforms (in which case it is assumed to have reasonable but platform-dependent defaults). The  ImageWriter  currently uses  canvas.toBlob()  on browsers, so referring to related documentation might be helpful.","headings":[{"value":"ImageWriter","depth":1},{"value":"Usage","depth":2},{"value":"Data Format","depth":2},{"value":"Options","depth":2},{"value":"Remarks","depth":2}],"slug":"modules/images/docs/api-reference/image-writer","title":"ImageWriter"},{"excerpt":"loadImages A function that loads an array of images. Primarily intended for loading: an array of images for a WebGL  TEXTURE_2D_ARRAY  or  TEXTURE_3D  textures an array of images representing mip levels of a single WebGL  TEXTURE_2D  texture or one  TEXTURE_CUBE  face. Usage Loading an array of images getUrl Callback Parameters the  getUrl  callback will be called for each image with the following parameters: Parameter Description index The index of the image being loaded, from  0  to  count - 1 . lod The mip level image being loaded, from  0  to  mipLevels - 1 . Note: In addition to these values, all  options  passed in to  loadImageArray  are also available in the  getUrl  method. loadImageArray(count : Number | String, getUrl : ({index}) => String, options? : Object) : image[] | image Parameters: count : Number of images to load. getUrl : A function that generates the url for each image, it is called for each image with the  index  of that image. options : Supports the same options as  ImageLoader . Returns an array of images (or array of arrays of mip images) Options Accepts the same options as  ImageLoader , and Option Type Default Description image.mipLevels `Number String` 0 If  'auto'  or non-zero, loads an array of mip images. Number of mip level images to load: Use  0  to indicate a single image with no mips. Supplying the string  'auto'  will infer the mipLevel from the size of the  lod = 0  image. Remarks Returned images can be passed directly to WebGL texture methods. See  ImageLoader  for details about the type of the returned images.","headings":[{"value":"loadImages","depth":1},{"value":"Usage","depth":2},{"value":"getUrl Callback Parameters","depth":2},{"value":"loadImageArray(count : Number | String, getUrl : ({index}) => String, options? : Object) : image[] | image","depth":3},{"value":"Options","depth":2},{"value":"Remarks","depth":2}],"slug":"modules/images/docs/api-reference/load-image-array","title":"loadImages"},{"excerpt":"Image Utilities A small set of image utility functions functions intended to help write image handling code that works across platforms. Background: The image returned by the  ImageLoader  depends on the environment, i.e. whether the application is running in a new or old browser, or under Node.js. Usage E.g., the  getImageData  method enables the application to get width, height and pixel data from an image returned by the  ImageLoader  in a platform independent way: Functions isImageTypeSupported(type : string) : boolean type : value to test Returns  true  if  type  is one of the types that  @loaders.gl/images  can use on the current platform (depends on browser, or whether running under Node.js). isImage(image : any) : boolean image : An image returned by an image category loader, such as  ImageLoader Returns  true  if  image  is one of the types that  @loaders.gl/images  can return. getImageType(image : any) : String Returns the type of an image. Can be used when loading images with the default setting of  options.type: 'auto'  to discover what type was actually returned. image : An image returned by an image category loader, such as  ImageLoader Returns a string describing the type of the image. Throws if  image  is not of a recognized type. Type JavaScript Type Description data Image data object:  data ,  width ,  height  .. Node.js representation imagebitmap ImageBitmap The newer HTML5 image class (modern browsers only) image Image  aka  HTMLImageElement More widely supported (but less performant and flexible) image class getImageData(image : any) : Object image : An image returned by an image category loader, such as  ImageLoader Returns and image data object with the following fields data  typed array containing the pixels of the image width height Throws if  image  is not of a recognized type.","headings":[{"value":"Image Utilities","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"isImageTypeSupported(type : string) : boolean","depth":3},{"value":"isImage(image : any) : boolean","depth":3},{"value":"getImageType(image : any) : String","depth":3},{"value":"getImageData(image : any) : Object","depth":3}],"slug":"modules/images/docs/api-reference/parsed-image-api","title":"Image Utilities"},{"excerpt":"ImageLoader An image loader that works under both Node.js (requires  @loaders.gl/polyfills ) and the browser. Loader Characteristic File Extension .png ,  .jpg ,  .jpeg ,  .gif ,  .webp ,  .bmp ,  .ico ,  .svg File Type Binary File Format JPEG, PNG, ... Data Format ImageBitmap ,  Image  or \"image data\" Supported APIs load ,  parse Worker Thread No (but may run on separate native thread in browsers) Streaming No Usage Data Format The  ImageLoader  parses binary encoded images (such as JPEG or PNG images) into one of three different in-memory representations: ImageBitmap  ( type: 'imagebitmap ) - Optimized image class on modern browsers. Image  ( type: 'image ) - Works on all browsers, less performant. An \"image data object\" ( type: 'data' ) - An  ImageData  like object that can holds the raw bytes to the image and works in both browsers and Node.js Options Option Type Default Description image.type String 'auto' Set to  imagebitmap ,  image  or  data  to explicitly control type of returned image.  auto  selects the most efficient supported format ( imagebitmap  on Chrome and Firefix) image.decode boolean true Applies to  image  type images only, ensures image is fully decoded before loading promise resolves. ImageBitmap Options In addition, for  imagebitmap  type images, it is possible to pass through options to  createImageBitmap  to control image extraction, via the separate  options.imagebitmap  object. However, for portability it may be best to avoid relying on these options for now, since some browsers do not support  ImageBitmap  options (and some browsers do not support  ImageBitmap s at all). Option Type Default Description imagebitmap.imageOrientation string 'none' image should be flipped vertically. Either  'none'  or  'flipY' . imagebitmap.premultiplyAlpha string 'default' Premultiply color channels by the alpha channel. One of  'none' ,  'premultiply' , or  'default' . imagebitmap.colorSpaceConversion string 'default' Decode using color space conversion. Either  'none'  or  'default'  default indicates implementation-specific behavior. imagebitmap.resizeWidth number - Output image width. imagebitmap.resizeHeight number - Output image height. imagebitmap.resizeQuality string 'low' Algorithm to be used for resizing the input to match the output dimensions. One of pixelated, low (default), medium, or high. Portability note: The exact set of  imagebitmap  options supported may depend on the browser. Remarks While generic, the  ImageLoader  is designed with WebGL applications in mind, ensuring that loaded image data can be used to create a  WebGLTexture  both in the browser and in headless gl under Node.js Node.js support requires import  @loaders.gl/polyfills  before installing this module.","headings":[{"value":"ImageLoader","depth":1},{"value":"Usage","depth":2},{"value":"Data Format","depth":2},{"value":"Options","depth":2},{"value":"ImageBitmap Options","depth":3},{"value":"Remarks","depth":2}],"slug":"modules/images/docs/api-reference/image-loader","title":"ImageLoader"},{"excerpt":"loadCubeImages A function that loads 6 images representing the faces of a cube. Primarily intended for loading images for WebGL  GL.TEXTURE_CUBE  textures. Usage Load images for a cubemap with one image per face Load images for a cubemap with an array of mip images per face getUrl Callback Parameters The following fields will be supplied as named parameters to the  getUrl  function when loading cube maps: faceIndex face direction axis sign 0 GL.TEXTURE_CUBE_MAP_POSITIVE_X  (0x8515) 'right' 'x' 'positive' 1 GL.TEXTURE_CUBE_MAP_NEGATIVE_X  (0x8516) 'left' 'x' 'negative' 2 GL.TEXTURE_CUBE_MAP_POSITIVE_Y  (0x8517) 'top' 'y' 'positive' 3 GL.TEXTURE_CUBE_MAP_NEGATIVE_Y  (0x8518) 'bottom' 'y' 'negative' 4 GL.TEXTURE_CUBE_MAP_POSITIVE_Z  (0x8519) 'front' 'z' 'positive' 5 GL.TEXTURE_CUBE_MAP_NEGATIVE_Z  (0x851a) 'back' 'z' 'negative' Note: In addition to these values, all  options  passed in to  loadImageCube  are also available in the  getUrl  method. loadImageCube(getUrl : ({face, direction, index}) => String, options? : Object) : Object Loads and image cube, i.e. 6 images keyed by WebGL face constants (see table). Parameters: getUrl : A function that generates the url for each image, it is called for each image with the  index  of that image. options : Supports the same options as  ImageLoader . Returns An object with 6 key/value pairs containing images (or arrays of mip images) for for each cube face. They keys are the (stringified) numeric values of the GL constant for the respective faces of the cube Options Accepts the same options as  ImageLoader , and Option Type Default Description image.mipLevels `Number String` 0 If  'auto'  or non-zero, loads an array of mip images. Number of mip level images to load: Use  0  to indicate a single image with no mips. Supplying the string  'auto'  will infer the mipLevel from the size of the  lod = 0  image. Remarks Returned images can be passed directly to WebGL texture methods. See  ImageLoader  for details about the type of the returned images.","headings":[{"value":"loadCubeImages","depth":1},{"value":"Usage","depth":2},{"value":"getUrl Callback Parameters","depth":2},{"value":"loadImageCube(getUrl : ({face, direction, index}) => String, options? : Object) : Object","depth":3},{"value":"Options","depth":2},{"value":"Remarks","depth":2}],"slug":"modules/images/docs/api-reference/load-image-cube","title":"loadCubeImages"},{"excerpt":"@loaders.gl/i3s (Experimental) This module contains a loader for  i3s  (Indexed SceneLayers). loaders.gl  is a collection of loaders for big data visualizations. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/i3s (Experimental)","depth":1}],"slug":"modules/i3s","title":"@loaders.gl/i3s (Experimental)"},{"excerpt":"loadImage Usage Function loadImage(getUrl : String | Function, options? : Object]) : image | image[] A basic image loading function for loading a single image (or an array of mipmap images representing a single image). getUrl : A function that generates the url for each image, it is called for each image with the  lod  of that image. options : Supports the same options as  ImageLoader . Returns image or array of images Options Accepts the same options as  ImageLoader , and Option Type Default Description image.mipLevels `Number String` 0 If  'auto'  or non-zero, loads an array of mip images. Number of mip level images to load: Use  0  to indicate a single image with no mips. Supplying the string  'auto'  will infer the mipLevel from the size of the  lod = 0  image.","headings":[{"value":"loadImage","depth":1},{"value":"Usage","depth":2},{"value":"Function","depth":2},{"value":"loadImage(getUrl : String | Function, options? : Object]) : image | image[]","depth":3},{"value":"Options","depth":2}],"slug":"modules/images/docs/api-reference/load-image","title":"loadImage"},{"excerpt":"Overview The  @loaders.gl/i3s  module supports loading and traversing Indexed 3d Scene Layer (I3S). References I3S Tiles Specification  - The living specification. I3S Tiles Standard  - The official standard from  OGC , the Open Geospatial Consortium. Installation API A standard complement of loader is provided to load the individual 3d Tile file formats: I3SLoader , a loader for loading a top-down or nested tileset and its tiles. To handle the complex dynamic tile selection and loading required to performantly render larger-than-browser-memory tilesets, additional helper classes are provided in  @loaders.gl/tiles  module: Tileset3D  to work with the loaded tileset. Tile3D  to access data for a specific tile. Remarks @loaders.gl/3s  is still under experimental, and mainly support decoding  MeshPyramids  (3D Object and Integrated Mesh) profiles. Attribution","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"API","depth":2},{"value":"Remarks","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/i3s/docs","title":"Overview"},{"excerpt":"Overview The  @loaders.gl/gltf  module provides loaders and writers of the GLB/glTF formats. Installation Optionally, to support Draco encoded gltf files GLTFScenegraph API To simplify traversing and building glTF data objects, the  GLTFScenegraph  class can be used. A glTF data object can also be built programmatically using the GLTFScenegraph's \"fluent API\": GLTF Post Processing The  postProcessGLTF  function implements a number of transformations on the loaded glTF data that would typically need to be performed by the application after loading the data, and is provided as an optional function that applications can call after loading glTF data. Refer to the reference page for details on what transformations are performed. Context: the glTF data object returned by the GLTF loader contains the \"raw\" glTF JSON structure (to ensure generality and \"data fidelity\" reasons). However, most applications that are going to use the glTF data to visualize it in (typically in WebGL) will need to do some processing of the loaded data before using it. Using GLB as a \"Binary Container\" for Arbitrary Data The GLB binary container format used by glTF addresses a general need to store a mix of JSON and binary data, and can potentially be used as a foundation for building custom loaders and writers. To allow for this (and also to generally improve the glTF code structure), the  GLTFLoader  and  GLTFBuilder  classes are built on top of GLB focused classes ( GLBLoader  and  GLBBuilder ) that can be used independently of the bigger glTF classes. glTF Extension Support Certain glTF extensions are fully or partially supported by the glTF classes. For details on which extensions are supported, see  glTF Extensions . Draco Mesh and Point Cloud Compression Draco encoding and decoding is supported by the  GLTFBuilder  and  GLTFParser  classes but requires the DracoWriter and DracoLoader dependencies to be \"injected\" by the application.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"GLTFScenegraph API","depth":2},{"value":"GLTF Post Processing","depth":2},{"value":"Using GLB as a \"Binary Container\" for Arbitrary Data","depth":2},{"value":"glTF Extension Support","depth":2},{"value":"Draco Mesh and Point Cloud Compression","depth":2}],"slug":"modules/gltf/docs","title":"Overview"},{"excerpt":"@loaders.gl/gltf loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loader and writers for the glTF format. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/gltf","depth":1}],"slug":"modules/gltf","title":"@loaders.gl/gltf"},{"excerpt":"GLBLoader The  GLBLoader  parses a GLB binary \"envelope\" extracting the embedded JSON and binary chunks. Note: applications that want to parse GLB-formatted glTF files would normally use the  GLTFLoader  instead. The  GLBLoader  can be used to load custom data that combines JSON and binary resources. Loader Characteristic File Extensions .glb File Type Binary File Format GLB v2 ,  GLB v1   * Data Format See below Supported APIs load ,  parse ,  parseSync *  From  , the  GLBLoader  can also load GLB v1 formatted files, returning a normalized GLB v2 compatible data structure, but with the  version  field set to  1 . Usage Options Option Type Default Description glb.strict  (DEPRECATED) Boolean false Whether to support non-standard JSON/BIN chunk type numbers. Data Format Returns Field Type Default Description type String glTF String containing the first four bytes of the file version Number 2 The version number, only version 2 is supported json Object {} Parsed JSON from the JSON chunk binChunks ArrayBuffer null The binary chunk binChunks[\\*].arrayBuffer ArrayBuffer null The binary chunk binChunks[\\*].byteOffset Number null offset of BIN (e.g. embedded in larger binary block) binChunks[\\*].byteLength ArrayBuffer null length of BIN (e.g. embedded in larger binary block) header.byteLength Number - length of GLB (e.g. embedded in larger binary block) header.byteOffset Number 0 offset of GLB (e.g. embedded in larger binary block)","headings":[{"value":"GLBLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Data Format","depth":2}],"slug":"modules/gltf/docs/api-reference/glb-loader","title":"GLBLoader"},{"excerpt":"I3SLoader The  I3SLoader  is experimental. Currently only support I3S  MeshPyramids  data format. A loader for loading an  Indexed 3d Scene (I3S) layer , and its geometries and textures data. Loader Characteristic File Format I3S Layer File Type Json, Binary File Extension .json  (layer),  .bin  (geometries) File Format i3s Data Format Data formats Supported APIs load ,  parse Terms The terms and concepts used in  i3s  module have the corresponding parts  I3S Spec . tileset : I3S Indexed 3D Layer File. tile : I3S node file. tileContent : I3S node content: geometries, textures, etc. Usage As an I3S tileset contains multiple file formats,  I3SLoader  is needed to be explicitly specified when using  load  function. Load I3S tileset and render with  deck.gl A simple react app umodules/3d-tiles/src/tiles-3d-loader.jsses  I3SLoader  to load  San Francisco Buildings , render with  deck.gl's   Tile3Dlayer  and dynamically load/unload tiles based on current viewport and adjust the level of details when zooming in and out. Example Codesandbox A more complex example could be found  here , checkout website  examples . Basic API Usage Basic API usage is illustrated in the following snippet. Create a  Tileset3D  instance, point it a valid tileset URL, set up callbacks, and keep feeding in new camera positions: Options Option Type Default Description options.i3s.isTileset Bool  or  auto auto Whether to load  Tileset  (Layer 3D Index) file. If  auto , will decide if follow  ArcGIS  tile layers' url convention options.i3s.isTileHeader Bool  or  auto auto Whether to load  TileHeader (node) file. If  auto , will decide if follow  argis  url convention options.i3s.loadContent Bool true Whether to load tile content (geometries, texture, etc.). Note: I3S dataset, each tile node has separate urls pointing to tile metadata and its actual tile payload. If  loadContent  is true, i3s loader will make a request to fetch the content fiile and decoded to the format as specified in  Tile Object . options.i3s.tileset Object null Tileset  object loaded by I3SLoader or follow the data format specified in  Tileset Object . It is required when loading i3s geometry content options.i3s.tile Object null Tile  object loaded by I3SLoader or follow the data format  Tile Object . It is required when loading i3s geometry content options.i3s.useDracoGeometry Bool true Use 'Draco' compressed geometry to show if applicable options.i3s.useCompressedTextures Bool true Use \"Compressed textures\" ( .dds or  .ktx) if available and supported by GPU Data formats This section specifies the loaded data formats. Tileset Object The following fields are guaranteed. Additionally, the loaded tileset object will contain all the data fetched from the provided url. Field Type Contents loader Object I3SLoader root Object The root tile header object url String The url of this tileset type String Value is  i3s . Indicates the returned object is an  i3s  tileset. lodMetricType String Root's level of detail (LoD) metric type, which is used to decide if a tile is sufficient for current viewport. Only support  maxScreenThreshold  for now. Check I3S  lodSelection  for more details. lodMetricValue Number Root's level of detail (LoD) metric value. Tile Object The following fields are guaranteed. Additionally, the loaded tile object will contain all the data fetched from the provided url. Field Type Contents url String The url of this tile. contentUrl String The url of this tile. featureUrl String The url of this tile. textureUrl String The url of this tile. boundingVolume Object A bounding volume in Cartesian coordinates converted from i3s node's  mbs  that encloses a tile or its content. Exactly one box, region, or sphere property is required. lodMetricType String Level of Detail (LoD) metric type, which is used to decide if a tile is sufficient for current viewport. Only support  maxScreenThreshold  for now. Check I3S  lodSelection  for more details. lodMetricValue String Level of Detail (LoD) metric value. children Array An array of objects that define child tiles. Each child tile content is fully enclosed by its parent tile's bounding volume and, generally, has more details than parent. for leaf tiles, the length of this array is zero, and children may not be defined. content String The actual payload of the tile or the url point to the actual payload. If  option.loadContent  is enabled, content will be populated with the loaded value following the Tile Content section id String Identifier of the tile, unique in a tileset refine String Refinement type of the tile, currently only support  REPLACE type String Type of the tile, value is  mesh  (currently only support  I3S MeshPyramids Tile Content After content is loaded, the following fields are guaranteed. But different tiles may have different extra content fields. Field Type Contents cartesianOrigin Number[3] \"Center\" of tile geometry in WGS84 fixed frame coordinates cartographicOrigin Number[3] \"Origin\" in lng/lat (center of tile's bounding volume) modelMatrix Number[16] Transforms tile geometry positions to fixed frame coordinates vertexCount Number Transforms tile geometry positions to fixed frame coordinates attributes Object Each attribute follows luma.gl  accessor  properties texture Object Loaded texture by  loaders.gl/image featureData Object Loaded feature data for parsing the geometies (Will be deprecated in 2.x) attributes  contains following fields Field Type Contents attributes.positions Object {value, type, size, normalized} attributes.normals Object {value, type, size, normalized} attributes.colors Object {value, type, size, normalized} attributes.texCoords Object {value, type, size, normalized}","headings":[{"value":"I3SLoader","depth":1},{"value":"Terms","depth":2},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Data formats","depth":2},{"value":"Tileset Object","depth":3},{"value":"Tile Object","depth":3},{"value":"Tile Content","depth":3}],"slug":"modules/i3s/docs/api-reference/i3s-loader","title":"I3SLoader"},{"excerpt":"GLBWriter The  GLBWriter  is a writer for the GLB binary \"envelope\" format. Note: applications that want to encode GLB-formatted glTF files should normally use the  GLTFWriter  instead. The  GLBWriter  enables applications to save custom data that combines JSON and binary resources. Loader Characteristic File Extensions .glb File Type Binary Data Format See below File Format GLB v2 Supported APIs encode ,  encodeSync Usage Options Option Type Default Description N/A N/A N/A N/A Data Format See  GLBLoader . Remarks While the  GLBLoader  supports reading both GLB v1 and v2, only GLB v2 can be written.","headings":[{"value":"GLBWriter","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Data Format","depth":2},{"value":"Remarks","depth":2}],"slug":"modules/gltf/docs/api-reference/glb-writer","title":"GLBWriter"},{"excerpt":"glTF Extensions glTF extensions can be present in glTF files, and will be present in the parsed JSON. glTF extensions can supported by applications by inspecting the  extensions  fields inside glTF objects, and it is up to each application to handle or ignore them. loaders.gl aims to provide support for glTF extensions that can be handled completely or partially during loading, and article describes glTF extensions that are fully or partially processed by the  @loaders.gl/gltf  classes. Note that many glTF extensions affect aspects that are firmly outside of the scope of loaders.gl (e.g. rendering), and no attempt is made to process those extensions in loaders.gl. Extension Description KHR_draco_mesh_compression KHR_lights_punctual KHR_materials_unlit Official Extensions KHR_draco_mesh_compression Supports compression of mesh attributes (geometry). Specification:  KHR_draco_mesh_compression . Parsing Support: By adding the  decompress: true  options to the  GLTFParser  any decompressed by the  GLTFParser . The expanded attributes are placed in the mesh object (effectively making it look as if it had never been compressed). The extension objects are removed from the glTF file. Encoding Support: Meshes can be compressed as they are added to the  GLTFBuilder . KHR_lights_punctual Supports specification of point light sources and addition of such sources to the scenegraph node. Specification:  KHR_lights_punctual Parsing Support: Any nodes with a  KHR_lights_punctual  extension will get a  light  field with value containing a light definition object with properties defining the light (this object will be resolved by index from the global  KHR_lights_punctual  extension object's  lights  array) . The  KHR_lights_punctual  extensions will be removed from all nodes. Finally, the global  KHR_lights_punctual  extension (including its light list)) will be removed. Encoding Support: N/A KHR_materials_unlit Specifies that a material should not be affected by light. Useful for pre-lit materials (e.g. photogrammetry). KHR_materials_unlit Custom Extensions UBER_draco_point_cloud_compression Specification: Similar to  KHR_draco_mesh_compression , but supports point clouds (draw mode 0). Also does not support any fallback or non-compressed accessors/attributes. Parsing support: The primitive's accessors field will be populated after decompression. After decompression, the extension will be removed (as if the point cloud was never compressed). Encoding support: Point clouds can be compressed as they are added to the  GLTFBuilder  and decompressed by the  GLTFParser .","headings":[{"value":"glTF Extensions","depth":1},{"value":"Official Extensions","depth":2},{"value":"KHR_draco_mesh_compression","depth":3},{"value":"KHR_lights_punctual","depth":3},{"value":"KHR_materials_unlit","depth":3},{"value":"Custom Extensions","depth":2},{"value":"UBER_draco_point_cloud_compression","depth":3}],"slug":"modules/gltf/docs/api-reference/gltf-extensions","title":"glTF Extensions"},{"excerpt":"GLTFLoader Parses a glTF file. Can load both the  .glb  (binary) and  .gltf  (application/json) file format variants. A glTF file contains a hierarchical scenegraph description that can be used to instantiate corresponding hierarcy of actual  Scenegraph  related classes in most WebGL libraries. Loader Characteristic File Extensions .glb ,  .gltf File Type Binary, JSON, Linked Assets File Format glTF v2 ,  GLTF v1   * Data Format Scenegraph Supported APIs load ,  parse Subloaders DracoLoader ,  ImageLoader *  From  , the  GLTFLoader  offers optional, best-effort support for converting older glTF v1 files to glTF v2 format ( options.gltf.normalize: true ). This conversion has a number of limitations and the parsed data structure may be only partially converted to glTF v2, causing issues to show up later e.g. when attempting to render the scenegraphs. Usage To decompress Draco-compressed meshes: Overview The  GLTFLoader  aims to take care of as much processing as possible, while remaining framework-independent. The GLTF Loader returns an object with a  json  field containing the glTF Scenegraph. In its basic mode, the  GLTFLoader  does not modify the loaded JSON in any way. Instead, the results of additional processing are placed in parallel top-level fields such as  buffers  and  images . This ensures that applications that want to work with the standard glTF data structure can do so. Optionally, the loaded gltf can be \"post processed\", which lightly annotates and transforms the loaded JSON structure to make it easier to use. Refer to  postProcessGLTF  for details. In addition, certain glTF extensions, in particular Draco mesh encoding, can be fully or partially processed during loading. When possible (and extension processing is enabled), such extensions will be resolved/decompressed and replaced with standards conformant representations. See  glTF Extensions  for more information. Note: while supported, synchronous parsing of glTF (e.g. using  parseSync() ) has significant limitations. When parsed asynchronously (using  await parse()  or  await load() ), the following additional capabilities are enabled: linked binary resource URI:s will be loaded and resolved (assuming a valid base url is available). base64 encoded binary URI:s inside the JSON payload will be decoded. linked image URI:s can be loaded and decoded. Draco meshes can be decoded asynchronously on worker threads (in parallel!). Options Option Type Default Description gltf.loadBuffers Boolean false Fetch any referenced binary buffer files (and decode base64 encoded URIS). gltf.loadImages Boolean false Load any referenced image files (and decode base64 encoded URIS). gltf.decompressMeshes Boolean true Decompress Draco compressed meshes (if DracoLoader available). gltf.postProcess Boolean true Perform additional post processing on the loaded glTF data. gltf.normalize Boolean false Optional, best-effort attempt at converting glTF v1 files to glTF2 format. Remarks: The  gltf.postProcess  option activates additional  post processing  that transforms parts of JSON structure in the loaded glTF data, to make glTF data easier use in applications and WebGL libraries (e.g replacing indices with links to the indexed objects). However, the data structure returned by the  GLTFLoader  will no longer be fully glTF compatible. Data Format With Post Processing When the  GLTFLoader  is called with  gltf.postProcess  option set to  true  (the default),the parsed JSON chunk will be returned, and  post processing  will have been performed, which will link data from binary buffers into the parsed JSON structure using non-standard fields, and also modify the data in other ways to make it easier to use. At the top level, this will look like a standard glTF JSON structure: However, the objects inside these arrays will have been pre-processed to simplify usage. For details on changes and extra fields added to the various glTF objects, see  post processing . Without Post Processing By setting  gltf.postProcess  to  false , an unprocessed glTF/GLB data structure will be returned, with binary buffers provided as an  ArrayBuffer  array. Field Type Default Description baseUri String `` length of GLB (e.g. embedded in larger binary block) json Object {} Parsed JSON from the JSON chunk buffers Object[] [] The version number buffers[\\*].arrayBuffer ArrayBuffer null The binary chunk buffers[\\*].byteOffset Number null offset of buffer (embedded in larger binary block) buffers[\\*].byteLength ArrayBuffer null length of buffer (embedded in larger binary block) _glb ? Object N/A The output of the GLBLoader if the parsed file was GLB formatted","headings":[{"value":"GLTFLoader","depth":1},{"value":"Usage","depth":2},{"value":"Overview","depth":2},{"value":"Options","depth":2},{"value":"Data Format","depth":2},{"value":"With Post Processing","depth":3},{"value":"Without Post Processing","depth":3}],"slug":"modules/gltf/docs/api-reference/gltf-loader","title":"GLTFLoader"},{"excerpt":"GLTFScenegraph The  GLTFScenegraph  class provides an API for accessing and modifying glTF data. Caveat: Modification of binary data chunks has limitations, and this class is currently not intended to be a generic utility for modifying glTF data. Usage Accessing Modifying Fields gltf: Object Contains all data of gltf including the json part and the binary chunk. The binary chunk is generated by  createBinChunk()  method sourceBuffers: Array < ArrayBuffer | { arrayBuffer?: ArrayBuffer; buffer?: ArrayBuffer; byteOffset: number; byteLength: number} > Accumulates buffers of resources added by  addMesh() ,  addImage()  etc. These buffers should be compiled before encoding using  createBinaryChunk() byteLength: number Total byteLength of the binary part of gltf Accessor Methods constructor(gltf : Object) Creates a new  GLTFScenegraph  instance from a pure JavaScript object. json() getApplicationData(key : String) : Object Returns the given data field in the top-level glTF JSON object. getExtraData(key : String) : Object? Returns a key in the top-level glTF  extras  JSON object. getExtension(name : String) : Object? Returns the top-level extension by  name , if present. getUsedExtensions() : String[] Returns an array of extension names (covering all extensions used at any level of the glTF hierarchy). getRequiredExtensions() : String[] Returns an array of extensions at any level of the glTF hierarchy that are required to properly display this file (covering all extensions used at any level of the glTF hierarchy). getObjectExtension(object, extensionName) getScene( index : Number ) : Object? Returns the scene (scenegraph) with the given index, or the default scene if no index is specified. getScene(index : Number) : Object getNode(index : Number) : Object getSkin(index : Number) : Object getMesh(index : Number) : Object getMaterial(index : Number) : Object getAccessor(index : Number) : Object getCamera(index : Number) : Object getTexture(index : Number) : Object getSampler(index : Number) : Object getImage(index : Number) : Object Returns the image with specified index getBufferView(index : Number) : Object getBuffer(index : Number) : Object getTypedArrayForBufferView(bufferView : Number | Object) : Uint8Array Accepts buffer view index or buffer view object getTypedArrayForAccessor(accessor : Number | Object) : Uint8Array | Float32Array | ... Accepts accessor index or accessor object. Returns a typed array with type that matches the types getTypedArrayForImageData(image : Number | Object) : Uint8Array accepts accessor index or accessor object Modifiers addApplicationData(key, data) Add an extra application-defined key to the top-level data structure addExtraData(key, data) extras  - Standard GLTF field for storing application specific data Add to GLTF top level extension object, mark as used addRequiredExtension(extensionName, data) Add GLTF top level extension object, mark as used and required registerUsedExtension(extensionName) Add extensionName to list of used extensions registerRequiredExtension(extensionName) Add extensionName to list of required extensions removeExtension(extensionName) Removes an extension from the top-level list setObjectExtension(object, extensionName, data) setDefaultScene(sceneIndex: number); Set the default scene which is to be displayed at load time addScene(arguments: {nodeIndices: number[]}): number; Add a scene to the json part addNode(arguments: {meshIndex: number}): number; Add a node to the json part addMesh(arguments: {attributes: object, indices: object, material: number, mode = 4}) addPointCloud(attributes) addBufferView(buffer) Add one untyped source buffer, create a matching glTF  bufferView , and return its index The binary data will not be added to the gltf buffer until  createBinChunk()  is called. addTexture(arguments: {imageIndex: number}): number; Add a texture to the json part addMaterial(pbrMaterialInfo: Object): number; Add a material to the json part addAccessor(bufferViewIndex, accessor) Adds an accessor to a bufferView The binary data will not be added to the gltf buffer until  createBinChunk()  is called. addImage(imageData, mimeType) Adds a binary image. Builds glTF \"JSON metadata\" and saves buffer reference\nBuffer will be copied into BIN chunk during \"pack\" The binary data will not be added to the gltf buffer until  createBinChunk()  is called. createBinChunk() Packs any pending binary data into the first binary glTF buffer. Note: Overwrites the existing first buffer if present.","headings":[{"value":"GLTFScenegraph","depth":1},{"value":"Usage","depth":2},{"value":"Accessing","depth":3},{"value":"Modifying","depth":3},{"value":"Fields","depth":2},{"value":"gltf: Object","depth":3},{"value":"sourceBuffers: Array<ArrayBuffer | { arrayBuffer?: ArrayBuffer; buffer?: ArrayBuffer; byteOffset: number; byteLength: number}>","depth":3},{"value":"byteLength: number","depth":3},{"value":"Accessor Methods","depth":2},{"value":"constructor(gltf : Object)","depth":3},{"value":"json()","depth":3},{"value":"getApplicationData(key : String) : Object","depth":3},{"value":"getExtraData(key : String) : Object?","depth":3},{"value":"getExtension(name : String) : Object?","depth":3},{"value":"getUsedExtensions() : String[]","depth":3},{"value":"getRequiredExtensions() : String[]","depth":3},{"value":"getObjectExtension(object, extensionName)","depth":3},{"value":"getScene(index : Number) : Object?","depth":3},{"value":"getScene(index : Number) : Object","depth":3},{"value":"getNode(index : Number) : Object","depth":3},{"value":"getSkin(index : Number) : Object","depth":3},{"value":"getMesh(index : Number) : Object","depth":3},{"value":"getMaterial(index : Number) : Object","depth":3},{"value":"getAccessor(index : Number) : Object","depth":3},{"value":"getCamera(index : Number) : Object","depth":3},{"value":"getTexture(index : Number) : Object","depth":3},{"value":"getSampler(index : Number) : Object","depth":3},{"value":"getImage(index : Number) : Object","depth":3},{"value":"getBufferView(index : Number) : Object","depth":3},{"value":"getBuffer(index : Number) : Object","depth":3},{"value":"getTypedArrayForBufferView(bufferView : Number | Object) : Uint8Array","depth":3},{"value":"getTypedArrayForAccessor(accessor : Number | Object) : Uint8Array | Float32Array | ...","depth":3},{"value":"getTypedArrayForImageData(image : Number | Object) : Uint8Array","depth":3},{"value":"Modifiers","depth":2},{"value":"addApplicationData(key, data)","depth":3},{"value":"addExtraData(key, data)","depth":3},{"value":"addRequiredExtension(extensionName, data)","depth":3},{"value":"registerUsedExtension(extensionName)","depth":3},{"value":"registerRequiredExtension(extensionName)","depth":3},{"value":"removeExtension(extensionName)","depth":3},{"value":"setObjectExtension(object, extensionName, data)","depth":3},{"value":"setDefaultScene(sceneIndex: number);","depth":3},{"value":"addScene(arguments: {nodeIndices: number[]}): number;","depth":3},{"value":"addNode(arguments: {meshIndex: number}): number;","depth":3},{"value":"addMesh(arguments: {attributes: object, indices: object, material: number, mode = 4})","depth":3},{"value":"addPointCloud(attributes)","depth":3},{"value":"addBufferView(buffer)","depth":3},{"value":"addTexture(arguments: {imageIndex: number}): number;","depth":3},{"value":"addMaterial(pbrMaterialInfo: Object): number;","depth":3},{"value":"addAccessor(bufferViewIndex, accessor)","depth":3},{"value":"addImage(imageData, mimeType)","depth":3},{"value":"createBinChunk()","depth":3}],"slug":"modules/gltf/docs/api-reference/gltf-scenegraph","title":"GLTFScenegraph"},{"excerpt":"GLTFWriter The  GLTFWriter  is a writer for glTF scenegraphs. Loader Characteristic File Extensions .glb , .gltf File Types Binary, JSON, Linked Assets Data Format Scenegraph File Format glTF Supported APIs encode ,  encodeSync Usage Options Option Type Default Description DracoWriter DracoWriter null To enable DRACO encoding, the application needs to import and supply the  DracoWriter  class. DracoLoader DracoLoader null To enable DRACO encoding, the application needs to import and supply the  DracoLoader  class.","headings":[{"value":"GLTFWriter","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/gltf/docs/api-reference/gltf-writer","title":"GLTFWriter"},{"excerpt":"@loaders.gl/flatgeobuf This module contains a geometry loader for FlatGeobuf. loaders.gl  is a collection of framework independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/flatgeobuf","depth":1}],"slug":"modules/flatgeobuf","title":"@loaders.gl/flatgeobuf"},{"excerpt":"glbdump glbdump  is a utility for inspecting the structure of GLB/glTF binary container files. Installing loaders.gl/gltf makes the  glbdump  command line tool available. It can be run using  npx .","headings":[{"value":"glbdump","depth":2}],"slug":"modules/gltf/docs/api-reference/glbdump","title":"glbdump"},{"excerpt":"postProcessGLTF The  postProcessGLTF  function transforms parsed GLTF JSON to make it easier to use. It adds loaded buffers and images to the glTF JSON objects It creates typed arrays for buffer views Usage Postprocessing is done by default by the  GLTFLoader : To turn post processing off, and then optionally post process via  postProcessGLTF  function: After post-processing, the gltf scenegraphs are now easier to iterate over as indices have been resolved to object references: Functions postProcessGLTF(gltf : Object, options? : Object) : Object gltf  is expected to have  json  and  buffers  fields per the GLTF Data Format Category. options.uri  - Set base URI (for image loading) The GLTF post processor copies objects in the input gltf json field as necessary to avoid modifying the input JSON, but does not do a deep copy on sub-objects that do not need to be modified. General Post Processing Replace indices with references The first thing that  postProcessGLTF  does is replace glTF indices with object references to simplify iteration over the scenegraph. Background: The GLTF file format describes a tree structure, however it links nodes through numeric indices rather than direct references. (As an example the  nodes  field in the top-level glTF  scenegraph  array is an array of indices into the top-level  nodes  array. Each node has a  mesh  attribute that is an index into to the  meshes  array, and so on). Adds  id  to every node The postprocessor makes sure each node and an  id  value, unless already present. Node Specific Post Processing Buffers The following fields will be populated from the supplied  gltf.buffers  parameter (this parameter is populated by the loader via  options.loadLinkedResources: true ): buffer.arrayBuffer  - buffer.byteOffset  - buffer.byteLength  - BufferViews bufferView.data  - Typed arrays ( Uint8Arrays ) will be created for buffer views and stored in this field. These typed arrays can be used to upload data to WebGL buffers. Accessors The accessor parameters which are textual strings in glTF will be resolved into WebGL constants (which are just numbers, e.g.  5126  =  GL.FLOAT ), to prepare for use with WebGL frameworks. accessor.value  - This will be set to a typed array that is a view into the underlying bufferView. Remarks: While it can be very convenient to initialize WebGL buffers from  accessor.value , this approach will defeat any memory sharing on the GPU that the glTF file specifies through accessors sharing  bufferViews . The canonical way of instantitating a glTF model is for an application to create one WebGL buffer for each  bufferView  and then use accessors to reference data chunks inside those WebGL buffers with  offset  and  stride . Images image.image  - Populated from the supplied  gltf.images  array. This array is populated by the  GLTFLoader  via  options.loadImages: true ): image.uri  - If loaded image in the  images  array is not available, uses  gltf.baseUri  or  options.baseUri  is available, to resolve a relative URI and replaces this value. Materials ...texture  - Since each texture object in the material has an  ...index  field next to other fields, the post processor will add a  ...texture  field instead of replacing the  ...index  field. Samplers Modifies parameters  - see table Sampler parameters (which are textual in glTF) will be resolved into WebGL constants. glTF constant WebGL constant magFilter GL.TEXTURE_MAG_FILTER minFilter GL.TEXTURE_MIN_FILTER wrapS GL.TEXTURE_WRAP_S wrapT GL.TEXTURE_WRAP_T Texture Modifies sampler  - will be resolved the the corresponding image object. source  - will be resolved the the corresponding image object.","headings":[{"value":"postProcessGLTF","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"postProcessGLTF(gltf : Object, options? : Object) : Object","depth":3},{"value":"General Post Processing","depth":2},{"value":"Replace indices with references","depth":3},{"value":"Adds id to every node","depth":3},{"value":"Node Specific Post Processing","depth":2},{"value":"Buffers","depth":3},{"value":"BufferViews","depth":3},{"value":"Accessors","depth":3},{"value":"Images","depth":2},{"value":"Materials","depth":3},{"value":"Samplers","depth":3},{"value":"Texture","depth":3}],"slug":"modules/gltf/docs/api-reference/post-process-gltf","title":"postProcessGLTF"},{"excerpt":"@loaders.gl/gis This module contains helper classes for the GIS category of loaders. loaders.gl  is a collection of framework-independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/gis","depth":1}],"slug":"modules/gis","title":"@loaders.gl/gis"},{"excerpt":"@loaders.gl/gis This module contains helper classes for the GIS category of loaders. Installation Utility Functions Utility Functions geojson-to-binary","headings":[{"value":"@loaders.gl/gis","depth":1},{"value":"Installation","depth":2},{"value":"Utility Functions","depth":2}],"slug":"modules/gis/docs","title":"@loaders.gl/gis"},{"excerpt":"GeoJSON to TypedArrays Helper function to transform an array of GeoJSON  Feature s into binary typed\narrays. This is designed to speed up geospatial loaders by removing the need for\nserialization and deserialization of data transferred by the worker back to the\nmain process. Usage Outputs geojsonToBinary  returns an object containing typed arrays sorted by geometry\ntype.  positions  is a flat array of coordinates;  globalFeatureIds  references\nindices in the original  features  array;  featureIds  references feature\nindices of the same geometry type;  numericProps  contains  TypedArray s\ngenerated from numeric feature properties;  properties  is an array of\nnon-numeric property objects of the given geometry type. Each  TypedArray  is wrapped inside an  accessor object , where  .value  contains the raw numeric data, and  .size  gives the number of values per vertex. For example, corresponds to 3D coordinates, where each vertex is defined by three numbers. Options Option Type Default Description PositionDataType class Float32Array Data type used for positions arrays. numericPropKeys Array Derived from data Names of feature properties that should be converted to numeric  TypedArray s. Passing  []  will force all properties to be returned as normal objects. coordLength number Derived from data Number of dimensions per coordinate. Notes In the case of the source geoJson features having an object as a property, it would not be deep cloned, so it would be linked from the output object (be careful on further mutations).","headings":[{"value":"GeoJSON to TypedArrays","depth":1},{"value":"Usage","depth":2},{"value":"Outputs","depth":2},{"value":"Options","depth":2},{"value":"Notes","depth":2}],"slug":"modules/gis/docs/api-reference/geojson-to-binary","title":"GeoJSON to TypedArrays"},{"excerpt":"@loaders.gl/json This module contains a table loader for the JSON and line delimited JSON formats. loaders.gl  is a collection of framework-independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/json","depth":1}],"slug":"modules/excel","title":"@loaders.gl/json"},{"excerpt":"Overview The  @loaders.gl/excel  module handles tabular data stored in the Excel file format. Installation Loaders and Writers Loader ExcelLoader Additional APIs See table category.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Additional APIs","depth":2}],"slug":"modules/excel/docs","title":"Overview"},{"excerpt":"ExcelLoader (Non-streaming) table loader for Excel files. Loader Characteristic File Extension .xls ,  .xlsb ,  .xlsx File Type Binary File Format Excel Data Format Classic Table Supported APIs load ,  parse Usage Options Option From Type Default Description excel.sheet [ ] `string null` null Which worksheet to load. By default loads first sheet Attribution The  ExcelLoader  is a wrapper around  SheetJS  which is Apache 2.0 licensed.","headings":[{"value":"ExcelLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/excel/docs/api-reference/excel-loader","title":"ExcelLoader"},{"excerpt":"@loaders.gl/draco loaders.gl  is a collection of framework-independent 3D and geospatial parsers and encoders. This module contains loader and writer for Draco compressed meshes and point clouds. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/draco","depth":1}],"slug":"modules/draco","title":"@loaders.gl/draco"},{"excerpt":"Overview The  @loaders.gl/draco  module handles compressing and decompressing of 3D meshes and point clouds with  DRACO . Installation Loaders and Writers Loader DracoLoader DracoWorkerLoader DracoWriter Additional APIs See point cloud / mesh category. Dependencies Draco support requires the Draco libraries, which are quite big (see table below). By default, these will be loaded from CDN but can optionally be bundled and supplied by the application through the top-level  options.modules  option: Bundling the entire  draco3d  library: Bundling only the WebAssembly decoder Library Import Install Size Description options.libs.draco3d require('draco3d') npm install draco3d ~1.5MB The full Draco library (encode + decode, web assembly + IE11 javascript fallback). options.libs['draco_decoder.wasm'] ArrayBuffer ~320K manual copy Web Assembly Decoder (access using  draco_wasm_wrapper.js ) options.libs['draco_wasm_wrapper.js'] require('.../draco_decoder.js') ~64K manual copy JavaScript wrapper for  draco_decoder.wasm options.libs['draco_decoder.js'] require('.../draco_decoder.js') ~790K manual copy JavaScript decoder (fallback for IE11) options.libs['draco_encoder.js'] require('.../draco_encode.js') ~900K manual copy Encoder part of the library Remarks Due to the size of the Draco libraries, a reasonable strategy for applications that wish to bundle their dependencies (e.g to avoid relying on a potentially flaky CDN) might be to bundle and supply only  draco_decoder.wasm  and  draco_wasm_wrapper.js , and still rely on the default setup to load the IE11 fallback library and the encoder code from CDN when needed. Web Assembly code ( wasm  files) must be imported/loaded as binary data ( ArrayBuffer ). An option for webpack users is the  arraybuffer-loader  webpack \"loader\". Attributions Based on Draco examples, under the Apache 2.0 license.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Additional APIs","depth":2},{"value":"Dependencies","depth":2},{"value":"Attributions","depth":2}],"slug":"modules/draco/docs","title":"Overview"},{"excerpt":"DracoWriter The  DracoWriter  encodes a mesh or point cloud (maps of attributes) using  Draco3D  compression. Loader Characteristic File Extension .drc File Typoe Binary Data Format Mesh File Format Draco Encoder Type Synchronous Worker Thread Support Yes Streaming Support No Usage Options Option Type Default Description pointcloud Boolean false set to  true  to compress pointclouds (mode= 0  and no  indices ). method String MESH_EDGEBREAKER_ENCODING set Draco encoding method (applies to meshes only). speed Number, Number set Draco speed options. log Function callback for debug info.","headings":[{"value":"DracoWriter","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/draco/docs/images/draco-writer","title":"DracoWriter"},{"excerpt":"DracoWriter The  DracoWriter  encodes a mesh or point cloud using  Draco  compression. Loader Characteristic File Extension .drc File Type Binary File Format Draco Data Format Mesh Support API encode Usage Options Option Type Default Description draco.pointcloud Boolean false Whether to compress as point cloud (GL.POINTS) draco.speed Number Speed vs Quality, see  Draco  documentation draco.method String Compression method, see  Draco  documentation draco.quantization [Number, Number] Quantization parameters, see  Draco  documentation Dependencies Draco libraries by default are loaded from CDN, but can be bundled and injected. See  modules/draco/docs  for details.","headings":[{"value":"DracoWriter","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Dependencies","depth":2}],"slug":"modules/draco/docs/api-reference/draco-writer","title":"DracoWriter"},{"excerpt":"@loaders.gl/csv This module contains a table loader for the CSV and DSV formats. loaders.gl  is a collection of framework-independent visualization-focused loaders (parsers).","headings":[{"value":"@loaders.gl/csv","depth":1}],"slug":"modules/csv","title":"@loaders.gl/csv"},{"excerpt":"Overview The  @loaders.gl/csv  module handles tabular data stored in the  CSV/DSV file format . Installation Loaders and Writers Loader CSVLoader CSVWorkerLoader Additional APIs See table category. Attributions CSVLoader is based on a fork of the  papaparse  module, under MIT license.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Additional APIs","depth":2},{"value":"Attributions","depth":2}],"slug":"modules/csv/docs","title":"Overview"},{"excerpt":"DracoLoader The  DracoLoader  decodes a mesh or point cloud (maps of attributes) using  DRACO  compression. Loader Characteristic File Extension .drc File Type Binary File Format Draco Data Format Mesh Supported APIs parse Usage Options Option Type Default Description Dependencies Draco libraries by default are loaded from CDN, but can be bundled and injected. See  modules/draco/docs  for details.","headings":[{"value":"DracoLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Dependencies","depth":2}],"slug":"modules/draco/docs/api-reference/draco-loader","title":"DracoLoader"},{"excerpt":"CSVLoader Streaming loader for comma-separated value and  delimiter-separated value  encoded files. Loader Characteristic File Extension .csv ,  .dsv File Type Text File Format RFC4180 Data Format Classic Table Supported APIs load ,  parse ,  parseSync ,  parseInBatches Usage Options Option Type Default Description csv.header Boolean  |  String auto If  true , the first row of parsed data will be interpreted as field names. If  false , the first row is interpreted as data. csv.rowFormat String auto Can be set to  'object'  go force rows to be objects, or  'auto' . csv.columnPrefix String column The prefix to use when naming columns for CSV files with no header. Defaults to 'column1', 'column2' etc. csv.delimiter String auto-detect The delimiting character. csv.newline String auto-detect The newline sequence. Must be  \\r ,  \\n , or  \\r\\n . csv.quoteChar String \" The character used to quote fields. csv.escapeChar String \" The character used to escape the quote character within a field. csv.dynamicTyping Boolean true If  true , numeric and boolean data values will be converted to their type (instead if strings.  Note : if you disable  dynamicTyping , you need to explicitly set  header  to a boolean value. Otherwise,  header: 'auto'  would automatically treat the first row as a header. csv.comments String false Comment indicator (for example, \"#\" or \"//\"). Lines starting with this string are skipped. csv.skipEmptyLines String false If  true , lines that are completely empty (those which evaluate to an empty string) will be skipped. If set to  'greedy' , lines that don't have any content (those which have only whitespace after parsing) will also be skipped. csv.transform Function - A function to apply on each value. The function receives the value as its first argument and the column number or header name when enabled as its second argument. The return value of the function will replace the value it received. The transform function is applied before dynamicTyping. csv.delimitersToGuess Array `[',', '\\t', ' ', ';']` An array of delimiters to guess from if the  delimiter  option is not set. csv.fastMode Boolean auto-detect Force set \"fast mode\". Fast mode speeds up parsing significantly for large inputs but only works when the input has no quoted fields. Fast mode will be auto enabled if no  \"  characters appear in the input. Remarks: A complication with the CSV format is that CSV files can come with or without an initial header line. Use  options.csv.header  to specify how to handle the first line. Many options are passed on to papaparse, so the  papaparse docs  can serve as a source for more information.","headings":[{"value":"CSVLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/csv/docs/api-reference/csv-loader","title":"CSVLoader"},{"excerpt":"Overview The  @loaders.gl/crypto  module provides a selection of optional cryptographic hash plugins for loaders.gl. Cryptographic Formats MD5, SHA256 and many more, see  crypto-js Cryptographic Hash API The API offers \"transforms\" that can calculate a cryptographic hash incrementally on data as it comes in on a stream. Transforms Sync Description CRC32HashTransform Y Base64-encoded Cryptographic Hash CRC32CHashTransform Y Base64-encoded Cryptographic Hash MD5HashTransform Y Base64-encoded Cryptographic Hash CryptographicHashTransform Y Base64-encoded Cryptographic Hash Using Transforms The  @loaders.gl/crypto  libraries exports transform that can be used to incrementally calculate a cryptographic hash as data is being loaded and parsed: Note that by using a transform, the hash is calculated incrementally as batches are loaded and parsed, and does not require having the entire data source loaded into memory. It also distributes the potentially heavy hash calculation over the batches, keeping the main thread responsive. Performance Note that cryptographic hashing is a computationally expensive operation, linear in the size of the data being hashed. Hashing speeds are currently in the order of ~20-30MB/s on 2019 Macbook Pros.","headings":[{"value":"Overview","depth":1},{"value":"Cryptographic Formats","depth":2},{"value":"Cryptographic Hash API","depth":2},{"value":"Using Transforms","depth":2},{"value":"Performance","depth":2}],"slug":"modules/crypto/docs","title":"Overview"},{"excerpt":"@loaders.gl/crypto This module contains compression/decompression \"transforms\" for loaders.gl, a collection of framework-independent 3D and geospatial loaders (parsers). For documentation please visit the  website .","headings":[{"value":"@loaders.gl/crypto","depth":1}],"slug":"modules/crypto","title":"@loaders.gl/crypto"},{"excerpt":"CRC32HashTransform Static Methods CRC32HashTransform.run(data: ArrayBuffer, options?: object): Promise<string> Calculates the CRC32 hash of a byte array. Remarks This transform supports streaming hashing.","headings":[{"value":"CRC32HashTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"CRC32HashTransform.run(data: ArrayBuffer, options?: object): Promise<string>","depth":4},{"value":"Remarks","depth":2}],"slug":"modules/crypto/docs/api-reference/crc32-hash-transform","title":"CRC32HashTransform"},{"excerpt":"CRC32CHashTransform Static Methods CRC32CHashTransform.run(data: ArrayBuffer, options?: object): Promise<string> Calculates the CRC32c hash of a byte array. Remarks This transform supports streaming hashing.","headings":[{"value":"CRC32CHashTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"CRC32CHashTransform.run(data: ArrayBuffer, options?: object): Promise<string>","depth":4},{"value":"Remarks","depth":2}],"slug":"modules/crypto/docs/api-reference/crc32c-hash-transform","title":"CRC32CHashTransform"},{"excerpt":"MD5HashTransform Static Methods MD5HashTransform.run(data: ArrayBuffer, options?: object): Promise<string> Calculates the MD5 hash of a byte array. Remarks This transform does not yet support streaming hashing. Only the static method is defined.","headings":[{"value":"MD5HashTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"MD5HashTransform.run(data: ArrayBuffer, options?: object): Promise<string>","depth":4},{"value":"Remarks","depth":2}],"slug":"modules/crypto/docs/api-reference/md5-hash-transform","title":"MD5HashTransform"},{"excerpt":"@loaders.gl/core This module contains shared utilities for loaders.gl, a collection of framework-independent 3D and geospatial loaders (parsers). For documentation please visit the  website .","headings":[{"value":"@loaders.gl/core","depth":1}],"slug":"modules/core","title":"@loaders.gl/core"},{"excerpt":"Overview The  @loaders.gl/core  module contains the core API of loaders.gl The core API offers functions to parse data in various ways using loaders parse parseSync parseInBatches To fetch data fetchFile To load (fetch and parse) data load To register loaders, or select a loader that matches a file from a list of candidate loaders: registerLoaders selectLoader To encode and save data encode write-file save As well as some utility functions.","headings":[{"value":"Overview","depth":1}],"slug":"modules/core/docs","title":"Overview"},{"excerpt":"CryptoHashTransform The  CryptoHashTransform  is a wrapper around  crypto-js  which is an archived project. Make your choices around whether and how to use this class accordingly (i.e. building on a module that is not actively maintenaned is not ideal for security related algorithms). Static Methods CryptoHashTransform.run(data: ArrayBuffer, options?: object): Promise<string> options.modules.CryptoJS  the CryptoJS library needs to be supplied by the application. Remarks This transform supports streaming hashing.","headings":[{"value":"CryptoHashTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"CryptoHashTransform.run(data: ArrayBuffer, options?: object): Promise<string>","depth":4},{"value":"Remarks","depth":2}],"slug":"modules/crypto/docs/api-reference/crypto-hash-transform","title":"CryptoHashTransform"},{"excerpt":"encode Functions encode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object  , url : String ]) : Promise.Any Encodes data asynchronously using the provided writer. data  - loaded data, either in binary or text format. writer  - can be a single writer or an array of writers. options  - optional, options for the writer (see documentation of the specific writer). url  - optional, assists in the autoselection of a writer if multiple writers are supplied to  writer . options.log = console  Any object with methods  log ,  info ,  warn  and  error . By default set to  console . Setting log to  null  will turn off logging. encodeSync(fileData : ArrayBuffer | String, writer : Object | Array, [, options : Object  , url : String ]) : any Encodes data synchronously using the provided writer, if possible. If not, returns  null , in which case asynchronous loading is required. data  - loaded data, either in binary or text format. writer  - can be a single writer or an array of writers. options  - optional, options for the writer (see documentation of the specific writer). url  - optional, assists in the autoselection of a writer if multiple writers are supplied to  writer .","headings":[{"value":"encode","depth":1},{"value":"Functions","depth":2},{"value":"encode(fileData : ArrayBuffer | String, writer : Object | Array [, options : Object , url : String]) : Promise.Any","depth":3},{"value":"encodeSync(fileData : ArrayBuffer | String, writer : Object | Array, [, options : Object , url : String]) : any","depth":3}],"slug":"modules/core/docs/api-reference/encode","title":"encode"},{"excerpt":"fetchFile The  fetchFile  function is a wrapper around  fetch  which provides support for path prefixes and some additional loading capabilities. Usage Use the  fetchFile  function as follows: The  Response  object from  fetchFile  is usually passed to  parse  as follows: Note that if you don't need the extra features in  fetchFile , you can just use the browsers built-in  fetch  method. Functions fetchFile(url : String  , options : Object ) : Promise.Response A wrapper around the platform  fetch  function with some additions: Supports  setPathPrefix : If path prefix has been set, it will be appended if  url  is relative (e.g. does not start with a  / ). Supports  File  and  Blob  objects on the browser (and returns \"mock\" fetch response objects). Returns: A promise that resolves into a fetch  Response  object, with the following methods/fields: headers :  Headers  - A  Headers  object. arrayBuffer() : Promise.ArrayBuffer - Loads the file as an ArrayBuffer`. text() : Promise.String` - Loads the file and decodes it into text. json() : Promise.String` - Loads the file and decodes it into JSON. body  : ReadableStream` - A stream that can be used to incrementally read the contents of the file. Options: Under Node.js, options include (see  fs.createReadStream ): options.highWaterMark  (Number) Default: 64K (64  *  1024) - Determines the \"chunk size\" of data read from the file. readFileSync(url : String  , options : Object ) : ArrayBuffer | String This function only works on Node.js or using data URLs. Reads the raw data from a file asynchronously. Notes: Any path prefix set by  setPathPrefix  will be appended to relative urls. Remarks fetchFile  will delegate to  fetch  after resolving the URL. For some data sources such as node.js and  File / Blob  objects a mock  Response  object will be returned, and not all fields/members may be implemented. When possible,  Content-Length  and  Content-Type   headers  are also populated for non-request data sources including  File ,  Blob  and Node.js files. fetchFile  is intended to be a small (in terms of bundle size) function to help applications work with files in a portable way. The  Response  object returned on Node.js does not implement all the functionality the browser does. If you run into the need In fact, the use of any of the file utilities including  readFile  and  readFileAsync  functions with other loaders.gl functions is entirely optional. loader objects can be used with data loaded via any mechanism the application prefers, e.g. directly using  fetch ,  XMLHttpRequest  etc. The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases. The stream utilities are intended to be small optional helpers that facilitate writing platform independent code that works with streams. This can be valuable as JavaScript Stream APIs are still maturing and there are still significant differences between platforms. However, streams and iterators created directly using platform specific APIs can be used as parameters to loaders.gl functions whenever a stream is expected, allowing the application to take full control when desired.","headings":[{"value":"fetchFile","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"fetchFile(url : String , options : Object) : Promise.Response","depth":3},{"value":"readFileSync(url : String , options : Object) : ArrayBuffer | String","depth":3},{"value":"Remarks","depth":2}],"slug":"modules/core/docs/api-reference/fetch-file","title":"fetchFile"},{"excerpt":"fetchProgress This function is still experimental A function that tracks a fetch response object and calls  onProgress  callbacks. Usage _ fetchProgress(response : Response | Promise, onProgress : function, onDone : function, onError : function) : Response onProgress: (percent: number, {loadedBytes : number, totalBytes : number}) => void","headings":[{"value":"fetchProgress","depth":1},{"value":"Usage","depth":2},{"value":"_fetchProgress(response : Response | Promise, onProgress : function, onDone : function, onError : function) : Response","depth":2}],"slug":"modules/core/docs/api-reference/fetch-progress","title":"fetchProgress"},{"excerpt":"Iterator Utilities Functions getStreamIterator(stream : Stream) : AsyncIterator Returns an async iterator that can be used to read chunks of data from the stream (or write chunks of data to the stream, in case of writable streams). Works on both Node.js 8+ and browser streams.","headings":[{"value":"Iterator Utilities","depth":1},{"value":"Functions","depth":2},{"value":"getStreamIterator(stream : Stream) : AsyncIterator","depth":3}],"slug":"modules/core/docs/api-reference/iterator-utilities","title":"Iterator Utilities"},{"excerpt":"load The  load  function can be used with any  loader object . They takes a  url  and one or more  loader objects , checks what type of data that loader prefers to work on (e.g. text, JSON, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader. load(url : String | File, loaders : Object | Object[], options?: object]): Promise.Response load(url : String | File, options?: Object): Promise.Response The  load  function is used to load and parse data with a specific  loader object . An array of loader objects can be provided, in which case  load  will attempt to autodetect which loader is appropriate for the file. The  loaders  parameter can also be omitted, in which case any  loader objects  previously registered with  registerLoaders  will be used. url  - Urls can be data urls ( data:// ) or a request ( http://  or  https:// ) urls, or a file name (Node.js only). Also accepts  File  or  Blob  object (Browser only). Can also accept any format that is accepted by  parse , with the exception of strings that are interpreted as urls. loaders  - can be a single loader or an array of loaders. If single loader is provided, will force to use it. If ommitted, will use the list of pre-registered loaders (see  registerLoaders ) options  - optional, contains both options for the read process and options for the loader (see documentation of the specific loader). Returns: Return value depends on the  loader category . Notes: If  url  is not a  string ,  load  will call  parse  directly. Any path prefix set by  setPathPrefix  will be appended to relative urls. load  takes a  url  and a loader object, checks what type of data that loader prefers to work on (e.g. text, binary, stream, ...), loads the data in the appropriate way, and passes it to the loader. If  @loaders.gl/polyfills  is installed,  load  will work under Node.js as well. Options A loader object, that can contain a mix of options: options defined by the  parse  function can be specified. options specific to any loaders can also be specified (in loader specific sub-objects). Please refer to the corresponding documentation page for for  parse  and for each loader for details.","headings":[{"value":"load","depth":1},{"value":"load(url : String | File, loaders : Object | Object[], options?: object]): Promise.Response","depth":3},{"value":"load(url : String | File, options?: Object): Promise.Response","depth":3},{"value":"Options","depth":2}],"slug":"modules/core/docs/api-reference/load","title":"load"},{"excerpt":"loadInBatches loadInBatches  opens a  url  as a stream and passes it and options to  parseInBatches . See the documentation of  load  and  parseInBatches  for more details. Starting with  ,  loadInBatches  can also load and parse multiple files from a list of  File  objects or urls. In this mode, it iterates over the supplied files, looking for valid loader matches, ignores files that do not match a loader and calls  parseInBatches  on each valid file/loader combination, returning an array of async batch iterators. More importantly, when called with multiple files,  loadInBatches  makes all the supplied files avialable to all loaders (enabling multi-file loaders such as the ShapefileLoader to access multiple files). Usage loadInBatches(url: string | File | ... , loaders: object | object[], options?: object]): Promise<AsyncIrerator<any>> loadInBatches(files: (string | File | ...)[] | FileList, loaders: object | object[], options?: object]): Promise<AsyncIterator<any>> Loads data in batches from a stream, releasing each batch to the application while the stream is still being read. Parses data with the selected  loader object . An array of  loaders  can be provided, in which case an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching). files : loaded data or an object that allows data to be loaded. Plese refer to the table below for valid types. loaders  - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see  registerLoaders ) options : optional, options for the loader (see documentation of the specific loader). Returns: Returns an async iterator that yields batches of data. The exact format for the batches depends on the  loader object  category. Notes: The  loaders  parameter can also be ommitted, in which case any  loaders  previously registered with  registerLoaders  will be used. Options A loader object, that can contain a mix of options: options specific to  loadInBatches , see below. options defined by the  parseInBatches  and  parse  functions can be specified. options specific to any loaders can also be specified (in loader specific sub-objects). Please refer to the corresponding documentation page for for  parse  and for each loader for details. Option Type Default Description options.ignoreUnknownFiles boolean true Ignores unknown files if multiple files are provided.","headings":[{"value":"loadInBatches","depth":1},{"value":"Usage","depth":3},{"value":"loadInBatches(url: string | File | ... , loaders: object | object[], options?: object]): Promise<AsyncIrerator<any>>","depth":3},{"value":"loadInBatches(files: (string | File | ...)[] | FileList, loaders: object | object[], options?: object]): Promise<AsyncIterator<any>>","depth":3},{"value":"Options","depth":2}],"slug":"modules/core/docs/api-reference/load-in-batches","title":"loadInBatches"},{"excerpt":"Binary Utilities loaders.gl provides a set of functions to simplify working with binary data. There are a couple of different ways to deal with binary data in the JavaScript APIs for browser and Node.js, and some small but annoying \"gotchas\" that can trip up programmers when working with binary data. Usage Functions toArrayBuffer(binaryData :  * ) : ArrayBuffer \"Repackages\" a binary data in non-array-buffer form as an  ArrayBuffer . binaryData - ArrayBuffer, Buffer (Node.js), typed array, blob, ... Remarks Most functions in loaders.gl that accept binary data call  toArrayBuffer(...)  on input parameters before starting processing, thus ensuring that functions work on all types of input data.","headings":[{"value":"Binary Utilities","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"toArrayBuffer(binaryData : *) : ArrayBuffer","depth":3},{"value":"Remarks","depth":2}],"slug":"modules/core/docs/api-reference/binary-utilities","title":"Binary Utilities"},{"excerpt":"parseInBatches The  parseInBatches  function can parse incrementally from a stream of data as it arrives and emit \"batches\" of parsed data. Batched parsing is only supported by a subset of loaders. Check documentation of each loader before using this function. From    parseInBatches  can be used with all loaders. Non-supporting loaders will wait until all data has arrived, and emit a single batch containing the parsed data for the entire input (effectively behave as if  parse  had been called). Usage Parse CSV in batches (emitting a batch of rows every time data arrives from the network): Parse CSV in batches, requesting an initial metadata batch: Functions async parseInBatches(data: DataSource, loaders: object | object [ ], options?: object): AsyncIterator async parseInBatches(data: DataSource, options?: object]]): AsyncIterator Parses data in batches from a stream, releasing each batch to the application while the stream is still being read. Parses data with the selected  loader object . An array of  loaders  can be provided, in which case an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching). data : loaded data or an object that allows data to be loaded. Plese refer to the table below for valid types. loaders  - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see  registerLoaders ) options : optional, options for the loader (see documentation of the specific loader). url : optional, assists in the autoselection of a loader if multiple loaders are supplied to  loader . Returns: Returns an async iterator that yields batches of data. The exact format for the batches depends on the  loader object  category. Notes: The  loaders  parameter can also be ommitted, in which case any  loaders  previously registered with  registerLoaders  will be used. Input Types Data Type Description Comments Response Response  object, e.g returned by  fetch  or  fetchFile . Data will be streamed from the  response.body  stream. AsyncIterator iterator that yields promises that resolve to binary ( ArrayBuffer ) chunks or string chunks. converted into async iterators behind the scenes.) Iterator Iterator that yields binary chunks ( ArrayBuffer ) or string chunks string chunks only work for loaders that support textual input. Promise A promise that resolves to any of the other supported data types can also be supplied. Note that many other data sources can also be parsed by first converting them to  Response  objects, e.g. with  fetchResoure : http urls, data urls,  ArrayBuffer ,  String ,  File ,  Blob ,  ReadableStream  etc. Remarks Option Type Default Description options.metadata boolean false An initial batch with  batchType: 'metadata'  will be added with information about the data being loaded. options.batches.chunkSize? number N/A When set, \"atomic\" inputs (like  ArrayBuffer  or  string ) are chunked, enabling batched parsing. No effect if input is already an iterator or stream. options.fetch `object (url: string) => Response` {} Specifies either an object with options to pass to  fetchFile , or a function that is called in place of  fetchFile  to fetch data in any subloaders. options.transforms Transform[] [] An array with transforms that can be applied to the input data before parsing.","headings":[{"value":"parseInBatches","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"async parseInBatches(data: DataSource, loaders: object | object[], options?: object): AsyncIterator","depth":3},{"value":"async parseInBatches(data: DataSource, options?: object]]): AsyncIterator","depth":3},{"value":"Input Types","depth":2},{"value":"Remarks","depth":2}],"slug":"modules/core/docs/api-reference/parse-in-batches","title":"parseInBatches"},{"excerpt":"parseSync Synchronous parsing is not supported by all loaders. Refer to the documentation for each loader. For supporting loaders, the synchronous  parseSync  function works on already loaded data. Usage Handling errors Functions parseSync(data: ArrayBuffer | String, loaders: Object | Object [ ], options?: Object, url?: String]]) : any Parses data synchronously using the provided loader, if possible. If not, returns  null , in which case asynchronous parsing is required. data : already loaded data, either in binary or text format. This parameter can be any of the following types: Response :  fetch  response object returned by  fetchFile  or  fetch . ArrayBuffer : Parse from binary data in an array buffer String : Parse from text data in a string. (Only works for loaders that support textual input). Iterator : Iterator that yeilds binary ( ArrayBuffer ) chunks or string chunks (string chunks only work for loaders that support textual input).\ncan also be supplied. loaders  - can be a single loader or an array of loaders. If ommitted, will use the list of registered loaders (see  registerLoaders ) options : optional, options for the loader (see documentation of the specific loader). url : optional, assists in the autoselection of a loader if multiple loaders are supplied to  loader . Returns: Return value depends on the  loader object  category","headings":[{"value":"parseSync","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"parseSync(data: ArrayBuffer | String, loaders: Object | Object[], options?: Object, url?: String]]) : any","depth":3}],"slug":"modules/core/docs/api-reference/parse-sync","title":"parseSync"},{"excerpt":"registerLoaders The loader registry allows applications to cherry-pick which loaders to include in their application bundle by importing just the loaders they need and registering them during initialization. Applications can then make all those imported loaders available (via format autodetection) to all subsequent  parse  and  load  calls, without those calls having to specify which loaders to use. Usage Sample application initialization code that imports and registers loaders: Some other file that needs to load CSV: Functions registerLoaders(loaders : Object | Object[]) Registers one or more  loader objects  to a global  loader object registry , these loaders will be used if no loader object is supplied to  parse  and  load . loaders  - can be a single loader or an array of loaders. The specified loaders will be added to any previously registered loaders.","headings":[{"value":"registerLoaders","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"registerLoaders(loaders : Object | Object[])","depth":3}],"slug":"modules/core/docs/api-reference/register-loaders","title":"registerLoaders"},{"excerpt":"save Needs update save  and  saveSync  function can be used with any writer.  save  takes a  url  and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer. Functions save(url : String | File, writer : Object  , options : Object ) : Promise.ArrayBuffer| Promi se.String The  save  function can be used with any writer. save  takes a  url  and a writer object, checks what type of data that writer prefers to work on (e.g. text, JSON, binary, stream, ...), saves the data in the appropriate way, and passes it to the writer. url  - Can be a string, either a data url or a request url, or in Node.js, a file name, or in the browser, a File object. data  - saveed data, either in binary or text format. writer  - can be a single writer or an array of writers. options  - optional, contains both options for the read process and options for the writer (see documentation of the specific writer). options.dataType = arraybuffer  - By default reads as binary. Set to 'text' to read as text. Returns: Return value depends on the category Notes: Any path prefix set by  setPathPrefix  will be appended to relative urls. saveSync(url : String  , options : Object ) : ArrayBuffer | String Similar to  save  except saves and parses data synchronously. Note that for  saveSync  to work, the  url  needs to be saveable synchronously  and  the writer used must support synchronous parsing. Synchronous saveing only works on data URLs or files in Node.js. In many cases, the asynchronous  save  is more appropriate.","headings":[{"value":"save","depth":1},{"value":"Functions","depth":2},{"value":"save(url : String | File, writer : Object , options : Object) : Promise.ArrayBuffer| Promi","depth":3},{"value":"saveSync(url : String , options : Object) : ArrayBuffer | String","depth":3}],"slug":"modules/core/docs/api-reference/save","title":"save"},{"excerpt":"selectLoader A core feature of loaders.gl is the ability to automatically select an appropriate loader for a specific resource among a list of candidate loaders. This feature is built-in to the  parse  and  load  functions, but applications can also access this feature directly through the  selectLoader  API. Loader selection heuristics are based on: Filename (or url) extensions MIME types (extracted from  Response   content-type  headers or  Blob.type / File.type  fields) Initial data - for certain inputs, the intial bytes can be compared against known headers for the candidate loaders. (Does not work for  Response / Stream / AsyncIterator  data). selectLoader  is also aware of the  loader registry . If no loaders are provided (by passing in a falsy value such as  null )  selectLoader  will search the list of pre-registered loaders. selectLoaderSync  is also aware of the  loader registry . If no loaders are provided (by passing in a falsy value such as  null )  selectLoader  will search the list of pre-registered loaders. Usage Select a loader from a list of provided loaders: Select a loader from pre-registered loaders in the loader registry: Select a loader by specifying MIME type (using unregistered MIME types, see below) The async  selectLoader  function can identify loaders without extension and mimeType by content sniffing  Blob  and  File  objects (useful when user drags and drops files into your application). Functions selectLoader(data: Response | ArrayBuffer | String | Blob, ..., loaders?: LoaderObject[], options?: object, context?: object): Promise<boolean> Selects an appropriate loader for a file from a list of candidate loaders by examining the  data  parameter, looking at URL extension, mimeType ('Content-Type') and/or an initial data chunk. Parameters: data  - data to perform autodetection against loaders  - can be a single loader or an array of loaders, or null. options.nothrow = false  - Return null instead of throwing exception if no loader can be found Returns: A single loader (or  null  if  options.nothrow  was set and no matching loader was found). Throws: If no matching loader was found, and  options.nothrow  was not set. Regarding the  loaders  parameter: A single loader object will be returned without matching. a  null  loader list will use the pre-registered list of loaders. A supplied list of loaders will be searched for a matching loader. selectLoaderSync(data: Response | ArrayBuffer | String | Blob, ..., loaders?: LoaderObject[], options?: object, context?: object): boolean Supported Formats strings / non-data urls: strings / data urls: The mime type will be extracted from the data url prologue (if available) fetch  Response  objects:  url  and  headers.get('Content-Type')  fields will be used. File  and  Blob  objects: Peeking into batched input sources is not supported directly by  selectLoader : Response : Avoids requesting initial data to make sure the response body is not marked as used. Stream : It is not possible to non-destructively peek into a stream. Iterator/AsyncIterator : it is not possible to peek into an iterator. Instead use helpers to get access to initialContents and pass it in separately. MIME types If the standard MIME types for each format are not precise enough, loaders.gl also supports  unregistered  MIME types. Each loader will match the  application/x.<id>  where the  <id>  is the documented  id  of the loader, e.g.  application/x.ply / application/x.draco /etc ... Remarks File extensions - An attempt will be made to extract a file extension by stripping away query parameters and base path before matching against known loader extensions. Stream autodetection - Currently not well supported.","headings":[{"value":"selectLoader","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"selectLoader(data: Response | ArrayBuffer | String | Blob, ..., loaders?: LoaderObject[], options?: object, context?: object): Promise<boolean>","depth":3},{"value":"selectLoaderSync(data: Response | ArrayBuffer | String | Blob, ..., loaders?: LoaderObject[], options?: object, context?: object): boolean","depth":3},{"value":"Supported Formats","depth":2},{"value":"MIME types","depth":2},{"value":"Remarks","depth":2}],"slug":"modules/core/docs/api-reference/select-loader","title":"selectLoader"},{"excerpt":"parse This function \"atomically\" parses data (i.e. parses the entire data set in one operation). It can be called on \"already loaded\" data such as  ArrayBuffer  and  string  objects. In contrast to  load ,  parse  does not accept URLs (it treats strings as data to be parsed) however it does read data from  Response  objects (which can involve loading data from a source).  Response  objects are returned by  fetch  but can also be manually created to wrap other data types, which makes  parse  quite flexible. Usage The return value from  fetch  or  fetchFile  is a  Promise  that resolves to the fetch  Response  object and can be passed directly to the non-sync parser functions: Batched (streaming) parsing is supported by some loaders Handling errors Functions parse(data: Response | ArrayBuffer | String, loaders: Object | Object [ ], options?: Object) : Promise\\<Any > Parses data asynchronously either using the provided loader or loaders, or using the pre-registered loaders (see  register-loaders ). data : loaded data or an object that allows data to be loaded. This parameter can be any of the following types: Response  - response object returned by  fetchFile  or  fetch . ArrayBuffer  - Parse from binary data in an array buffer String  - Parse from text data in a string. (Only works for loaders that support textual input). Iterator  - Iterator that yeilds binary ( ArrayBuffer ) chunks or string chunks (string chunks only work for loaders that support textual input). AsyncIterator  - iterator that yeilds promises that resolve to binary ( ArrayBuffer ) chunks or string chunks. ReadableStream  - A DOM or Node stream. File  - A browser file object (from drag-and-drop or file selection operations). Promise  - A promise that resolves to any of the other supported data types can also be supplied. loaders  - can be a single loader or an array of loaders. If single loader is provided, will force to use it. If ommitted, will use the list of pre-registered loaders (see  registerLoaders ) data : loaded data or an object that allows data to be loaded. See table below for valid input types for this parameter. loaders  - can be a single loader or an array of loaders. If ommitted, will use the list of pre-registered loaders (see  registerLoaders ) options : optional, options for the loader (see documentation of the specific loader). url : optional, assists in the autoselection of a loader if multiple loaders are supplied to  loader . Returns: Return value depends on the  loader object  category Notes: If multiple  loaders  are provided (or pre-registered), an attempt will be made to autodetect which loader is appropriate for the file (using url extension and header matching). Data Type Description Comments Response fetch  response object returned by e.g.  fetchFile  or  fetch . Data will be streamed from the  response.body  stream. ArrayBuffer Parse from binary data in an array buffer String Parse from text data in a string. Only works for loaders that support textual input. converted into async iterators behind the scenes. File A browser file object (from drag-and-drop or file selection operations). Promise A promise that resolves to any of the other supported data types can also be supplied. |  Iterator  | Iterator that yields binary ( ArrayBuffer ) chunks or string chunks | string chunks only work for loaders that support textual input. |\n|  AsyncIterator  | iterator that yields promises that resolve to binary ( ArrayBuffer ) chunks or string chunks. | Note that additional data types can be converted to  Response  objects and used with  parse , e.g. with  new Response(new FormData(...)) . See browser documentation for the  Response  class for more details. Options Top-level options Option Type Default Description options.fetch object or function {} Specifies either an object with options to pass to  fetchFile , or a function that is called in place of  fetchFile  to fetch data in any subloaders. options.metadata boolean false Currently only implemented for  parseInBatches , adds initial metadata batch options.log object console By default set to a  console  wrapper. Setting log to  null  will turn off logging. options.worker boolean true If the selected loader is equipped with a worker url (and the runtime environment supports it) parse on a worker thread. options.maxConcurrency number 3 How many worker instances should be created for each loader. Note that setting this higher than roughly the number CPU cores on your current machine will not provide much benefit and may create extra overhead. option.maxMobileConcurrency number 1 How many worker instances should be created for each loader on mobile devices. Mobile devicee have fewer cores and less memory available. options.reuseWorkers boolean true By default, worker threads are kept in memory and reused. But if  reuseWorkers  is  false  workers will be automatically terminated after job completion and reloaded for each job. options.<loader-id>.workerUrl string per-loader If the corresponding loader can parse on a worker, the url to the worker script can be controller with this option. options.modules  (experimental) object - Supply bundled modules (like draco3d) instead of loading from CDN. options.CDN  (experimental) string - Controls certain script loading from CDN.  true  loads from  unpkg.com/@loaders.gl .  false  load from local urls.  string  alternate CDN url.","headings":[{"value":"parse","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"parse(data: Response | ArrayBuffer | String, loaders: Object | Object[], options?: Object) : Promise\\<Any>","depth":3},{"value":"Options","depth":2}],"slug":"modules/core/docs/api-reference/parse","title":"parse"},{"excerpt":"setPathPrefix resolvePath(path : String) : String Applies aliases and path prefix, in that order. Returns an updated path. setPathPrefix(prefix : String) This sets a path prefix that is automatically prepended to relative path names provided to load functions. getPathPrefix() : String Returns the current path prefix set by  setPathPrefix .","headings":[{"value":"setPathPrefix","depth":1},{"value":"resolvePath(path : String) : String","depth":3},{"value":"setPathPrefix(prefix : String)","depth":3},{"value":"getPathPrefix() : String","depth":3}],"slug":"modules/core/docs/api-reference/set-path-prefix","title":"setPathPrefix"},{"excerpt":"writeFile A file save utilities that (attempts to) work consistently across browser and node. Usage Functions writeFile(url : String  , options : Object ) : Promise.ArrayBuffer Reads the raw data from a file asynchronously. Notes: Any path prefix set by  setPathPrefix  will be appended to relative urls. writeFileSync(url : String  , options : Object ) : ArrayBuffer Only works on Node.js or using data URLs. Reads the raw data from a \"file\" synchronously. Notes: Any path prefix set by  setPathPrefix  will be appended to relative urls. Remarks The use of the loaders.gl  writeFile  and  writeFileAsync  functions is optional, loaders.gl loaders can be used with any data loaded via any mechanism the application prefers, e.g.  fetch ,  XMLHttpRequest  etc. The \"path prefix\" support is intentended to be a simple mechanism to support certain work-arounds. It is intended to help e.g. in situations like getting test cases to load data from the right place, but was never intended to support general application use cases.","headings":[{"value":"writeFile","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"writeFile(url : String , options : Object) : Promise.ArrayBuffer","depth":3},{"value":"writeFileSync(url : String , options : Object) : ArrayBuffer","depth":3},{"value":"Remarks","depth":2}],"slug":"modules/core/docs/api-reference/write-file","title":"writeFile"},{"excerpt":"@loaders.gl/compression This module contains compression/decompression \"transforms\" for loaders.gl, a collection of framework-independent 3D and geospatial loaders (parsers). For documentation please visit the  website .","headings":[{"value":"@loaders.gl/compression","depth":1}],"slug":"modules/compression","title":"@loaders.gl/compression"},{"excerpt":"setLoaderOptions Set the supplied options onto the current global options object Usage Bundling the entire  draco3d  library (instead of loading it on-demand from CDN): Functions setLoaderOptions(options : Object) : void Merges the supplied options into the current global options Options A loader object, that can contain a mix of options: options defined by the  parse  function can be specified. options specific to any loaders can also be specified (in loader specific sub-objects). Please refer to the corresponding documentation page for for  parse  and for each loader for details.","headings":[{"value":"setLoaderOptions","depth":1},{"value":"Usage","depth":2},{"value":"Functions","depth":2},{"value":"setLoaderOptions(options : Object) : void","depth":3},{"value":"Options","depth":2}],"slug":"modules/core/docs/api-reference/set-loader-options","title":"setLoaderOptions"},{"excerpt":"Overview The  @loaders.gl/compression  module provides a small selection of lossless, legally unencumbered compression/decompression (aka deflate/inflate) \"transforms\" that work as plugins for loaders.gl Compression Formats Format Intended usage Notes zlib gzip( .gz ) and Zip( .zip ) Essentially the `deflate' method in PKWARE's PKZIP 2.x) lz4 Arrow Feather Optimized for speed (real-time compression) Decompression API The API offers \"transforms\" that can inflate/deflate data. Transforms Sync Description ZlibDeflateTransform Y Compress using Zlib codec ZlibInflateTransform Y Decompress using Zlib codec LZ4DeflateTransform Y Compress using LZ4 codec LZ4InflateTransform Y Decompress using LZ4 codec","headings":[{"value":"Overview","depth":1},{"value":"Compression Formats","depth":2},{"value":"Decompression API","depth":2}],"slug":"modules/compression/docs","title":"Overview"},{"excerpt":"LZ4InflateTransform Static Methods LZ4InflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer> Decompresses (inflates) LZ4 encoded data.","headings":[{"value":"LZ4InflateTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"LZ4InflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer>","depth":4}],"slug":"modules/compression/docs/api-reference/lz4-inflate-transform","title":"LZ4InflateTransform"},{"excerpt":"ZlibDeflateTransform Static Methods ZlibDeflateTransform.deflate(data: ArrayBuffer, options?: object): Promise<ArrayBuffer> ZlibDeflateTransform.deflateSync(data: ArrayBuffer, options?: object): ArrayBuffer Compresses (deflates) Zlib encoded data. Remarks options are passed through to the underlying  pako  library.","headings":[{"value":"ZlibDeflateTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"ZlibDeflateTransform.deflate(data: ArrayBuffer, options?: object): Promise<ArrayBuffer>","depth":4},{"value":"ZlibDeflateTransform.deflateSync(data: ArrayBuffer, options?: object): ArrayBuffer","depth":4},{"value":"Remarks","depth":2}],"slug":"modules/compression/docs/api-reference/zlib-deflate-transform","title":"ZlibDeflateTransform"},{"excerpt":"LZ4DeflateTransform Static Methods LZ4DeflateTransform.deflate(data: ArrayBuffer, options?: object): Promise<ArrayBuffer> LZ4DeflateTransform.deflateSync(data: ArrayBuffer, options?: object): ArrayBuffer Compresses (deflates) LZ4 encoded data.","headings":[{"value":"LZ4DeflateTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"LZ4DeflateTransform.deflate(data: ArrayBuffer, options?: object): Promise<ArrayBuffer>","depth":4},{"value":"LZ4DeflateTransform.deflateSync(data: ArrayBuffer, options?: object): ArrayBuffer","depth":4}],"slug":"modules/compression/docs/api-reference/lz4-deflate-transform","title":"LZ4DeflateTransform"},{"excerpt":"ZstdInflateTransform Static Methods ZstdInflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer> Decompresses (inflates) Zstandard encoded data.","headings":[{"value":"ZstdInflateTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"ZstdInflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer>","depth":4}],"slug":"modules/compression/docs/api-reference/zstd-inflate-transform","title":"ZstdInflateTransform"},{"excerpt":"ZlibInflateTransform Static Methods ZlibInflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer> Decompresses (inflates) Zlib encoded data. Remarks options are passed through to the underlying  pako  library.","headings":[{"value":"ZlibInflateTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"ZlibInflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer>","depth":3},{"value":"Remarks","depth":2}],"slug":"modules/compression/docs/api-reference/zlib-inflate-transform","title":"ZlibInflateTransform"},{"excerpt":"@loaders.gl/arrow This module contains a table loader for the Apache Arrow format. loaders.gl  is a collection of loaders for big data visualizations. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/arrow","depth":1}],"slug":"modules/arrow","title":"@loaders.gl/arrow"},{"excerpt":"Overview The  @loaders.gl/arrow  module handles  Apache Arrow , an emerging standard for large in-memory columnar data. Installation Loaders and Writers Loader ArrowLoader ArrowWorkerLoader Writer ArrowWriter Additional APIs Arrow provides a rich JavaScript API for working with Arrow formatted data. Please refer to the  ArrowJS  API documentation. Attributions @loaders.gl/arrow  was developed with the benefit of extensive technical advice from Paul Taylor @ Graphistry.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"Loaders and Writers","depth":2},{"value":"Additional APIs","depth":2},{"value":"Attributions","depth":2}],"slug":"modules/arrow/docs","title":"Overview"},{"excerpt":"ZstdDeflateTransform Static Methods ZstdDeflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer> Compresses (deflates) Zstandard encoded data.","headings":[{"value":"ZstdDeflateTransform","depth":1},{"value":"Static Methods","depth":2},{"value":"ZstdDeflateTransform.run(data: ArrayBuffer, options?: object): Promise<ArrayBuffer>","depth":4}],"slug":"modules/compression/docs/api-reference/zstd-deflate-transform","title":"ZstdDeflateTransform"},{"excerpt":"ArrowLoader The Arrow loaders are still under development. The  ArrowLoader  parses the Apache Arrow columnar table format. Loader Characteristic File Format IPC: Encapsulated Message Format File Extension .arrow File Type Binary Data Format Columnar Table Decoder Type load ,  parse ,  parseSync ,  parseInBatches Worker Thread Support Yes Streaming Support Yes Usage Options Option Type Default Description","headings":[{"value":"ArrowLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2}],"slug":"modules/arrow/docs/api-reference/arrow-loader","title":"ArrowLoader"},{"excerpt":"ArrowWriter The  ArrowWriter  encodes a set of arrays into an ArrayBuffer of Apach Arrow columnar format. Loader Characteristic File Extensions .arrow ,  .feather File Type Binary File Format Arrow Data Format Arrow Columnar Format Support API encodeSync Usage Options Option Type Default Description Dependencies Apache Arrow JS  library is included into the bundle.","headings":[{"value":"ArrowWriter","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Dependencies","depth":2}],"slug":"modules/arrow/docs/api-reference/arrow-writer","title":"ArrowWriter"},{"excerpt":"Tileset3D The 3D tiles loaders are still under development. The definition of a  3D Tiles tileset . Usage Properties asset : Object (readonly) Gets the tileset's asset object property, which contains metadata about the tileset. See the  asset schema reference  in the 3D Tiles spec for the full set of properties. Gets the tileset's properties dictionary object, which contains metadata about per-feature properties. See the  properties schema reference  in the 3D Tiles spec for the full set of properties. see Cesium3DTileFeature#getProperty\nsee Cesium3DTileFeature#setProperty ready When  true , the tileset's root tile is loaded and the tileset is ready to render.\nThis is set to  true  right before  Tileset3D.readyPromise  is resolved. readyPromise Gets the promise that will be resolved when the tileset's root tile is loaded and the tileset is ready to render. This promise is resolved at the end of the frame before the first frame the tileset is rendered in. url : String (readonly) The url to a tileset JSON file. basePath : String (readonly) (deprecated) The base path that non-absolute paths in tileset JSON file are relative to. maximumScreenSpaceError The maximum screen space error used to drive level of detail refinement. This value helps determine when a tile refines to its descendants, and therefore plays a major role in balancing performance with visual quality. A tile's screen space error is roughly equivalent to the number of pixels wide that would be drawn if a sphere with a\nradius equal to the tile's  geometric error  were rendered at the tile's position. If this value exceeds\n maximumScreenSpaceError  the tile refines to its descendants. Depending on the tileset,  maximumScreenSpaceError  may need to be tweaked to achieve the right balance. Higher values provide better performance but lower visual quality. maximumMemoryUsage : Number ^default 16 ^exception  maximumScreenSpaceError  must be greater than or equal to zero. The maximum amount of GPU memory (in MB) that may be used to cache tiles. This value is estimated from\ngeometry, textures, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata. Tiles not in view are unloaded to enforce this. If decreasing this value results in unloading tiles, the tiles are unloaded the next frame. If tiles sized more than  maximumMemoryUsage  are needed\nto meet the desired screen space error, determined by  Tileset3D.maximumScreenSpaceError  ,\nfor the current view, then the memory usage of the tiles loaded will exceed\n maximumMemoryUsage .  For example, if the maximum is 256 MB, but\n300 MB of tiles are needed to meet the screen space error, then 300 MB of tiles may be loaded.  When\nthese tiles go out of view, they will be unloaded. ^default 512 ^exception  maximumMemoryUsage  must be greater than or equal to zero.\n^see Tileset3D#gpuMemoryUsageInBytes root : Tile3DHeader The root tile header. boundingSphere : BoundingSphere The tileset's bounding sphere. modelMatrix : Matrix4 A 4x4 transformation matrix that transforms the entire tileset. timeSinceLoad : Number Returns the time, in milliseconds, since the tileset was loaded and first updated. maximumMemoryUsage : Number gpuMemoryUsageInBytes : Number The total amount of GPU memory in bytes used by the tileset. This value is estimated from\ngeometry, texture, and batch table textures of loaded tiles. For point clouds, this value also\nincludes per-point metadata. statistics classificationType (Experimental) readonly Determines whether terrain, 3D Tiles or both will be classified by this tileset. This option is only applied to tilesets containing batched 3D models, geometry data, or vector data. Even when undefined, vector data and geometry data\nmust render as classifications and will default to rendering on both terrain and other 3D Tiles tilesets. When enabled for batched 3D model tilesets, there are a few requirements/limitations on the glTF: This feature is using part of the 3D Tiles spec that is not final and is subject to change without the standard deprecation policy. ellipsoid : Ellipsoid Gets an ellipsoid describing the shape of the globe. Returns the  extras  property at the top-level of the tileset JSON, which contains application specific metadata.\nReturns  undefined  if  extras  does not exist. Exception The tileset is not loaded. Use Tileset3D.readyPromise or wait for Tileset3D.ready to be true. See  Extras  in the 3D Tiles specification.} unloadTileset Unloads all tiles that weren't selected the previous frame. This can be used to\nexplicitly manage the tile cache and reduce the total number of tiles loaded below\n Tileset3D.maximumMemoryUsage . Tile unloads occur at the next frame to keep all the WebGL delete calls\nwithin the render loop. isDestroyed() : Boolean Returns true if this object was destroyed; otherwise, false. If this object was destroyed, it should not be used; calling any function other than\n isDestroyed  will result in an exception. ^returns  Boolean :  true  if this object was destroyed; otherwise,  false . destroy() Destroys the WebGL resources held by this object. Destroying an object allows for deterministic\nrelease of WebGL resources, instead of relying on the garbage collector to destroy this object. Once an object is destroyed, it should not be used; calling any function other than  isDestroyed  will result in an exception. Therefore, assign the return value  undefined  to the object as done in the example. Wxception This object was destroyed, i.e., destroy() was called. Methods constructor(tileset, url, options) tileset  ( Object ) - The loaded tileset (parsed JSON) url  - (`String) The url to a tileset JSON file. options  Options object, see  with the following properties: Notes: The  version  tileset must be 3D Tiles version 0.0 or 1.0. hasExtension(extensionName : String) : Boolean true  if the tileset JSON file lists the extension in extensionsUsed; otherwise,  false .\n^param {String} extensionName The name of the extension to check.  * \n^returns {Boolean}  true  if the tileset JSON file lists the extension in extensionsUsed; otherwise,  false . Options options.url  ( Resource|String|Promise.Resource|Promise.String ) The url to a tileset JSON file. options.show = true  ( Boolean ) - Determines if the tileset will be shown. options.modelMatrix = Matrix4.IDENTITY  ( Matrix4 ) - A 4x4 transformation matrix that transforms the tileset's root tile. options.maximumScreenSpaceError = 16 ] ( Number ) - The maximum screen space error used to drive level of detail refinement. options.maximumMemoryUsage = 512 ] ( Number ) - The maximum amount of memory in MB that can be used by the tileset. options.dynamicScreenSpaceError = false ] ( Boolean ) - Optimization option. Reduce the screen space error for tiles that are further away from the camera. options.dynamicScreenSpaceErrorDensity = 0.00278 ] ( Number ) - Density used to adjust the dynamic screen space error, similar to fog density. options.dynamicScreenSpaceErrorFactor = 4.0 ] ( Number ) - A factor used to increase the computed dynamic screen space error. options.skipLevelOfDetail = true  ( Boolean ) - Optimization option. Determines if level of detail skipping should be applied during the traversal. options.baseScreenSpaceError = 1024  ( Number ) - When  skipLevelOfDetail  is  true , the screen space error that must be reached before skipping levels of detail. options.ellipsoid = Ellipsoid.WGS84  ( Ellipsoid ) - The ellipsoid determining the size and shape of the globe. Callbacks options.onTileLoad  ( void(tileHeader) ) - options.onTileUnload  ( void(tileHeader) ) - options.onTileError  ( void(tileHeader, message : String) ) - dynamicScreenSpaceError = false Optimization option. Whether the tileset should refine based on a dynamic screen space error. Tiles that are further away will be rendered with lower detail than closer tiles. This improves performance by rendering fewer tiles and making less requests, but may result in a slight drop in visual quality for tiles in the distance. The algorithm is biased towards \"street views\" where the camera is close to the ground plane of the tileset and looking at the horizon. In addition results are more accurate for tightly fitting bounding volumes like box and region. dynamicScreenSpaceErrorDensity = 0.00278 A scalar that determines the density used to adjust the dynamic screen space error (similar to \"fog\"). Increasing this value has the effect of increasing the maximum screen space error for all tiles, but in a non-linear fashion. The error starts at 0.0 and increases exponentially until a midpoint is reached, and then approaches 1.0 asymptotically. This has the effect of keeping high detail in the closer tiles and lower detail in the further tiles, with all tiles beyond a certain distance all roughly having an error of 1.0. The dynamic error is in the range [0.0, 1.0) and is multiplied by  dynamicScreenSpaceErrorFactor  to produce the\nfinal dynamic error. This dynamic error is then subtracted from the tile's actual screen space error. Increasing  dynamicScreenSpaceErrorDensity  has the effect of moving the error midpoint closer to the camera.\nIt is analogous to moving fog closer to the camera. dynamicScreenSpaceErrorFactor = 4.0; A factor used to increase the screen space error of tiles for dynamic screen space error. As this value increases less tiles\nare requested for rendering and tiles in the distance will have lower detail. If set to zero, the feature will be disabled. onTileLoad(tileHeader : Tile3DHeader) : void Indicate ssthat a tile's content was loaded. The loaded  Tile3DHeader  is passed to the event listener. This event is fired during the tileset traversal while the frame is being rendered\nso that updates to the tile take effect in the same frame.  Do not create or modify\nentities or primitives during the event listener. onTileUnload(tileHeader : Tile3DHeader) : void Indicates that a tile's content was unloaded. The unloaded  Tile3DHeaders  is passed to the event listener. This event is fired immediately before the tile's content is unloaded while the frame is being\nrendered so that the event listener has access to the tile's content.  Do not create\nor modify entities or primitives during the event listener. See Tileset3D#maximumMemoryUsage Tileset3D#trimLoadedTiles onTileError(tileHeader : Tile3DHeader) : void Called to indicate that a tile's content failed to load. By default, error messages will be logged to the console. The error object passed to the listener contains two properties: url : the url of the failed tile. message : the error message. skipLevelOfDetail : Boolean Default: true Optimization option. Determines if level of detail skipping should be applied during the traversal. The common strategy for replacement-refinement traversal is to store all levels of the tree in memory and require\nall children to be loaded before the parent can refine. With this optimization levels of the tree can be skipped\nentirely and children can be rendered alongside their parents. The tileset requires significantly less memory when\nusing this optimization. baseScreenSpaceError : Number Default: 1024 The screen space error that must be reached before skipping levels of detail. Only used when  skipLevelOfDetail  is  true . skipScreenSpaceErrorFactor : Number Default: 16 Multiplier defining the minimum screen space error to skip.\nFor example, if a tile has screen space error of 100, no tiles will be loaded unless they\nare leaves or have a screen space error  <= 100 / skipScreenSpaceErrorFactor . Only used when  Tileset3D.skipLevelOfDetail  is  true . skipLevels Default: 1 Constant defining the minimum number of levels to skip when loading tiles. When it is 0, no levels are skipped.\nFor example, if a tile is level 1, no tiles will be loaded unless they are at level greater than 2. Only used when  Tileset3D.skipLevelOfDetail  is  true . immediatelyLoadDesiredLevelOfDetail : false When true, only tiles that meet the maximum screen space error will ever be downloaded.\nSkipping factors are ignored and just the desired tiles are loaded. Only used when  Tileset3D.skipLevelOfDetail  is  true . loadSiblings: false Determines whether siblings of visible tiles are always downloaded during traversal.\nThis may be useful for ensuring that tiles are already available when the viewer turns left/right. Only used when  Tileset3D.skipLevelOfDetail  is  true .","headings":[{"value":"Tileset3D","depth":1},{"value":"Usage","depth":2},{"value":"Properties","depth":3},{"value":"asset : Object (readonly)","depth":3},{"value":"ready","depth":3},{"value":"readyPromise","depth":3},{"value":"url : String (readonly)","depth":3},{"value":"basePath : String (readonly) (deprecated)","depth":3},{"value":"maximumScreenSpaceError","depth":3},{"value":"maximumMemoryUsage : Number","depth":3},{"value":"root : Tile3DHeader","depth":3},{"value":"boundingSphere : BoundingSphere","depth":3},{"value":"modelMatrix : Matrix4","depth":3},{"value":"timeSinceLoad : Number","depth":3},{"value":"maximumMemoryUsage : Number","depth":3},{"value":"gpuMemoryUsageInBytes : Number","depth":3},{"value":"statistics","depth":3},{"value":"classificationType (Experimental) readonly","depth":3},{"value":"ellipsoid : Ellipsoid","depth":3},{"value":"unloadTileset","depth":3},{"value":"isDestroyed() : Boolean","depth":3},{"value":"destroy()","depth":3},{"value":"Methods","depth":2},{"value":"constructor(tileset, url, options)","depth":3},{"value":"hasExtension(extensionName : String) : Boolean","depth":3},{"value":"Options","depth":2},{"value":"dynamicScreenSpaceError","depth":3},{"value":"dynamicScreenSpaceErrorDensity","depth":3},{"value":"dynamicScreenSpaceErrorFactor","depth":3},{"value":"onTileLoad(tileHeader : Tile3DHeader) : void","depth":3},{"value":"onTileUnload(tileHeader : Tile3DHeader) : void","depth":3},{"value":"onTileError(tileHeader : Tile3DHeader) : void","depth":3},{"value":"skipLevelOfDetail : Boolean","depth":3},{"value":"baseScreenSpaceError : Number","depth":3},{"value":"skipScreenSpaceErrorFactor : Number","depth":3},{"value":"skipLevels","depth":3},{"value":"immediatelyLoadDesiredLevelOfDetail : false","depth":3},{"value":"loadSiblings: false","depth":3}],"slug":"modules/3d-tiles/wip/tileset-3d-full","title":"Tileset3D"},{"excerpt":"@loaders.gl/3d-tiles (Experimental) This module contains a loader for  3D tiles . loaders.gl  is a collection of loaders for big data visualizations. For documentation please visit the  website .","headings":[{"value":"@loaders.gl/3d-tiles (Experimental)","depth":1}],"slug":"modules/3d-tiles","title":"@loaders.gl/3d-tiles (Experimental)"},{"excerpt":"WIP Partly ported code from Cesium repo.","headings":[{"value":"WIP","depth":1}],"slug":"modules/3d-tiles/wip","title":"WIP"},{"excerpt":"Tile3DHeader The 3D tile loaders are still under development. The  Tile3DHeader  class contains sufficient information about each tile in the tileset to determine if that tile is visible from a certain viewing position (this information includes the tiles' bounding box, the list of its child tiles and a screen space error limit). Notes: Tile3DHeader s are instantiated by the  Tileset3D  class for all the tiles in the tileset. Additional  Tile3DHeader  instances can be created when  When a tile is first created, its content is not loaded; the content is loaded on-demand when that tile is determined to be in the view. Fields tileset : Tileset3D The tileset containing this tile. content : Tile3DContent The tile's content. This represents the actual tile's payload,\nnot the content's metadata in the tileset JSON file. boundingVolume : TileBoundingVolume Get the tile's bounding volume. contentBoundingVolume : TileBoundingVolume Get the bounding volume of the tile's contents. This defaults to the\ntile's bounding volume when the content's bounding volume is\n undefined . boundingSphere : BoundingSphere Get the bounding sphere derived from the tile's bounding volume. extras : any Returns the  extras  property in the tileset JSON for this tile, which contains application specific metadata.\nReturns  undefined  if  extras  does not exist. See  Extras in the 3D Tiles specification transform The local transform of this tile.\n@type {Matrix4} computedTransform The final computed transform of this tile.\n@type {Matrix4} The error, in meters, introduced if this tile is rendered and its children are not. geometricError : number This is used to compute screen space error, i.e., the error measured in pixels. refinement : 3DTileRefine Specifies the type of refinement that is used when traversing this tile for rendering. children : Tile3dHeader[] Gets the tile's children. parent : Tile3DHeader | null; This tile's parent or  undefined  if this tile is the root. When a tile's content points to an external tileset JSON file, the external tileset's root tile's parent is not  undefined ; instead, the parent references the tile (with its content pointing to an external tileset JSON file) as if the two tilesets were merged. hasEmptyContent : boolean When  true , the tile has no content. hasTilesetContent : boolean When  true , the tile's content points to an external tileset. This is  false  until the tile's content is loaded. Methods constructor(tileset, baseResource, header, parent) Note: Do not construct this directly, instead access tiles through {@link Tileset3D#tileVisible}. destroy() Releases resources managed by this tile. getScreenSpaceError(frameState, useParentGeometricError) : Number Get the tile's screen space error. updateVisibility(frameState) : void Update the tile's visibility. loadContent() Requests the tile's content. unloadContent() Unloads the tile's content. visibility(frameState : FrameState, parentVisibilityPlaneMask : Number) Determines whether the tile's bounding volume intersects the culling volume. frameState  The frame state. parentVisibilityPlaneMask  The parent's plane mask to speed up the visibility check. Returns Number  A plane mask as described in  CullingVolume.computeVisibilityWithPlaneMask . contentVisibility(frameState : FrameState) Assuming the tile's bounding volume intersects the culling volume, determines\nwhether the tile's content's bounding volume intersects the culling volume. FrameState frameState The frame state. Returns\n{Intersect} The result of the intersection: the tile's content is completely outside, completely inside, or intersecting the culling volume. distanceToTile(frameState : FrameState) : Number Computes the (potentially approximate) distance from the closest point of the tile's bounding volume to the camera. FrameState frameState The frame state. Returns Number  The distance, in meters, or zero if the camera is inside the bounding volume. distanceToTileCenter(frameState : FrameState) Computes the distance from the center of the tile's bounding volume to the camera. FrameState frameState The frame state. Returns Number  The distance, in meters. insideViewerRequestVolume(frameState : FrameState) : Boolean Checks if the camera is inside the viewer request volume. FrameState  frameState The frame state. Returns Boolean  Whether the camera is inside the volume.","headings":[{"value":"Tile3DHeader","depth":1},{"value":"Fields","depth":2},{"value":"tileset : Tileset3D","depth":3},{"value":"content : Tile3DContent","depth":3},{"value":"boundingVolume : TileBoundingVolume","depth":3},{"value":"contentBoundingVolume : TileBoundingVolume","depth":3},{"value":"boundingSphere : BoundingSphere","depth":3},{"value":"extras : any","depth":3},{"value":"transform","depth":3},{"value":"computedTransform","depth":3},{"value":"geometricError : number","depth":3},{"value":"refinement : 3DTileRefine","depth":3},{"value":"children : Tile3dHeader[]","depth":3},{"value":"parent : Tile3DHeader | null;","depth":3},{"value":"hasEmptyContent : boolean","depth":3},{"value":"hasTilesetContent : boolean","depth":3},{"value":"Methods","depth":2},{"value":"constructor(tileset, baseResource, header, parent)","depth":3},{"value":"destroy()","depth":3},{"value":"getScreenSpaceError(frameState, useParentGeometricError) : Number","depth":3},{"value":"updateVisibility(frameState) : void","depth":3},{"value":"loadContent()","depth":3},{"value":"unloadContent()","depth":3},{"value":"visibility(frameState : FrameState, parentVisibilityPlaneMask : Number)","depth":3},{"value":"contentVisibility(frameState : FrameState)","depth":3},{"value":"distanceToTile(frameState : FrameState) : Number","depth":3},{"value":"distanceToTileCenter(frameState : FrameState)","depth":3},{"value":"insideViewerRequestVolume(frameState : FrameState) : Boolean","depth":3}],"slug":"modules/3d-tiles/wip/tile-3d-header","title":"Tile3DHeader"},{"excerpt":"@math.gl/geospatial This library is being developed to support 3D tiles and will be moved to the math.gl repository when it stabilizes. This modile provides classes and utilities to facilitate working with the major geospatial coordinate systems and projections used with computer maps, primarily: WGS84  (World Geodetic System) coordinates. Web Mercator Projection Class Overview Class Dewscription Ellipsoid Implements ellipsoid Ellipsoid.WSG84 An  Ellipsoid  instance initialized with Earth radii per WGS84. CartographicRectangle A rectangle defined by cartographic longitudes and latitudes. Usage Examples A major use of this library is to convert between \"cartesian\" ( x ,  y ,  z ) and \"cartographic\" ( longitude ,  latitude ,  height ) representations of WSG84 coordinates. The  Ellipsoid  class implements these calculations. Framework Independence Like all non-core math.gl modules, this library can be used without the math.gl core classes. Any input vectors can be supplied as length 3 JavaScript  Array  instances. Any result vectors can be treated as length 3 JavaScript  Array  instances (they may be math.gl  Vector3 ). The core math.gl classes inherit from JavaScript  Array  and can be used directly as input.","headings":[{"value":"@math.gl/geospatial","depth":1},{"value":"Class Overview","depth":2},{"value":"Usage Examples","depth":2},{"value":"Framework Independence","depth":2}],"slug":"modules/3d-tiles/wip/geospatial","title":"@math.gl/geospatial"},{"excerpt":"Cartographic A class with static function to help convert geospatial coordinates, primarily in  WSG84  notation, between lng/lat/height and  cartesian  (earth-center relative  x , y , z ) coordinates. Usage Convert Cartesian coordinate to longitude/latitude/height-over-ellipsoid. Convert longitude/latitude/height-over-ellipsoid to Cartesian: Convert lng/lat/z with long lat in degrees to radians. Convert lng/lat/z with long lat in radians to degrees. Static Methods Cartographic.toRadians( longitude : Number, latitude : Number, height : Number , ellipsoid : Ellipsoid [, result : Number 3 ) : Number 3 Returns a new  Vector3  from longitude and latitude values given in degrees. longitude  The longitude, in degrees latitude  The latitude, in degrees height = 0.0  The height, in meters, above the ellipsoid. result = The object onto which to store the result. Cartographic.toDegrees( longitudeRadians : Number, latitudeRadians : Number, height : Number , ellipsoid : Ellipsoid [, result : Number 3 ]) : Number 3 Returns a Vector3 position from longitude and latitude values given in radians. longitude  The longitude, in degrees latitude  The latitude, in degrees height = 0.0  The height, in meters, above the ellipsoid. ellipsoid = Ellipsoid.WGS84  The ellipsoid on which the position lies.-  result = The object onto which to store the result. Cartographic.fromCartesian( longitude : Number, latitude : Number, height : Number , ellipsoid : Ellipsoid [, result : Number 3 ]) : Number 3 Converts a Cartesian geodetic position to a longitude/latitude/height vector. @param {Vector3} cartesian The Cartesian position to convert to cartographic representation. @param {Ellipsoid}  ellipsoid=Ellipsoid.WGS84  The ellipsoid on which the position lies. @param {Cartographic}  result  The object onto which to store the result. @returns {Cartographic} The modified result parameter, new Cartographic instance if none was provided, or undefined if the cartesian is at the center of the ellipsoid. Returns: a lng, lat, height from a Cartesian position. The values in the resulting object will be in radians. cartesian  The Cartesian position to convert to cartographic representation. ellipsoid = Ellipsoid.WGS84  The ellipsoid on which the position lies. result  The object onto which to store the result. Returns: Array of 3 numbers. The modified result parameter, a new vector if none was provided, or  undefined  if the cartesian is at the center of the ellipsoid. Cartographic.toCartesian( longitude : Number, latitude : Number, height : Number , ellipsoid : Ellipsoid [, result : Number 3 ]) : Number 3 Converts a lng/lat/height into a Cartesian position on the given ellipsoid. cartesian  The Cartesian position to convert to cartographic representation. ellipsoid = Ellipsoid.WGS84  The ellipsoid on which the position lies. result  The object onto which to store the result. Returns: The modified result parameter, new Cartographic instance if none was provided, or undefined if the cartesian is at the center of the ellipsoid. The values in the resulting object will be in degrees. Attribution This class is based on  Cesium  source code under the Apache 2 License.","headings":[{"value":"Cartographic","depth":1},{"value":"Usage","depth":2},{"value":"Static Methods","depth":2},{"value":"Cartographic.toRadians(longitude : Number, latitude : Number, height : Number, ellipsoid : Ellipsoid [, result : Number3) : Number3","depth":3},{"value":"Cartographic.toDegrees(longitudeRadians : Number, latitudeRadians : Number, height : Number, ellipsoid : Ellipsoid [, result : Number3]) : Number3","depth":3},{"value":"Cartographic.fromCartesian(longitude : Number, latitude : Number, height : Number, ellipsoid : Ellipsoid [, result : Number3]) : Number3","depth":3},{"value":"Cartographic.toCartesian(longitude : Number, latitude : Number, height : Number, ellipsoid : Ellipsoid [, result : Number3]) : Number3","depth":3},{"value":"Attribution","depth":2}],"slug":"modules/3d-tiles/wip/geospatial/cartographic","title":"Cartographic"},{"excerpt":"Tile3DStyle /** A style that is applied to a {@link Cesium3DTileset}.   Evaluates an expression defined using the {@link  https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification/Styling|3D  Tiles Styling language}.   @alias Tile3DStyle @constructor @param {Resource|String|Object}  style  The url of a style or an object defining a style. Attribution // This file is derived from the Cesium code base under Apache 2 license\n// See LICENSE.md and  https://github.com/AnalyticalGraphicsInc/cesium/blob/master/LICENSE.md Usage Creating a style instance Evaluating  show  (feature visibility) using a style Evaluating colors using the style Controlling pointSize using styles Changing  pointOutlineColor  (experimental) Setting label color Setting label outline color Evaluating a font using a style Evaluating a  labelStyle  using a style Evaluating a labelText using a style Member Fields style : Object (readonly) Gets the object defining the style using the\n{@link  https://github.com/AnalyticalGraphicsInc/3d-tiles/tree/master/specification/Styling|3D  Tiles Styling language}. Default:  {} show : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  show  property. Alternatively a boolean, string, or object defining a show style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return or convert to a  Boolean . This expression is applicable to all tile formats. color : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  color  property. Alternatively a string or object defining a color style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Color . This expression is applicable to all tile formats. pointSize : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  pointSize  property. Alternatively a string or object defining a point size style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Number . This expression is only applicable to point features in a Vector tile or a Point Cloud tile. pointOutlineColor : StyleExpression experimental  This feature is using part of the 3D Tiles spec that is not final and is subject to change without a standard deprecation policy. Gets or sets the {@link StyleExpression} object used to evaluate the style's  pointOutlineColor  property. Alternatively a string or object defining a color style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Color . This expression is only applicable to point features in a Vector tile. pointOutlineWidth : StyleExpression experimental  This feature is using part of the 3D Tiles spec that is not final and is subject to change without a standard deprecation policy. Gets or sets the {@link StyleExpression} object used to evaluate the style's  pointOutlineWidth  property. Alternatively a string or object defining a number style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Number . This expression is only applicable to point features in a Vector tile. labelColor : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  labelColor  property. Alternatively a string or object defining a color style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Color . This expression is only applicable to point features in a Vector tile. labelOutlineColor : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  labelOutlineColor  property. Alternatively a string or object defining a color style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Color . This expression is only applicable to point features in a Vector tile. labelOutlineWidth : StuleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  labelOutlineWidth  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Number . This expression is only applicable to point features in a Vector tile. @example\nconst style = new Tile3DStyle();\n// Override labelOutlineWidth expression with a string\nstyle.labelOutlineWidth = '5'; @example\nconst style = new Tile3DStyle();\n// Override labelOutlineWidth expression with a condition\nstyle.labelOutlineWidth = {\n     conditions : [\n      '${height} > 2', '5' ,\n    'true', '0' \n   ]\n}; font Gets or sets the {@link StyleExpression} object used to evaluate the style's  font  property. Alternatively a string or object defining a string style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  String . This expression is only applicable to point features in a Vector tile. @type {StyleExpression} labelStyle : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  label style  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  LabelStyle . This expression is only applicable to point features in a Vector tile. labelText : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  labelText  property. Alternatively a string or object defining a string style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  String . This expression is only applicable to point features in a Vector tile. Gets or sets the {@link StyleExpression} object used to evaluate the style's  backgroundColor  property. Alternatively a string or object defining a color style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Color . This expression is only applicable to point features in a Vector tile. heightOffset : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  heightOffset  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Number . This expression is only applicable to point features in a Vector tile. anchorLineEnabled : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  anchorLineEnabled  property. Alternatively a string or object defining a boolean style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Boolean . This expression is only applicable to point features in a Vector tile. anchorLineColor : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  anchorLineColor  property. Alternatively a string or object defining a color style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Color . This expression is only applicable to point features in a Vector tile. Gets or sets the {@link StyleExpression} object used to evaluate the style's  image  property. Alternatively a string or object defining a string style can be used. The getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  String . This expression is only applicable to point features in a Vector tile. Gets or sets the {@link StyleExpression} object used to evaluate the style's  disableDepthTestDistance  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  Number . This expression is only applicable to point features in a Vector tile. Gets or sets the {@link StyleExpression} object used to evaluate the style's  horizontalOrigin  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  HorizontalOrigin . This expression is only applicable to point features in a Vector tile. verticalOrigin : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  verticalOrigin  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  VerticalOrigin . This expression is only applicable to point features in a Vector tile. labelVerticalOrigin : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  labelHorizontalOrigin  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  HorizontalOrigin . This expression is only applicable to point features in a Vector tile. labelVerticalOrigin : StyleExpression Gets or sets the {@link StyleExpression} object used to evaluate the style's  labelVerticalOrigin  property. Alternatively a string or object defining a number style can be used.\nThe getter will return the internal {@link Expression} or {@link ConditionsExpression}, which may differ from the value provided to the setter. The expression must return a  VerticalOrigin . This expression is only applicable to point features in a Vector tile. meta : StyleExpression Gets or sets the object containing application-specific expression that can be explicitly evaluated, e.g., for display in a UI.","headings":[{"value":"Tile3DStyle","depth":1},{"value":"Attribution","depth":2},{"value":"Usage","depth":2},{"value":"Member Fields","depth":2},{"value":"style : Object (readonly)","depth":3},{"value":"show : StyleExpression","depth":3},{"value":"color : StyleExpression","depth":3},{"value":"pointSize : StyleExpression","depth":3},{"value":"pointOutlineColor : StyleExpression","depth":3},{"value":"pointOutlineWidth : StyleExpression","depth":3},{"value":"labelColor : StyleExpression","depth":3},{"value":"labelOutlineColor : StyleExpression","depth":3},{"value":"labelOutlineWidth : StuleExpression","depth":3},{"value":"font","depth":3},{"value":"labelStyle : StyleExpression","depth":3},{"value":"labelText : StyleExpression","depth":3},{"value":"heightOffset : StyleExpression","depth":3},{"value":"anchorLineEnabled : StyleExpression","depth":3},{"value":"anchorLineColor : StyleExpression","depth":3},{"value":"","depth":3},{"value":"verticalOrigin : StyleExpression","depth":3},{"value":"labelVerticalOrigin : StyleExpression","depth":3},{"value":"labelVerticalOrigin : StyleExpression","depth":3},{"value":"meta : StyleExpression","depth":3}],"slug":"modules/3d-tiles/wip/styles/docs/tile-3d-style","title":"Tile3DStyle"},{"excerpt":"Ellipsoid A quadratic surface defined in Cartesian coordinates by the equation  (x / a)^2 + (y / b)^2 + (z / c)^2 = 1 . Primarily used to represent the shape of planetary bodies. The main use of this class is to convert between the \"cartesian\" and \"cartographic\" coordinate systems. Rather than constructing this object directly, one of the provided constants is used. Usage Create a Cartographic and determine it's Cartesian representation on a WGS84 ellipsoid. Static Fields Ellipsoid.WGS84 : Ellipsoid (readonly) An Ellipsoid instance initialized to the WGS84 standard. Ellipsoid.UNIT_SPHERE : Ellipsoid (readonly) An Ellipsoid instance initialized to radii of (1.0, 1.0, 1.0). Ellipsoid.MOON : Ellipsoid (readonly) An Ellipsoid instance initialized to a sphere with the lunar radius. Members radii : Vector3 (readonly) Gets the radii of the ellipsoid. radiiSquared : Vector3 (readonly) Gets the squared radii of the ellipsoid. radiiToTheFourth : Vector3 (readonly) Gets the radii of the ellipsoid raise to the fourth power. oneOverRadii : Vector3 (readonly) Gets one over the radii of the ellipsoid. oneOverRadiiSquared : Vector3 (readonly) Gets one over the squared radii of the ellipsoid. minimumRadius : Number (readonly) Gets the minimum radius of the ellipsoid. maximumRadius : Number Gets the maximum radius of the ellipsoid. Methods constructor(x : Number, y : Number, z : Number) x = 0  The radius in the x direction. y = 0  The radius in the y direction. z = 0  The radius in the z direction. Throws All radii components must be greater than or equal to zero. clone() : Ellipsoid Duplicates an Ellipsoid instance. {Ellipsoid}  result  Optional object onto which to store the result, or undefined if a new\ninstance should be created. Returns The cloned Ellipsoid. (Returns undefined if ellipsoid is undefined) equals(right) Compares this Ellipsoid against the provided Ellipsoid componentwise and returns  true  if they are equal,  false  otherwise.  * {Ellipsoid}  right  The other Ellipsoid. used. Returns - {Boolean}  true  if they are equal,  false  otherwise. toString() : String Creates a string representing this Ellipsoid in the format  used.'(radii.x, radii.y, radii.z)'.  * Returns A string representing this ellipsoid in the format '(radii.x, radii.y, radii.z)'. geocentricSurfaceNormal(cartesian : Number 3 , result : Number 3 ]) : Vector3 | Number 3 Computes the unit vector directed from the center of this ellipsoid toward the provided Cartesian position. cartesian  - The WSG84 cartesian coordinate for which to to determine the geocentric normal. result  - Optional object onto which to store the result. Returns The modified result parameter or a new  Vector3  instance if none was provided. geodeticSurfaceNormalCartographic(cartographic : Number 3 , result : Number 3 ]) : Vector3 | Number 3 Computes the normal of the plane tangent to the surface of the ellipsoid at the provided position. cartographic  The cartographic position for which to to determine the geodetic normal. result  Optional object onto which to store the result. Returns The modified result parameter or a new  Vector3  instance if none was provided. geodeticSurfaceNormal(cartesian : Number 3 , result : Number 3 ]) : Vector3 | Number 3 Computes the normal of the plane tangent to the surface of the ellipsoid at the provided position. cartesian  The Cartesian position for which to to determine the surface normal. result  Optional object onto which to store the result. Returns The modified  result  parameter or a new  Vector3  instance if none was provided. cartographicToCartesian(cartographic : Number 3 , result : Number 3 ]) : Vector3 | Number 3 Converts the provided cartographic to Cartesian representation. cartographic  The cartographic position. result  Optional object onto which to store the result. Returns The modified  result  parameter or a new  Vector3  instance if none was provided. cartesianToCartographic(cartesian : Number 3 , result : Number 3 ]) : Vector3 | Number 3  |  undefined Converts the provided cartesian to cartographic representation. The cartesian is  undefined  at the center of the ellipsoid. cartesian  The Cartesian position to convert to cartographic representation. result  Optional object onto which to store the result. Returns The modified result parameter, new  Vector3  instance if none was provided, or undefined if the cartesian is at the center of the ellipsoid. scaleToGeodeticSurface(cartesian : Number 3 , result : Number 3 ]) : Vector3 | Number 3  |  undefined Scales the provided Cartesian position along the geodetic surface normal so that it is on the surface of this ellipsoid. If the position is at the center of the ellipsoid, this function returns  undefined . cartesian  The Cartesian position to scale. result  Optional object onto which to store the result. Returns The modified result parameter, a new  Vector3  instance if none was provided, or undefined if the position is at the center. scaleToGeocentricSurface(cartesian : Number 3 , result : Number 3 ]) : Vector3 | Number 3 Scales the provided Cartesian position along the geocentric surface normal so that it is on the surface of this ellipsoid. cartesian  The Cartesian position to scale. result  Optional object onto which to store the result. Returns The modified  result  parameter or a new  Vector3  instance if none was provided. transformPositionToScaledSpace(position : Number 3 , result : Number 3 ]) : Vector3 | Number 3 Transforms a Cartesian X, Y, Z position to the ellipsoid-scaled space by multiplying its components by the result of  Ellipsoid.oneOverRadii . position  The position to transform. result  Optional array into which to copy the result. Returns The position expressed in the scaled space. The returned instance is the one passed as the  result  parameter if it is not undefined, or a new instance of it is. transformPositionFromScaledSpace(position : Number 3 , result : Number 3 ]) : Vector3 | Number 3 Transforms a Cartesian X, Y, Z position from the ellipsoid-scaled space by multiplying its components by the result of  Ellipsoid.radii . position  The position to transform. result  Optional array to which to copy the result. Returns The position expressed in the unscaled space. The returned array is the one passed as the  result  parameter, or a new  Vector3  instance. getSurfaceNormalIntersectionWithZAxis(position, buffer, result) : | undefined Computes a point which is the intersection of the surface normal with the z-axis. position  the position. must be on the surface of the ellipsoid. buffer = 0.0  A buffer to subtract from the ellipsoid size when checking if the point is inside the ellipsoid. result  Optional array into which to copy the result. Returns The intersection point if it's inside the ellipsoid,  undefined  otherwise. Throws position  is required. Ellipsoid  must be an ellipsoid of revolution ( radii.x == radii.y ). Ellipsoid.radii.z must be greater than 0. Notes: In earth case, with common earth datums, there is no need for this buffer since the intersection point is always (relatively) very close to the center. In WGS84 datum, intersection point is at max z = +-42841.31151331382 (0.673% of z-axis). Intersection point could be outside the ellipsoid if the ratio of MajorAxis / AxisOfRotation is bigger than the square root of 2 Attribution This class was ported from  Cesium  under the Apache 2 License.","headings":[{"value":"Ellipsoid","depth":1},{"value":"Usage","depth":2},{"value":"Static Fields","depth":2},{"value":"Ellipsoid.WGS84 : Ellipsoid (readonly)","depth":3},{"value":"Ellipsoid.UNIT_SPHERE : Ellipsoid (readonly)","depth":3},{"value":"Ellipsoid.MOON : Ellipsoid (readonly)","depth":3},{"value":"Members","depth":2},{"value":"radii : Vector3 (readonly)","depth":3},{"value":"radiiSquared : Vector3 (readonly)","depth":3},{"value":"radiiToTheFourth : Vector3 (readonly)","depth":3},{"value":"oneOverRadii : Vector3 (readonly)","depth":3},{"value":"oneOverRadiiSquared : Vector3 (readonly)","depth":3},{"value":"minimumRadius : Number (readonly)","depth":3},{"value":"maximumRadius : Number","depth":3},{"value":"Methods","depth":2},{"value":"constructor(x : Number, y : Number, z : Number)","depth":3},{"value":"clone() : Ellipsoid","depth":3},{"value":"equals(right)","depth":3},{"value":"toString() : String","depth":3},{"value":"geocentricSurfaceNormal(cartesian : Number3, result : Number3]) : Vector3 | Number3","depth":3},{"value":"geodeticSurfaceNormalCartographic(cartographic : Number3, result : Number3]) : Vector3 | Number3","depth":3},{"value":"geodeticSurfaceNormal(cartesian : Number3, result : Number3]) : Vector3 | Number3","depth":3},{"value":"cartographicToCartesian(cartographic : Number3, result : Number3]) : Vector3 | Number3","depth":3},{"value":"cartesianToCartographic(cartesian : Number3, result : Number3]) : Vector3 | Number3 | undefined","depth":3},{"value":"scaleToGeodeticSurface(cartesian : Number3, result : Number3]) : Vector3 | Number3 | undefined","depth":3},{"value":"scaleToGeocentricSurface(cartesian : Number3, result : Number3]) : Vector3 | Number3","depth":3},{"value":"transformPositionToScaledSpace(position : Number3, result : Number3]) : Vector3 | Number3","depth":3},{"value":"transformPositionFromScaledSpace(position : Number3, result : Number3]) : Vector3 | Number3","depth":3},{"value":"getSurfaceNormalIntersectionWithZAxis(position, buffer, result) : | undefined","depth":3},{"value":"Attribution","depth":2}],"slug":"modules/3d-tiles/wip/geospatial/ellipsoid","title":"Ellipsoid"},{"excerpt":"Overview The  @loaders.gl/3d-tiles  module supports loading and traversing 3D Tiles. References 3D Tiles Specification  - The living specification. 3D Tiles Standard  - The official standard from  OGC , the Open Geospatial Consortium. Installation API A standard complement of loaders and writers are provided to load the individual 3d Tile file formats: Tiles3DLoader , a loader for loading a top-down or nested tileset and its tiles. CesiumIonLoader , a loader extends from  Tiles3DLoader  with resolving credentials from Cesium ion. To handle the complex dynamic tile selection and loading required to performantly render larger-than-browser-memory tilesets, additional helper classes are provided in  @loaders.gl/tiles  module: Tileset3D  to work with the loaded tileset. Tile3D  to access data for a specific tile. Usage Basic API usage is illustrated in the following snippet. Create a  Tileset3D  instance, point it a valid tileset URL, set up callbacks, and keep feeding in new camera positions: Remarks @loaders.gl/3d-tiles  does not yet support the full 3D tiles standard. Notable omissions are: Region bounding volumes  are supported but not optimally Styling  is not yet supported Viewer request volumes  are not yet supported Attribution @loaders.gl/3d-tiles  is a fork of 3D tile related code in the  Cesium github repository  under Apache 2 License, and is developed in collabration with the Cesium engineering team.","headings":[{"value":"Overview","depth":1},{"value":"Installation","depth":2},{"value":"API","depth":2},{"value":"Usage","depth":2},{"value":"Remarks","depth":2},{"value":"Attribution","depth":2}],"slug":"modules/3d-tiles/docs","title":"Overview"},{"excerpt":"CartographicRectangle A two dimensional region specified as longitude and latitude coordinates. Usage Creates a rectangle given the boundary longitude and latitude in degrees. Creates a rectangle given the boundary longitude and latitude in radians. Static Members CartographicRectangle.MAX_VALUE new CartographicRectangle(-Math.PI, -CesiumMath.PI_OVER_TWO, Math.PI, CesiumMath.PI_OVER_TWO)); Members west : Number The westernmost longitude in radians in the range  -Pi, Pi . default 0.0 south : Number The southernmost latitude in radians in the range  -Pi/2, Pi/2 . default 0.0 east : Number The easternmost longitude in radians in the range  -Pi, Pi . default 0.0 north : Number The northernmost latitude in radians in the range  -Pi/2, Pi/2 . default 0.0 Methods constructor(west, south, east, north) west = 0.0   The westernmost longitude, in radians, in the range  -Pi, Pi . south = 0.0   The southernmost latitude, in radians, in the range  -Pi/2, Pi/2 . east = 0.0   The easternmost longitude, in radians, in the range  -Pi, Pi . north = 0.0   The northernmost latitude, in radians, in the range  -Pi/2, Pi/2 . computeWidth() Computes the width of a rectangle in radians. @returns {Number} The width. computeHeight() Computes the height of a rectangle in radians. @returns {Number} The height. fromDegrees(west, south, east, north, result) Creates a rectangle given the boundary longitude and latitude in degrees. @param {Number}  west=0.0  The westernmost longitude in degrees in the range  -180.0, 180.0 .\n@param {Number}  south=0.0  The southernmost latitude in degrees in the range  -90.0, 90.0 .\n@param {Number}  east=0.0  The easternmost longitude in degrees in the range  -180.0, 180.0 .\n@param {Number}  north=0.0  The northernmost latitude in degrees in the range  -90.0, 90.0 .\n@param {CartographicRectangle}  result  The object onto which to store the result, or undefined if a new instance should be created.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if none was provided. fromRadians(west, south, east, north, result) Creates a rectangle given the boundary longitude and latitude in radians. @param {Number}  west=0.0  The westernmost longitude in radians in the range  -Math.PI, Math.PI .\n@param {Number}  south=0.0  The southernmost latitude in radians in the range  -Math.PI/2, Math.PI/2 .\n@param {Number}  east=0.0  The easternmost longitude in radians in the range  -Math.PI, Math.PI .\n@param {Number}  north=0.0  The northernmost latitude in radians in the range  -Math.PI/2, Math.PI/2 .\n@param {CartographicRectangle}  result  The object onto which to store the result, or undefined if a new instance should be created.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if none was provided. fromCartographicArray(cartographics, result) Creates the smallest possible CartographicRectangle that encloses all positions in the provided array. @param {Cartographic[]} cartographics The list of Cartographic instances.\n@param {CartographicRectangle}  result  The object onto which to store the result, or undefined if a new instance should be created.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if none was provided. fromCartesianArray(cartesians, ellipsoid, result) Creates the smallest possible CartographicRectangle that encloses all positions in the provided array. @param {Cartesian3[]} cartesians The list of Cartesian instances.\n@param {Ellipsoid}  ellipsoid=Ellipsoid.WGS84  The ellipsoid the cartesians are on.\n@param {CartographicRectangle}  result  The object onto which to store the result, or undefined if a new instance should be created.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if none was provided. clone(rectangle, result) Duplicates a CartographicRectangle. @param {CartographicRectangle} rectangle The rectangle to clone.\n@param {CartographicRectangle}  result  The object onto which to store the result, or undefined if a new instance should be created.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if none was provided. (Returns undefined if rectangle is undefined) Compares the provided CartographicRectangles componentwise and returns\n true  if they pass an absolute or relative tolerance test,\n false  otherwise. @param {CartographicRectangle}  left  The first CartographicRectangle.\n@param {CartographicRectangle}  right  The second CartographicRectangle.\n@param {Number} absoluteEpsilon The absolute epsilon tolerance to use for equality testing.\n@returns {Boolean}  true  if left and right are within the provided epsilon,  false  otherwise.\n/\n\tCartographicRectangle.equalsEpsilon = function(left, right, absoluteEpsilon)\n\t\t//>>includeStart('debug', pragmas.debug);\n\t\tCheck.typeOf.number('absoluteEpsilon', absoluteEpsilon);\n\t\t//>>includeEnd('debug'); Duplicates this CartographicRectangle. @param {CartographicRectangle}  result  The object onto which to store the result.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if none was provided.\n/\n\tCartographicRectangle.prototype.clone = function(result)\n\t\treturn CartographicRectangle.clone(this, result);\n\t}; Compares the provided CartographicRectangle with this CartographicRectangle componentwise and returns\n true  if they are equal,  false  otherwise. @param {CartographicRectangle}  other  The CartographicRectangle to compare.\n@returns {Boolean}  true  if the CartographicRectangles are equal,  false  otherwise.\n/\n\tCartographicRectangle.prototype.equals = function(other)\n\t\treturn CartographicRectangle.equals(this, other);\n\t}; Compares the provided rectangles and returns  true  if they are equal,\n false  otherwise. CartographicRectangle.equals = function(left, right) @param {CartographicRectangle}  left  The first CartographicRectangle.\n@param {CartographicRectangle}  right  The second CartographicRectangle.\n@returns {Boolean}  true  if left and right are equal; otherwise  false . Compares the provided CartographicRectangle with this CartographicRectangle componentwise and returns\n true  if they are within the provided epsilon,\n false  otherwise. CartographicRectangle.prototype.equalsEpsilon = function(other, epsilon) @param {CartographicRectangle}  other  The CartographicRectangle to compare.\n@param {Number} epsilon The epsilon to use for equality testing.\n@returns {Boolean}  true  if the CartographicRectangles are within the provided epsilon,  false  otherwise. validate() Checks a CartographicRectangle's properties and throws if they are not in valid ranges. Throws north  must be in the interval  -Pi/2 ,  Pi/2 . south  must be in the interval  -Pi/2 ,  Pi/2 . east  must be in the interval  -Pi ,  Pi . west  must be in the interval  -Pi ,  Pi . southwest(rectangle, result) Computes the southwest corner of a rectangle. @param {CartographicRectangle} rectangle The rectangle for which to find the corner\n@param {Cartographic}  result  The object onto which to store the result.\n@returns {Cartographic} The modified result parameter or a new Cartographic instance if none was provided. northwest(rectangle, result) Computes the northwest corner of a rectangle. @param {CartographicRectangle} rectangle The rectangle for which to find the corner\n@param {Cartographic}  result  The object onto which to store the result.\n@returns {Cartographic} The modified result parameter or a new Cartographic instance if none was provided. northeast(rectangle, result) Computes the northeast corner of a rectangle. @param {CartographicRectangle} rectangle The rectangle for which to find the corner\n@param {Cartographic}  result  The object onto which to store the result.\n@returns {Cartographic} The modified result parameter or a new Cartographic instance if none was provided. southeast(rectangle, result) Computes the southeast corner of a rectangle. @param {CartographicRectangle} rectangle The rectangle for which to find the corner\n@param {Cartographic}  result  The object onto which to store the result.\n@returns {Cartographic} The modified result parameter or a new Cartographic instance if none was provided. center = function(rectangle, result) Computes the center of a rectangle. @param {CartographicRectangle} rectangle The rectangle for which to find the center\n@param {Cartographic}  result  The object onto which to store the result.\n@returns {Cartographic} The modified result parameter or a new Cartographic instance if none was provided. intersection = function(rectangle, CartographicotherRectangle, result) Computes the intersection of two rectangles.  This function assumes that the rectangle's coordinates are\nlatitude and longitude in radians and produces a correct intersection, taking into account the fact that\nthe same angle can be represented with multiple values as well as the wrapping of longitude at the\nanti-meridian.  For a simple intersection that ignores these factors and can be used with projected\ncoordinates, see {@link CartographicRectangle.simpleIntersection}. @param {CartographicRectangle} rectangle On rectangle to find an intersection\n@param {CartographicRectangle} CartographicotherRectangle Another rectangle to find an intersection\n@param {CartographicRectangle}  result  The object onto which to store the result. simpleIntersection = function(rectangle, CartographicotherRectangle, result) Computes a simple intersection of two rectangles.  Unlike {@link CartographicRectangle.intersection}, this function\ndoes not attempt to put the angular coordinates into a consistent range or to account for crossing the\nanti-meridian.  As such, it can be used for rectangles where the coordinates are not simply latitude\nand longitude (i.e. projected coordinates). @param {CartographicRectangle} rectangle On rectangle to find an intersection\n@param {CartographicRectangle} CartographicotherRectangle Another rectangle to find an intersection\n@param {CartographicRectangle}  result  The object onto which to store the result.\n@returns {CartographicRectangle|undefined} The modified result parameter, a new CartographicRectangle instance if none was provided or undefined if there is no intersection. union(rectangle, CartographicotherRectangle, result) Computes a rectangle that is the union of two rectangles. @param {CartographicRectangle} rectangle A rectangle to enclose in rectangle.\n@param {CartographicRectangle} CartographicotherRectangle A rectangle to enclose in a rectangle.\n@param {CartographicRectangle}  result  The object onto which to store the result.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if none was provided. expand(rectangle, cartographic, result) Computes a rectangle by enlarging the provided rectangle until it contains the provided cartographic. @param {CartographicRectangle} rectangle A rectangle to expand.\n@param {Cartographic} cartographic A cartographic to enclose in a rectangle.\n@param {CartographicRectangle}  result  The object onto which to store the result.\n@returns {CartographicRectangle} The modified result parameter or a new CartographicRectangle instance if one was not provided. contains(rectangle, cartographic) Returns true if the cartographic is on or inside the rectangle, false otherwise. @param {CartographicRectangle} rectangle The rectangle\n@param {Cartographic} cartographic The cartographic to test.\n@returns {Boolean} true if the provided cartographic is inside the rectangle, false otherwise. subsample(rectangle, ellipsoid, surfaceHeight, result) Samples a rectangle so that it includes a list of Cartesian points suitable for passing to\n{@link BoundingSphere#fromPoints}.  Sampling is necessary to account\nfor rectangles that cover the poles or cross the equator. @param {CartographicRectangle} rectangle The rectangle to subsample.\n@param {Ellipsoid}  ellipsoid=Ellipsoid.WGS84  The ellipsoid to use.\n@param {Number}  surfaceHeight=0.0  The height of the rectangle above the ellipsoid.\n@param {Cartesian3[]}  result  The array of Cartesians onto which to store the result.\n@returns {Cartesian3[]} The modified result parameter or a new Array of Cartesians instances if none was provided. The largest possible rectangle.","headings":[{"value":"CartographicRectangle","depth":1},{"value":"Usage","depth":2},{"value":"Static Members","depth":2},{"value":"CartographicRectangle.MAX_VALUE","depth":3},{"value":"Members","depth":2},{"value":"west : Number","depth":3},{"value":"south : Number","depth":3},{"value":"east : Number","depth":3},{"value":"north : Number","depth":3},{"value":"Methods","depth":2},{"value":"constructor(west, south, east, north)","depth":3},{"value":"computeWidth()","depth":3},{"value":"computeHeight()","depth":3},{"value":"fromDegrees(west, south, east, north, result)","depth":3},{"value":"fromRadians(west, south, east, north, result)","depth":3},{"value":"fromCartographicArray(cartographics, result)","depth":3},{"value":"fromCartesianArray(cartesians, ellipsoid, result)","depth":3},{"value":"clone(rectangle, result)","depth":3},{"value":"validate()","depth":3},{"value":"southwest(rectangle, result)","depth":3},{"value":"northwest(rectangle, result)","depth":3},{"value":"northeast(rectangle, result)","depth":3},{"value":"southeast(rectangle, result)","depth":3},{"value":"center = function(rectangle, result)","depth":3},{"value":"intersection = function(rectangle, CartographicotherRectangle, result)","depth":3},{"value":"simpleIntersection = function(rectangle, CartographicotherRectangle, result)","depth":3},{"value":"union(rectangle, CartographicotherRectangle, result)","depth":3},{"value":"expand(rectangle, cartographic, result)","depth":3},{"value":"contains(rectangle, cartographic)","depth":3},{"value":"subsample(rectangle, ellipsoid, surfaceHeight, result)","depth":3}],"slug":"modules/3d-tiles/wip/geospatial/cartographic-rectangle","title":"CartographicRectangle"},{"excerpt":"Tiles3DLoader Parses a  3D tile . Loader Characteristic File Extensions .b3dm , .i3dm ,  .pnts ,  .cmpt File Type Binary (with linked assets) File Format 3D Tiles Data Format Data Formats Decoder Type Asynchronous Worker Thread Support No Streaming Support No  * Subloaders DracoLoader  ( .pnts ),  GLTFLoader  ( .b3dm ,  .i3dm ) *  Streaming is not supported for individual tiles, however tilesets are streamed by loading only the tiles needed for the Usage As a tileset contains multiple file formats,  Tiles3DLoader  is needed to be explicitly specified when using  load  function. Load a tileset file. To decompress tiles containing Draco compressed glTF models or Draco compressed point clouds: Load a tileset and dynamically load/unload tiles based on viewport with helper class  Tileset3D  ( @loaders.gl/tiles ) Options Option Type Default Description 3d-tiles.isTileset Bool  or  auto auto Whether to load a  Tileset  file. If  auto , will infer based on url extension. 3d-tiles.headers Object null Used to load data from server 3d-tiles.tileset Object null Tileset  object loaded by  Tiles3DLoader  or follow the data format specified in  Tileset Object . It is required when loading i3s geometry content 3d-tiles.tile Object null Tile  object loaded by  Tiles3DLoader  or follow the data format  Tile Object . It is required when loading i3s geometry content To enable parsing of DRACO compressed point clouds and glTF tiles, make sure to first register the  DracoLoader . Point cloud tie options Option Type Default Description 3d-tiles.decodeQuantizedPositions Boolean false Pre-decode quantized position on CPU For i3dm and b3dm tiles: Option Type Default Description 3d-tiles.loadGLTF Boolean true Fetch and parse any linked glTF files If  options['3d-tiles'].loadGLTF  is  true , GLTF loading can be controlled by providing  GLTFLoader  options  via the  options.gltf  sub options. Notes about Tile Types b3dm, i3dm glTF file into a hierarchical Scenegraph description that can be used to instantiate an actual Scenegraph in most WebGL libraries. Can load both binary  .glb  files and JSON  .gltf  files. Data formats This section specifies the loaded data formats. Tileset Object The following fields are guaranteed. Additionally, the loaded tileset object will contain all the data fetched from the provided url. Field Type Contents loader Object Tiles3DLoader root Object The root tile header object url String The url of this tileset type String Value is  TILES3D . Indicates the returned object is a Cesium  3D Tiles  tileset. lodMetricType String Root's Level of Detail (LoD) metric type, which is used to decide if a tile is sufficient for current viewport. Used for deciding if this tile is sufficient given current viewport. Cesium use  geometricError . lodMetricValue Number Root's level of detail (LoD) metric value. Tile Object The following fields are guaranteed. Additionally, the loaded tile object will contain all the data fetched from the provided url. Field Type Contents url String The url of this tile. contentUrl String The contentUrl of this tile. boundingVolume Object A bounding volume in Cartesian coordinates that encloses a tile or its content. Exactly one box, region, or sphere property is required. ( Reference ) lodMetricType String Level of Detail (LoD) metric type, which is used to decide if a tile is sufficient for current viewport. Used for deciding if this tile is sufficient given current viewport. Cesium use  geometricError . lodMetricValue String Level of Detail (LoD) metric value. children Array An array of objects that define child tiles. Each child tile content is fully enclosed by its parent tile's bounding volume and, generally, has more details than parent. for leaf tiles, the length of this array is zero, and children may not be defined. content String The actual payload of the tile or the url point to the actual payload. id String Identifier of the tile, unique in a tileset refine String Refinement type of the tile,  ADD  or  REPLACE type String Type of the tile, one of  pointcloud  ( .pnts ),  scenegraph  ( .i3dm ,  .b3dm ) transformMatrix Number[16] A matrix that transforms from the tile's local coordinate system to the parent tile's coordinate systemor the tileset's coordinate system in the case of the root tile Tile Content After content is loaded, the following fields are guaranteed. But different tiles may have different extra content fields. Field Type Contents cartesianOrigin Number[3] \"Center\" of tile geometry in WGS84 fixed frame coordinates cartographicOrigin Number[3] \"Origin\" in lng/lat (center of tile's bounding volume) modelMatrix Number[16] Transforms tile geometry positions to fixed frame coordinates attributes Object Each attribute follows luma.gl  accessor  properties attributes  contains following fields Field Type Contents attributes.positions Object {value, type, size, normalized} attributes.normals Object {value, type, size, normalized} attributes.colors Object {value, type, size, normalized} PointCloud Fields Field Type Contents pointCount Number Number of points color Number[3]  or  Number[4] Color of the tile when there are not  attributes.colors Scenegraph Fields Field Type Contents gltf Object check  GLTFLoader  for detailed spec","headings":[{"value":"Tiles3DLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Notes about Tile Types","depth":2},{"value":"b3dm, i3dm","depth":3},{"value":"Data formats","depth":2},{"value":"Tileset Object","depth":3},{"value":"Tile Object","depth":3},{"value":"Tile Content","depth":3}],"slug":"modules/3d-tiles/docs/api-reference/tiles-3d-loader","title":"Tiles3DLoader"},{"excerpt":"CesiumIonLoader Extends from  Tiles3DLoader , inherits all the options and share the same resolved  Tileset  and  Tile  format.\nAlong with the support of resolving tileset metadata and authorization from Cesium ion server. Parse  3D tile  fetched from Cesium ion server. Usage Load a tileset file from Cesium ion server. Options Inherit all the options from  Tiles3DLoader . Option Type Default Description ['cesium-ion'].isTileset Bool or  auto auto Whether to load a  Tileset  file. If  auto , will infer based on url extension. ['cesium-ion'].headers Object null Used to load data from server ['cesium-ion'].tileset Object null Tileset  object loaded by  Tiles3DLoader  or follow the data format specified in  Tileset Object . It is required when loading i3s geometry content ['cesium-ion'].tile Object null Tile  object loaded by  Tiles3DLoader  or follow the data format  Tile Object . It is required when loading i3s geometry content To enable parsing of DRACO compressed point clouds and glTF tiles, make sure to first register the  DracoLoader . Point cloud tie options Option Type Default Description ['cesium-ion'].decodeQuantizedPosition Boolean false Pre-decode quantized position on CPU For i3dm and b3dm tiles: Option Type Default Description ['cesium-ion'].loadGLTF Boolean true Fetch and parse any linked glTF files If  options['cesium-ion'].loadGLTF  is  true , GLTF loading can be controlled by providing  GLTFLoader  options  via the  options.gltf  sub options. Data formats The same as  Tiles3DLoader .","headings":[{"value":"CesiumIonLoader","depth":1},{"value":"Usage","depth":2},{"value":"Options","depth":2},{"value":"Data formats","depth":2}],"slug":"modules/3d-tiles/docs/api-reference/cesium-ion-loader","title":"CesiumIonLoader"},{"excerpt":"","headings":[],"slug":"arrowjs/docs/paul-drafts/visitors/index","title":""},{"excerpt":"","headings":[],"slug":"arrowjs/docs/paul-drafts/vectors/index","title":""},{"excerpt":"","headings":[],"slug":"arrowjs/docs/paul-drafts/tables/index","title":""},{"excerpt":"","headings":[],"slug":"arrowjs/docs/paul-drafts/ipc/index","title":""},{"excerpt":"","headings":[],"slug":"arrowjs/docs/paul-drafts/data-types/index","title":""},{"excerpt":"","headings":[],"slug":"arrowjs/docs/paul-drafts/builders/index","title":""},{"excerpt":"Contributing This page contains information for Arrow JS contributors. API Design Notes Understanding some of the design decisions made when defining the JavaScript binding API may help facilitate a better appreciateion of why the API is designed the way it is: To facilitate keeping the evolution of the JavaScript bindings matched to other bindings, the JavaScript Arrow API is designed to be close match to the C++ Arrow API, although some differences have been made where it makes sense. Some design patterns, like the way  RecordBatchReader.from()  returns different  RecordBatchReader  subclasses depending on what source is being read. Editing Documentation Markdown vs JSDoc Since the Arrow JavaScript API includes both manually written markdown and \"automatically\" generated jsdoc. Some main differences are: The markdown version contains a \"Developer Guide\" which is not present in the jsdoc. The markdown version of the \"API reference\" focuses on readability. It contains more text with semantic descriptions and examples of usage of classes and functions. It also omits more complex typescript annotions for function prototypes to ensure that the API documentation is easy to digest for all JavaScript programmers. The jsdoc version includes the full Typescript type information and is more richly hyperlinked and can be valuable to developers as a supplement to the markdown reference when those particular details matter. Updating Docs In general, the markdown docs should be considered the source of truth for the JavaScript API: To avoid excessive duplication and possible divergence between markdown and JSDoc, it is recommended that the JSDoc version contains brief summary texts only. Reviewers should make sure that PRs affecting the JS API (bothk features and bug fixes) contain appropriate changes to the markdown docs (in the same way that such PRs must contain appropriate changes to e.g. test cases). When appropriate, to ensure the markdown docs remain \"the source of truth\" for the Arrow JS API, bugs should be reviewed first towards the markdown documentation, e.g. to see if the documented behavior is incorrectly specified and needs to be fixed.","headings":[{"value":"Contributing","depth":1},{"value":"API Design Notes","depth":2},{"value":"Editing Documentation","depth":2},{"value":"Markdown vs JSDoc","depth":3},{"value":"Updating Docs","depth":3}],"slug":"arrowjs/docs/contributing","title":"Contributing"},{"excerpt":"Introduction The Arrow JavaScript API is designed to helps applications tap into the full power of working with binary columnar data in the Apache Arrow format. Arrow JS has a rich set of classes that supports use cases such as batched loading and writing, as well performing data frame operations on Arrow encoded data, including applying filters, iterating over tables, etc. Getting Started To install and start coding with Apache Arrow JS bindings, see the  Getting Started . About Apache Arrow Apache Arrow is a performance-optimized binary columnar memory layout specification for encoding vectors and table-like containers of flat and nested data. The Arrow spec is design to eliminate memory copies and aligns columnar data in memory to minimize cache misses and take advantage of the latest SIMD (Single input multiple data) and GPU operations on modern processors. Apache Arrow is emerging as the standard for large in-memory columnar data (Spark, Pandas, Drill, Graphistry, ...). By standardizing on a common binary interchange format, big data systems can reduce the costs and friction associated with cross-system communication. Resources There are some excellent resources available that can help you quickly get a feel for what capabilities the Arrow JS API offers: Observable:  Introduction to Apache Arrow Observable:  Using Apache Arrow JS with Large Datasets Observable:  Manipulating Flat Arrays, Arrow-Style Manipulating Flat Arrays  General article on Columnar Data and Data Frames Apache Arrow project links: Apache Arrow Home Apache Arrow JS on github Apache Arrow JS on npm","headings":[{"value":"Introduction","depth":1},{"value":"Getting Started","depth":2},{"value":"About Apache Arrow","depth":2},{"value":"Resources","depth":2}],"slug":"arrowjs/docs","title":"Introduction"},{"excerpt":"Roadmap What's Next for Apache Arrow in Javascript There are a lot of features we'd like to add over the next few Javascript releases: Inline predicates : Function calls in the inner loop of a scan over millions of records can be very expensive. We can potentially save that time by generating a new scan function with the predicates inlined when a filter is created. Cache filter results : Right now every time we do a scan on a filtered DataFrame we re-check the predicate on every row. There should be an (optional?) lazily computed index to store the predicate results for subsequent re-use. Friendlier API : I shouldn't have to write a custom scan function just to take a look at the results of a filter! Every DataFrame should have a toJSON() function (See ARROW-2202). node.js  (Python, C++, Java, ...) interaction : A big benefit of Arrow's common in-memory format is that different tools can operate on the same memory. Unfortunately we're pretty closed off in the browser, but node doesn't have that problem! Finishing ARROW-1700, node.js Plasma store client should make this type of interaction possible. Have an idea? Tell us! Generally JIRAs are preferred but we'll take GitHub issues too. If you just want to discuss something, reach out on the mailing list or slack. But PRs are the best of all, we can always use more contributors! Feature Completeness Ideally each Apache Arrow language binding would offer the same set of features, at least to the extent that the language/platform in question allows. In practice however, not all features have been implemented in all language bindings. In comparison with the C++ Arrow API bindings, there are some missing features in the JavaScript bindings: Tensors are not yet supported. No explicit support for Apache Arrow Flight","headings":[{"value":"Roadmap","depth":1},{"value":"Feature Completeness","depth":2}],"slug":"arrowjs/docs/roadmap","title":"Roadmap"},{"excerpt":"Introduction Apache Arrow is a binary specification and set of libraries for representing Tables and Columns of strongly-typed fixed-width, variable-width, and nested data structures in-memory and over-the-wire. Arrow represents columns of values in sets of contiguous buffers. This is in contrast to a row-oriented representation, where the values for each row are stored in a contiguous buffer. The columnar representation makes it easier to take advantage of SIMD instruction sets in modern CPUs and GPUs, and can lead to dramatic performance improvements processing large amounts of data. Components The Arrow library is organized into separate components responsible for creating, reading, writing, serializing, deserializing, or manipulating Tables or Columns. Data Types  - Classes that define the fixed-width, variable-width, and composite data types Arrow can represent Vectors  - Classes to read and decode JavaScript values from the underlying buffers or Vectors for each data type Builders  - Classes to write and encode JavaScript values into the underlying buffers or Vectors for each data type Visitors  - Classes to traverse, manipulate, read, write, or aggregate values from trees of Arrow Vectors or DataTypes IPC Readers and Writers  - Classes to read and write the Arrow IPC (inter-process communication) binary file and stream formats Fields, Schemas, RecordBatches, Tables, and Columns  - Classes to describe, manipulate, read, and write groups of strongly-typed Vectors or Columns Data Types At the heart of Arrow is set of well-known logical  data types , ensuring each Column in an Arrow Table is strongly-typed. These data types define how a Column's underlying buffers should be constructed and read, and includes configurable (and custom) metadata fields for further annotating a Column. A Schema describing each Column's name and data type is encoded alongside each Column's data buffers, allowing you to consume an Arrow data source without knowing the data types or column layout beforehand. Each data type falls into one of three rough categories: Fixed-width types, variable-width types, or composite types that contain other Arrow data types. All data types can represent null values, which are stored in a separate validity  bitmask . Follow the links below for a more detailed description of each data type. Fixed-width Data Types Fixed-width data types describe physical primitive values (bytes or bits of some fixed size), or logical values that can be represented as primitive values. In addition to an optional  Uint8Array  validity bitmask, these data types have a physical data buffer (a  TypedArray  corresponding to the data type's physical element width). Null  - A column of NULL values having no physical storage Bool  - Booleans as either 0 or 1 (bit-packed, LSB-ordered) Int  - Signed or unsigned 8, 16, 32, or 64-bit little-endian integers Float  - 2, 4, or 8-byte floating point values Decimal  - Precision-and-scale-based 128-bit decimal values FixedSizeBinary  - A list of fixed-size binary sequences, where each value occupies the same number of bytes Date  - Date as signed 32-bit integer days or 64-bit integer milliseconds since the UNIX epoch Time  - Time as signed 32 or 64-bit integers, representing either seconds, millisecond, microseconds, or nanoseconds since midnight (00:00:00) Timestamp  - Exact timestamp as signed 64-bit integers, representing either seconds, milliseconds, microseconds, or nanoseconds since the UNIX epoch Interval  - Time intervals as pairs of either (year, month) or (day, time) in SQL style FixedSizeList  - Fixed-size sequences of another logical Arrow data type Variable-width Data Types Variable-width types describe lists of values with different widths, including binary blobs, Utf8 code-points, or slices of another underlying Arrow data type. These types store the values contiguously in memory, and have a physical  Int32Array  of offsets that describe the start and end indicies of each list element. List  - Variable-length sequences of another logical Arrow data type Utf8  - Variable-length byte sequences of UTF8 code-points (strings) Binary  - Variable-length byte sequences (no guarantee of UTF8-ness) Composite Data Types Composite types don't have physical data buffers of their own. They contain other Arrow data types and delegate work to them. Union  - Union of logical child data types Map  - Map of named logical child data types Struct  - Struct of ordered logical child data types","headings":[{"value":"Introduction","depth":1},{"value":"Components","depth":2},{"value":"Data Types","depth":2},{"value":"Fixed-width Data Types","depth":3},{"value":"Variable-width Data Types","depth":3},{"value":"Composite Data Types","depth":3}],"slug":"arrowjs/docs/paul-drafts/introduction","title":"Introduction"},{"excerpt":"Examples Some short examples Get a table from an Arrow file on disk (in IPC format) Create a Table when the Arrow file is split across buffers Create a Table from JavaScript arrays Load data with  fetch Columns look like JS Arrays","headings":[{"value":"Examples","depth":1},{"value":"Get a table from an Arrow file on disk (in IPC format)","depth":3},{"value":"Create a Table when the Arrow file is split across buffers","depth":3},{"value":"Create a Table from JavaScript arrays","depth":3},{"value":"Load data with fetch","depth":3},{"value":"Columns look like JS Arrays","depth":3}],"slug":"arrowjs/docs/get-started/examples","title":"Examples"},{"excerpt":"What's New v0.4.1 TBA v0.4.0 TBA v0.3.0 TBA v0.3.0 TBA","headings":[{"value":"What's New","depth":1},{"value":"v0.4.1","depth":1},{"value":"v0.4.0","depth":1},{"value":"v0.3.0","depth":1},{"value":"v0.3.0","depth":1}],"slug":"arrowjs/docs/whats-new","title":"What's New"},{"excerpt":"Installing Installing Arrow JS The Apache Arrow JS bindings are published as an npm module. Importing Arrow JS You should now be able to import arrow into your projects","headings":[{"value":"Installing","depth":1},{"value":"Installing Arrow JS","depth":2},{"value":"Importing Arrow JS","depth":2}],"slug":"arrowjs/docs/get-started/installing","title":"Installing"},{"excerpt":"Working with BigInts Arrow supports big integers. If the JavaScript platform supports the recently introduced  BigInt64Array  typed array, Arrow JS will use this type. For convenience ArrowJS inject additional methods (on the object instance) that lets it be converted to JSON, strings, values and primitives bigIntArray.toJSON() bigIntArray.toString() bigIntArray.valueOf() bigIntArray[Symbol.toPrimitive](hint: 'string' | 'number' | 'default') Notes about Conversion Methods When you have one of the wide numeric types ( Int64 ,  Uint64 , or  Decimal  which is 128bit), those  Vector  instances always return/accept subarray slices of the underlying 32bit typed arrays. But to make life easier for people consuming the typed arrays, the Arrow JS API adds some  extra methods  to the typed arrays before they're returned. The goal of these methods is to handle conversion to and from the various primitive types ( number ,  string ,  bigint , and  JSON.stringify() ) so people usually \"fall into the pit of success\". One of the added methods is an implementation of  [Symbol.toPrimitive] , which JS will use when doing certain kinds of implicit primitive coercion.  The implementation of these methods is  bifurcated , so if you're in an environment with  BigInt  support we use the native type, but if not, we'll make a best-effort attempt to return something meaningful (usually the unsigned decimal representation of the number as a string, though we'd appreciate help if someone knows how to compute the signed decimal representation). Examples:","headings":[{"value":"Working with BigInts","depth":1},{"value":"Notes about Conversion Methods","depth":2}],"slug":"arrowjs/docs/developer-guide/big-ints","title":"Working with BigInts"},{"excerpt":"Data Types Arrow supports a rich set of data types: Fixed-length primitive types: numbers, booleans, date and times, fixed size binary, decimals, and other values that fit into a given number Variable-length primitive types: binary, string Nested types: list, struct, and union Dictionary type: An encoded categorical type Converting Dates Apache Arrow Timestamp is a 64-bit int of milliseconds since the epoch, represented as two 32-bit ints in JS to preserve precision. The fist number is the \"low\" int and the second number is the \"high\" int.","headings":[{"value":"Data Types","depth":1},{"value":"Converting Dates","depth":3}],"slug":"arrowjs/docs/developer-guide/data-types","title":"Data Types"},{"excerpt":"Data Frame Operations Part of the power of data frame operations is that they typically do not actually perform any modifications (copying etc) of the underlying data, and ultimately only impact how iteration over that data is done, and what \"view\" of the data is presented. This allows data frame operations to be extremely performant, even when applied on very big (multi-gigabyte) data aset. Note that the Arrow JS  Table  class inherits from the  DataFrame  class which is why the examples in this section can use  DataFrame  methods to  Table  instances. Also, most of the data frame operations do not modify the original  Table  or  DataFrame , but rather return a new similar object with new filtering or \"iteration constraints\" applied. So memory is usually not changed or modified during these operations. References: Much of the text in this section is adapted from Brian Hulette's  Introduction to Apache Arrow Removing Rows A simplest way to remove rows from a data frame mey be use  Table.slice(start, end) . As usual, rather than actually modifying memory, this operation returns a new  Table / DataFrame  with iteration constrained to a sub set of the rows in the original frame. Removing Columns The  Table.select(keys: String[])  method drops all columns except the columns with names that match the supplied  keys . Filtering Rows Another way to \"remove\" rows from data frames is to apply filters. Filters effectively \"removes\" rows that don't fullfill the predicates in the filter. For details see the note below. The predicates classes provided by arrow allows for the comparison of column values against literals or javascript values (equality, greater or equal than, less or equal than) as well as the creation of composite logical expressions ( and ,  or  and  not ) out of individual column comparisons. It is also possible to write custom predicates by supplying an arbitrary JavaScript function to filter a row, however performance is usually best when using the built-in comparison predicates. Note that calling  filter()  on a  DataFrame  doesn't actually remove any rows from the underlying data store (it just stores the predicates). It's not until you iterate over the date, e.g. by calling  countBy()  or  scan()  that we actually apply the filter on the rows. Counting Rows To count the number of times different values appear in a table, use  countBy() . Note that  countBy()  does not return a modified data frame or table, but instead returns a new  Table  that contains two columns,  value  and  count . Each distinct value in the specified column in the original table is listed once in  value , and the corresponding  count  field in the same row indicates how many times it was present in the original table. Note that the results are not sorted. Sorting DataFrames do not currently support sorting. To sort you need to move the data back to JavaScript arrays. Iterating over a DataFrame (Scanning) The  DataFrame.scan()  method lets you define a custom function that will be called for each (non-filtered) record in the  DataFrame . Note: For simpler use cases, it is recommended to use the Arrow API provided predicates etc rather than writing a custom scan function, as performance will often be better. Writing a  next  callback for  scan() In order to be more efficient, Arrow data is broken up into batches of records (which is what makes it possible to do concatenations despite the columnar layout, and  DataFrame.scan()  does not hide this implementation detail from you. Optimizing  scan()  performance with  bind()  callbacks In addition to the  next  callback, you can supply a  bind  function for scan to call each time it starts reading from a new  RecordBatch .  scan  will call these functions as illustrated in the following pseudo-code: Note: The  index  passed to next only applies to the current RecordBatch, it is not a global index. The current  RecordBatch  is passed to  next , so it is possible to access data without writing a bind function, but there will be a performance penalty if your data has a lot of batches.","headings":[{"value":"Data Frame Operations","depth":1},{"value":"Removing Rows","depth":2},{"value":"Removing Columns","depth":2},{"value":"Filtering Rows","depth":2},{"value":"Counting Rows","depth":2},{"value":"Sorting","depth":2},{"value":"Iterating over a DataFrame (Scanning)","depth":2},{"value":"Writing a next callback for scan()","depth":3},{"value":"Optimizing scan() performance with bind() callbacks","depth":3}],"slug":"arrowjs/docs/developer-guide/data-frame-operations","title":"Data Frame Operations"},{"excerpt":"Extracting Data While keeping data in Arrow format allows for efficient data frame operations, there are of course cases where data needs to be extracted in a form that can be use with non-Arrow-aware JavaScript code. Converting Data Many arrow classes support the following methods: toArray()  - Typically returns a typed array. toJSON()  - Arrow JS types can be converted to JSON. toString()  - Arrow JS types can be converted to strings. Extracting Data by Row You can get a temporary object representing a row in a table. Note that the  row  does not retain the schema, so you'll either need to know the order of columns  row.get(0) , or use the  to*()  methods. Extracting Data by Column More efficient is to get a column. The column can be chunked, so to get a contiguous (typed) array, call Note that if there are multiple chunks in the array, this will create a new typed array and copy the typed arrays in the chunks into that array. Extracting data by Column and Batch A more efficient (zero-copy) way to get access to data (especially if the table has not been sliced or filtered) could be to walk through the chunks in each column and get the underlying typed array for that chunk.","headings":[{"value":"Extracting Data","depth":1},{"value":"Converting Data","depth":3},{"value":"Extracting Data by Row","depth":3},{"value":"Extracting Data by Column","depth":3},{"value":"Extracting data by Column and Batch","depth":3}],"slug":"arrowjs/docs/developer-guide/converting-data","title":"Extracting Data"},{"excerpt":"Notes on Memory Management Apache Arrow is a performance-optimized architecture, and the foundation of that performance is the approach to memory management. It can be useful to have an understanding of how. How Arrow Stores Data Arrow reads in arrow data as arraybuffer(s) and then creates chunks that are \"sub array views\" into that big array buffer, and lists of those chunks are then composed into \"logical\" arrays. Chunks are created for each column in each RecordBatch. The chunks can be \"sliced and diced\" by operations on  Column ,  Table  and  DataFrame  objects, but are never copied (as long as flattening is not requested) and are conceptually immutable. (There is a low-level  Vector.set()  method however given that it could modify data that is used by multiple objects its use should be reserved for cases where implications are fully understood).","headings":[{"value":"Notes on Memory Management","depth":1},{"value":"How Arrow Stores Data","depth":2}],"slug":"arrowjs/docs/developer-guide/memory-management","title":"Notes on Memory Management"},{"excerpt":"Data Sources and Sinks The Arrow JavaScript API is designed to make it easy to work with data sources both in the browser and in Node.js. Streams Both Node and DOM/WhatWG Streams can be used directly as input sources by the Arrow JS API. Fetch Responses Fetch responses (Promises) can be used where a data source is expected. ArrayBuffers Most data sources accept  Uint8Arrays . AsyncIterators Async iterators are the most general way to abstract \"streaming\" data sources and data sinks and are consistently accepted (and in many cased returned) by the Arrow JS API.","headings":[{"value":"Data Sources and Sinks","depth":1},{"value":"Streams","depth":2},{"value":"Fetch Responses","depth":2},{"value":"ArrayBuffers","depth":2},{"value":"AsyncIterators","depth":2}],"slug":"arrowjs/docs/developer-guide/data-sources","title":"Data Sources and Sinks"},{"excerpt":"Using Predicates The Arrow API provides standard predicates that allow for the comparison of column values against literals (equality, greater or equal than, less or eqial than) as well as the creation of composite logical expressions ( and ,  or  and  not ) out of individual column comparisons. It is of course also possible to write custom predicates, however the performance is best when using the built-ins. Note that for performance reasons, filters are specified using \"predicates\" rather than custom JavaScript functions. For details on available predicates see  Using Predicates . Filtering using Predicates Note that calling  filter()  on a  DataFrame  doesn't actually do anything (other than store the predicates). It's not until you call  countBy()  or  scan()  on the resulting object that Arrow actually scans through all of the data.","headings":[{"value":"Using Predicates","depth":1},{"value":"Filtering using Predicates","depth":2}],"slug":"arrowjs/docs/developer-guide/predicates","title":"Using Predicates"},{"excerpt":"Working with Tables References: Much of the text in this section is adapted from Brian Hulette's  Using Apache Arrow JS with Large Datasets Loading Arrow Data Applications often start with loading some Arrow formatted data. The Arrow API provides several ways to do this, but in many cases, the simplest approach is to use  Table.from() . Getting Records Count Getting Arrow Schema Metadata Accessing Arrow Table Row Data Record toJSON and toArray It is easy to converting Rows to JSON/Arrays/Strings: Similar conversion methods are avaiable on many Arrow classes. tables.get(0).toJSON() Slicing Arrow Data every10KRow = Array(17)  Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3), Array(3) Our custom arrow data range stepper for sampling data: range = (start, end, step) Iterating over Rows and Cells Converting Dates Apache Arrow Timestamp is a 64-bit int of milliseconds since the epoch, represented as two 32-bit ints in JS to preserve precision. The fist number is the \"low\" int and the second number is the \"high\" int. Column Data Vectors Apache Arrow stores columns in typed arrays and vectors: Typed vectors have convinience methods to convert Int32 arrays data to JS values you can work with. For example, to get timestamps in milliseconds: timestamps = Array(10)  2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01, 2017-01-01 Filtering Timestamped Data Our custom filter by date method uses custom arrow table predicate filter and scan methods to generate JS friendly data you can map or graph:","headings":[{"value":"Working with Tables","depth":1},{"value":"Loading Arrow Data","depth":2},{"value":"Getting Records Count","depth":2},{"value":"Getting Arrow Schema Metadata","depth":3},{"value":"Accessing Arrow Table Row Data","depth":3},{"value":"Record toJSON and toArray","depth":2},{"value":"Slicing Arrow Data","depth":2},{"value":"Iterating over Rows and Cells","depth":3},{"value":"Converting Dates","depth":3},{"value":"Column Data Vectors","depth":3},{"value":"Filtering Timestamped Data","depth":3}],"slug":"arrowjs/docs/developer-guide/tables","title":"Working with Tables"},{"excerpt":"Reading and Writing Arrow Data About RecordBatches Arrow tables are typically split into record batches, allowing them to be incrementally loaded or written, and naturally the Arrow API provides classes to facilite this reading. Reading Arrow Data The  Table  class provides a simple  Table.from  convenience method for reading an Arrow formatted data file into Arrow data structures: Using RecordBatchReader to read from a Data Source To read Arrow tables incrementally, you use the  RecordBatchReader  class. If you only have one table in your file (the normal case), then you'll only need one  RecordBatchReader : Reading Multiple Tables from a Data Source The JavaScript Arrow API supports arrow data streams that contain multiple tables (this is an \"extension\" to the arrow spec). Naturally, each Table comes with its own set of record batches, so to read all batches from all tables in the data source you will need a double loop: Note: this code also works if there is only one table in the data source, in which case the outer loop will only execute once. Writing Arrow Data The  RecordStreamWriter  class allows you to write Arrow  Table  and  RecordBatch  instances to a data source. Using Transform Streams Connecting to Python Processes A more complicated example of using Arrow to go from node -> python -> node: This example construct pipes of streams of events and that python process just reads from stdin, does a GPU-dataframe operation, and writes the results to stdout. (This example uses Rx/IxJS style functional streaming pipelines). compute_degrees_via_gpu_accelerated_sql  returns a node  child_process  that is also a duplex stream, similar to the  event-stream#child()  method","headings":[{"value":"Reading and Writing Arrow Data","depth":1},{"value":"About RecordBatches","depth":2},{"value":"Reading Arrow Data","depth":2},{"value":"Using RecordBatchReader to read from a Data Source","depth":3},{"value":"Reading Multiple Tables from a Data Source","depth":3},{"value":"Writing Arrow Data","depth":1},{"value":"Using Transform Streams","depth":2},{"value":"Connecting to Python Processes","depth":3}],"slug":"arrowjs/docs/developer-guide/reading-and-writing","title":"Reading and Writing Arrow Data"},{"excerpt":"Using with Typescript This documentation does not include advanced type definitions in the interest of simplicity and making the documentation accessible to more JavaScript developers. If you are working with Typescript in your application and would benefit from documentation that includes the Typescript definitions, you can refer to the auto generated JSDocs for the API. Considerations when Using Typescript To ensure that type information \"flows\" correctly from the types of function/constructor arguments to the types of returned objects, some special methods are provided (effectively working around limitations in Typescript). A key example is the availability of static  new()  methods on a number of classes that are intended to be used instead of calling  new  on the constructor. Accordingly,  Table.new()  is an alternative to  new Table() , that provides stronger type inference on the returned Table. You may want to leverage this syntax if your application is written in Typescript.","headings":[{"value":"Using with Typescript","depth":1},{"value":"Considerations when Using Typescript","depth":2}],"slug":"arrowjs/docs/developer-guide/typescript","title":"Using with Typescript"},{"excerpt":"Column An immutable column data structure consisting of a field (type metadata) and a chunked data array. Usage Copy a column Get a contiguous typed array from a  Column  (creates a new typed array unless only one chunk) columns are iterable Inheritance Column extends  Chunked Fields In addition to fields inherited from  Chunked , Colum also defines name : String The name of the column (short for  field.name ) field : Field Returns the  Field  instance that describes for the column. Methods constructor(field : Field, vectors: Vector, offsets?: Uint32Array) clone Returns a new  Column  instance with the same properties. getChildAt(index : Number) : Vector Returns the  Vector  that contains the element with ","headings":[{"value":"Column","depth":1},{"value":"Usage","depth":2},{"value":"Inheritance","depth":2},{"value":"Fields","depth":2},{"value":"name : String","depth":3},{"value":"field : Field","depth":3},{"value":"Methods","depth":2},{"value":"constructor(field : Field, vectors: Vector, offsets?: Uint32Array)","depth":3},{"value":"clone","depth":3},{"value":"getChildAt(index : Number) : Vector","depth":3}],"slug":"arrowjs/docs/api-reference/column","title":"Column"},{"excerpt":"DataFrame Extends  Table Methods filter(predicate: Predicate) : FilteredDataFrame Returns: A  FilteredDataFrame  which is a subclass of  DataFrame , allowing you to chain additional data frame operations, including applying additional filters. Note that this operation just registers filter predicates and is this very cheap to call. No actual filtering is done until iteration starts. scan(next: Function, bind?: Function) Performantly iterates over all non-filtered rows in the data frame. next   (idx: number, batch: RecordBatch) => void  - bind   (batch: RecordBatch) => void  - Optional, typically used to generate high-performance per-batch accessor functions for  next . countBy(name: Col | String) : CountByResult","headings":[{"value":"DataFrame","depth":1},{"value":"Methods","depth":2},{"value":"filter(predicate: Predicate) : FilteredDataFrame","depth":3},{"value":"scan(next: Function, bind?: Function)","depth":3},{"value":"countBy(name: Col | String) : CountByResult","depth":3}],"slug":"arrowjs/docs/api-reference/data-frame","title":"DataFrame"},{"excerpt":"Apache Arrow JavaScript API Reference Class List TODO - This is a class list from the C++ docs, it has only been partially updated to match JS API Class Summary Array Array base type Immutable data array with some logical type and some length ArrayData Mutable container for generic Arrow array data BinaryArray Concrete Array class for variable-size binary data BooleanArray Concrete Array class for boolean data Buffer Object containing a pointer to a piece of contiguous  memory with a particular size ChunkedArray A data structure managing a list of primitive Arrow arrays logically as one large array Column An immutable column data structure consisting of a field (type metadata) and a chunked data array Decimal128 Represents a signed 128-bit integer in two's  complement Decimal128Array Concrete Array class for 128-bit decimal data DictionaryArray Concrete Array class for dictionary data Field The combination of a field name and data type, with  optional metadata FixedSizeBinaryArray Concrete Array class for fixed-size  binary data FixedWidthType Base class for all fixed-width data types FlatArray Base class for non-nested arrays FloatingPoint Base class for all floating-point data types Int16Type Concrete type class for signed 16-bit integer data Int32Type Concrete type class for signed 32-bit integer data Int64Type Concrete type class for signed 64-bit integer data Int8Type Concrete type class for signed 8-bit integer data Integer Base class for all integral data types ListArray Concrete Array class for list data ListType Concrete type class for list data NestedType NullArray Degenerate null type Array NullType Concrete type class for always-null data Number Base class for all numeric data types NumericArray PrimitiveArray Base class for arrays of fixed-size logical  types RecordBatch Collection of equal-length arrays matching a  particular Schema RecordBatchReader Abstract interface for reading stream of  record batches Schema Sequence of arrow::Field objects describing the  columns of a record batch or table data structure Status StringArray Concrete Array class for variable-size string ( utf-8) data StructArray Concrete Array class for struct data Table Logical table as sequence of chunked arrays TableBatchReader Compute a sequence of record batches from a ( possibly chunked) Table TimeUnit UnionArray Concrete Array class for union data","headings":[{"value":"Apache Arrow JavaScript API Reference","depth":1},{"value":"Class List","depth":2}],"slug":"arrowjs/docs/api-reference","title":"Apache Arrow JavaScript API Reference"},{"excerpt":"Chunked Holds a \"chunked array\" that allows a number of array fragments (represented by  Vector  instances) to be treated logically as a single vector.  Vector  instances can be concatenated into a  Chunked  without any memory being copied. Usage Create a new contiguous typed array from a  Chunked  instance (note that this creates a new typed array unless only one chunk) A  Chunked  array supports iteration, random element access and mutation. Inheritance class Chunked extends  Vector Static Methods Chunked.flatten(...vectors: Vector[]) : Vector Utility method that flattens a number of  Vector  instances or Arrays of  Vector  instances into a single Array of  Vector  instances. If the incoming Vectors are instances of  Chunked , the child chunks are extracted and flattened into the resulting Array. Does not mutate or copy data from the Vector instances. Returns an Array of  Vector  instances. Chunked.concat(...chunks:  Vector<T>[] ): Chunked Concatenates a number of  Vector  instances of the same type into a single  Chunked  Vector. Returns a new  Chunked  Vector. Note: This method extracts the inner chunks of any incoming  Chunked  instances, and flattens them into the  chunks  array of the returned  Chunked  Vector. Members Symbol.iterator  : Iterator Chunked  arrays are iterable, allowing you to use constructs like  for (const element of chunked)  to iterate over elements. For in-order traversal, this is more performant than random-element access. type : T Returns the DataType instance which determines the type of elements this  Chunked  instance contains. All vector chunks will have this type. length: Number  (read-only) Returns the total number of elements in this  Chunked  instance, representing the length of of all chunks. chunks: Vector[]  (read-only) Returns an array of the  Vector  chunks that hold the elements in this  Chunked  array. typeId : TBD  (read-only) The  typeId  enum value of the  type  instance data : Data  (read-only) Returns the  Data  instance of the  first  chunk in the list of inner Vectors. ArrayType  (read-only) Returns the constructor of the underlying typed array for the values buffer as determined by this Vector's DataType. numChildren  (read-only) The number of logical Vector children for the Chunked Vector. Only applicable if the DataType of the Vector is one of the nested types (List, FixedSizeList, Struct, or Map). stride  (read-only) The number of elements in the underlying data buffer that constitute a single logical value for the given type. The stride for all DataTypes is 1 unless noted here: For  Decimal  types, the stride is 4. For  Date  types, the stride is 1 if the  unit  is DateUnit.DAY, else 2. For  Int ,  Interval , or  Time  types, the stride is 1 if  bitWidth <= 32 , else 2. For  FixedSizeList  types, the stride is the  listSize  property of the  FixedSizeList  instance. For  FixedSizeBinary  types, the stride is the  byteWidth  property of the  FixedSizeBinary  instance. nullCount  (read-only) Number of null values across all Vector chunks in this chunked array. indices :  ChunkedKeys<T>  | null  (read-only) If this is a dictionary encoded column, returns a  Chunked  instance of the indicies of all the inner chunks. Otherwise, returns  null . dictionary: ChunkedDict | null  (read-only) If this is a dictionary encoded column, returns the Dictionary. Methods constructor(type :  * , chunks? : Vector[] = [], offsets? : Number[]) Creates a new  Chunked  array instance of the given  type  and optionally initializes it with a list of  Vector  instances. type  - The DataType of the inner chunks chunks = - Vectors must all be compatible with  type . offsets = - A Uint32Array of offsets where each inner chunk starts and ends. If not provided, offsets are automatically calculated from the list of chunks. TBD - Confirm/provide some information on how  offsets  can be used? clone(chunks? : this.chunks): Chunked Returns a new  Chunked  instance that is a clone of this instance. Does not copy the actual chunks, so the new  Chunked  instance will reference the same chunks. concat(...others:  Vector<T>[] ): Chunked Concatenates a number of  Vector  instances after the chunks. Returns a new  Chunked  array. The supplied  Vector  chunks must be the same DataType as the  Chunked  instance. slice(begin?: Number, end?: Number): Chunked Returns a new chunked array representing the logical array containing the elements within the index range, potentially dropping some chunks at beginning and end. begin = 0  - The first logical index to be included as index 0 in the new array. end  - The first logical index to be included as index 0 in the new array. Defaults to the last element in the range. Returns a zero-copy slice of this Vector. The begin and end arguments are handled the same way as JS'  Array.prototype.slice ; they are clamped between 0 and  vector.length  and wrap around when negative, e.g.  slice(-1, 5)  or  slice(5, -1) getChildAt(index : Number): Chunked | null If this  Chunked  Vector's DataType is one of the nested types (Map or Struct), returns a  Chunked  Vector view over all the chunks for the child Vector at  index . search(index: Number):  number, number  | null; search(index: Number, then?: SearchContinuation):  ReturnType<N> ; search(index: Number, then?: SearchContinuation) Using an  index  that is relative to the whole  Chunked  Vector, binary search through the list of inner chunks using supplied \"global\"  index  to find the chunk at that location. Returns the child index of the inner chunk and an element index that has been adjusted to the keyspace of the found inner chunk. search()  can be called with only an integer index, in which case a pair of  [chunkIndex, valueIndex]  are returned as a two-element Array: If  search()  is called with an integer index and a callback, the callback will be invoked with the  Chunked  instance as the first argument, then the  chunkIndex  and  valueIndex  as the second and third arguments: isValid(index: Number): boolean Checks if the element at  index  in the logical array is valid. Checks the null map (if present) to determine if the value in the logical  index  is included. get(index : Number): T 'TValue'  | null Returns the element at  index  in the logical array, or  null  if no such element exists (e.e.g if  index  is out of range). set(index: Number, value: T 'TValue'  | null): void Writes the given  value  at the provided  index . If the value is null, the null bitmap is updated. indexOf(element: Type, offset?: Number): Number Returns the index of the first occurrence of  element , or  -1  if the value was not found. offset  - the index to start searching from. toArray(): TypedArray Returns a single contiguous typed array containing data in all the chunks (effectively \"flattening\" the chunks. Notes: Calling this function creates a new typed array unless there is only one chunk.","headings":[{"value":"Chunked","depth":1},{"value":"Usage","depth":2},{"value":"Inheritance","depth":2},{"value":"Static Methods","depth":2},{"value":"Chunked.flatten(...vectors: Vector[]) : Vector","depth":3},{"value":"Chunked.concat(...chunks: Vector<T>[]): Chunked","depth":3},{"value":"Members","depth":2},{"value":"Symbol.iterator : Iterator","depth":3},{"value":"type : T","depth":3},{"value":"length: Number  (read-only)","depth":3},{"value":"chunks: Vector[]  (read-only)","depth":3},{"value":"typeId : TBD  (read-only)","depth":3},{"value":"data : Data  (read-only)","depth":3},{"value":"ArrayType  (read-only)","depth":3},{"value":"numChildren  (read-only)","depth":3},{"value":"stride  (read-only)","depth":3},{"value":"nullCount  (read-only)","depth":3},{"value":"indices : ChunkedKeys<T> | null  (read-only)","depth":3},{"value":"dictionary: ChunkedDict | null  (read-only)","depth":3},{"value":"Methods","depth":2},{"value":"constructor(type : *, chunks? : Vector[] = [], offsets? : Number[])","depth":3},{"value":"clone(chunks? : this.chunks): Chunked","depth":3},{"value":"concat(...others: Vector<T>[]): Chunked","depth":3},{"value":"slice(begin?: Number, end?: Number): Chunked","depth":3},{"value":"getChildAt(index : Number): Chunked | null","depth":3},{"value":"search(index: Number): number, number | null;","depth":3},{"value":"search(index: Number, then?: SearchContinuation): ReturnType<N>;","depth":3},{"value":"search(index: Number, then?: SearchContinuation)","depth":3},{"value":"isValid(index: Number): boolean","depth":3},{"value":"get(index : Number): T'TValue' | null","depth":3},{"value":"set(index: Number, value: T'TValue' | null): void","depth":3},{"value":"indexOf(element: Type, offset?: Number): Number","depth":3},{"value":"toArray(): TypedArray","depth":3}],"slug":"arrowjs/docs/api-reference/chunked","title":"Chunked"},{"excerpt":"Dictionary A  Dictionary  stores index-to-value maps for dictionary encoded columns. Fields indices:  V<TKey>  readonly dictionary:  Vector<T>  readonly Static Methods Dictionary.from(values: Vector, indices: TKey, keys:  ArrayLike<number>  | TKey 'TArray' ) : Dictionary Methods constructor(data: Data) reverseLookup(value: T): number getKey(idx: number): TKey 'TValue'  | null getValue(key: number): T 'TValue'  | null setKey(idx: number, key: TKey 'TValue'  | null): void setValue(key: number, value: T 'TValue'  | null): void","headings":[{"value":"Dictionary","depth":1},{"value":"Fields","depth":2},{"value":"indices: V<TKey> readonly","depth":3},{"value":"dictionary: Vector<T> readonly","depth":3},{"value":"Static Methods","depth":2},{"value":"Dictionary.from(values: Vector, indices: TKey, keys: ArrayLike<number> | TKey'TArray') : Dictionary","depth":3},{"value":"Methods","depth":2},{"value":"constructor(data: Data)","depth":3},{"value":"reverseLookup(value: T): number","depth":3},{"value":"getKey(idx: number): TKey'TValue' | null","depth":3},{"value":"getValue(key: number): T'TValue' | null","depth":3},{"value":"setKey(idx: number, key: TKey'TValue' | null): void","depth":3},{"value":"setValue(key: number, value: T'TValue' | null): void","depth":3}],"slug":"arrowjs/docs/api-reference/dictionary","title":"Dictionary"},{"excerpt":"Data Untyped storage backing for  Vector . Can be thought of as array of  ArrayBuffer  instances. Also contains slice offset (including null bitmaps). Fields readonly type: T; readonly length: Number; readonly offset: Number; readonly stride: Number; readonly childData: Data[]; readonly values:  Buffers<T> BufferType.DATA ; readonly typeIds:  Buffers<T> BufferType.TYPE ; readonly nullBitmap:  Buffers<T> BufferType.VALIDITY ; readonly valueOffsets:  Buffers<T> BufferType.OFFSET ; readonly ArrayType: any; readonly typeId: T 'TType' ; readonly buffers:  Buffers<T> ; readonly nullCount: Number; Static Methods Convenience methods for creating Data instances for each of the Arrow Vector types. Data.Null<T extends Null> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer) : Data Data.Int<T extends Int> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Dictionary<T extends Dictionary> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Float<T extends Float> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Bool<T extends Bool> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Decimal<T extends Decimal> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Date<T extends Date_> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Time<T extends Time> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Timestamp<T extends Timestamp> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Interval<T extends Interval> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.FixedSizeBinary<T extends FixedSizeBinary> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data:  DataBuffer<T> ) : Data Data.Binary<T extends Binary> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data Data.Utf8<T extends Utf8> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data Data.List<T extends List> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, child: Data<T 'valueType' > | Vector<T 'valueType' >) : Data Data.FixedSizeList<T extends FixedSizeList> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, child: Data | Vector) : Data Data.Struct<T extends Struct> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data Data.Map<T extends Map_> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data Data.Union<T extends SparseUnion> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, children: (Data | Vector)[]) : Data Data.Union<T extends DenseUnion> (type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, valueOffsets: ValueOffsetsBuffer, children: (Data | Vector)[]) : Data } Methods constructor(type: T, offset: Number, length: Number, nullCount?: Number, buffers?:  Partial<Buffers<T> > |  Data<T> , childData?: (Data | Vector)[]); clone(type: DataType, offset?: Number, length?: Number, nullCount?: Number, buffers?:  Buffers<R> , childData?: (Data | Vector)[]) : Data; slice(offset: Number, length: Number) : Data","headings":[{"value":"Data","depth":1},{"value":"Fields","depth":2},{"value":"Static Methods","depth":2},{"value":"Data.Null<T extends Null>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer) : Data","depth":3},{"value":"Data.Int<T extends Int>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Dictionary<T extends Dictionary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Float<T extends Float>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Bool<T extends Bool>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Decimal<T extends Decimal>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Date<T extends Date_>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Time<T extends Time>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Timestamp<T extends Timestamp>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Interval<T extends Interval>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.FixedSizeBinary<T extends FixedSizeBinary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, data: DataBuffer<T>) : Data","depth":3},{"value":"Data.Binary<T extends Binary>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data","depth":3},{"value":"Data.Utf8<T extends Utf8>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, data: Uint8Array) : Data","depth":3},{"value":"Data.List<T extends List>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, valueOffsets: ValueOffsetsBuffer, child: Data<T'valueType'> | Vector<T'valueType'>) : Data","depth":3},{"value":"Data.FixedSizeList<T extends FixedSizeList>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, child: Data | Vector) : Data","depth":3},{"value":"Data.Struct<T extends Struct>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data","depth":3},{"value":"Data.Map<T extends Map_>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, children: (Data | Vector)[]) : Data","depth":3},{"value":"Data.Union<T extends SparseUnion>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, children: (Data | Vector)[]) : Data","depth":3},{"value":"Data.Union<T extends DenseUnion>(type: T, offset: Number, length: Number, nullCount: Number, nullBitmap: NullBuffer, typeIds: TypeIdsBuffer, valueOffsets: ValueOffsetsBuffer, children: (Data | Vector)[]) : Data","depth":3},{"value":"Methods","depth":2},{"value":"constructor(type: T, offset: Number, length: Number, nullCount?: Number, buffers?: Partial<Buffers<T>> | Data<T>, childData?: (Data | Vector)[]);","depth":3},{"value":"clone(type: DataType, offset?: Number, length?: Number, nullCount?: Number, buffers?: Buffers<R>, childData?: (Data | Vector)[]) : Data;","depth":3},{"value":"slice(offset: Number, length: Number) : Data","depth":3}],"slug":"arrowjs/docs/api-reference/data","title":"Data"},{"excerpt":"Field The combination of a field name and data type, with optional metadata. Fields are used to describe the individual constituents of a nested DataType or a Schema. Members name : String (read only) The name of this field. type : Type (read only) The type of this field. nullable : Boolean (read only) Whether this field can contain  null  values, in addition to values of  Type  (this creates an extra null value map). metadata : Object | null (read only) A field's metadata is represented by a map which holds arbitrary key-value pairs. Returns  null  if no metadata has been set. typeId : ? TBD? indices : ? TBD? Used if data type is a dictionary. Methods constructor(name : String, nullable?: Boolean, metadata?: Object) Creates an instance of  Field  with parameters initialized as follows: name  - Name of the column nullable = false  - Whether a null-array is maintained. metadata = null  - Map of metadata","headings":[{"value":"Field","depth":1},{"value":"Members","depth":2},{"value":"name : String (read only)","depth":3},{"value":"type : Type (read only)","depth":3},{"value":"nullable : Boolean (read only)","depth":3},{"value":"metadata : Object | null (read only)","depth":3},{"value":"typeId : ?","depth":3},{"value":"indices : ?","depth":3},{"value":"Methods","depth":2},{"value":"constructor(name : String, nullable?: Boolean, metadata?: Object)","depth":3}],"slug":"arrowjs/docs/api-reference/field","title":"Field"},{"excerpt":"Predicates Value Literal Col The Col predicate gets the value of the specified column bind(batch : RecordBatch) : Function Returns a more efficient accessor for the column values in this batch, taking local indices. Note: These accessors are typically created in the  DataFrame.scan  bind method, and then used in the the  DataFrame.next  method. ComparisonPredicate And Or Equals LTEq GTEq Not CustomPredicate","headings":[{"value":"Predicates","depth":1},{"value":"Value","depth":2},{"value":"Literal","depth":2},{"value":"Col","depth":2},{"value":"bind(batch : RecordBatch) : Function","depth":3},{"value":"ComparisonPredicate","depth":2},{"value":"And","depth":2},{"value":"Or","depth":2},{"value":"Equals","depth":2},{"value":"LTEq","depth":2},{"value":"GTEq","depth":2},{"value":"Not","depth":2},{"value":"CustomPredicate","depth":2}],"slug":"arrowjs/docs/api-reference/predicates","title":"Predicates"},{"excerpt":"RecordBatchReader The RecordBatchReader is the IPC reader for reading chunks from a stream or file Usage The JavaScript API supports streaming multiple arrow tables over a single socket. To read all batches from all tables in a data source: If you only have one table (the normal case), then there'll only be one RecordBatchReader/the outer loop will only execute once. You can also create just one reader via Methods readAll() :  AsyncIterable<RecordBatchReader> Reads all batches from all tables in the data source. from(data :  * ) : RecordBatchFileReader  |  RecordBatchStreamReader data Array fetch response object stream The  RecordBatchReader.from  method will also detect which physical representation it's working with (Streaming or File), and will return either a  RecordBatchFileReader  or  RecordBatchStreamReader  accordingly. Remarks: if you're fetching the table from a node server, make sure the content-type is  application/octet-stream toNodeStream() pipe() You can also turn the RecordBatchReader into a stream\nif you're in node, you can use either toNodeStream() or call the pipe(writable) methods in the browser (assuming you're using the UMD or \"browser\" fields in webpack), you can call toDOMStream() or pipeTo(writable)/pipeThrough(transform) In the browser (assuming you're using the UMD or \"browser\" fields in webpack), you can call  toDOMStream()  or  pipeTo(writable) / pipeThrough(transform) You can also create a transform stream directly, instead of using  RecordBatchReader.from() You can also create a transform stream directly, instead of using  RecordBatchReader.from() throughNode throughDOM via  throughNode()  and  throughDOM()  respectively: https://github.com/apache/arrow/blob/49b4d2aad50e9d18cb0a51beb3a2aaff1b43e168/js/test/unit/ipc/reader/streams-node-tests.ts#L54 https://github.com/apache/arrow/blob/49b4d2aad50e9d18cb0a51beb3a2aaff1b43e168/js/test/unit/ipc/reader/streams-dom-tests.ts#L50 By default the transform streams will only read one table from the source readable stream and then close, but you can change this behavior by passing  { autoDestroy: false }  to the transform creation methods Remarks Reading from multiple tables ( readAll() ) is technically an extension in the JavaScript Arrow API compared to the Arrow C++ API. The authors found it was useful to be able to send multiple tables over the same physical socket\nso they built the ability to keep the underlying socket open and read more than one table from a stream. Note that Arrow has two physical representations, one for streaming, and another for random-access so this only applies to the streaming representation. The IPC protocol is that a stream of ordered Messages are consumed atomically. Messages can be of type  Schema ,  DictionaryBatch ,  RecordBatch , or  Tensor  (which we don't support yet). The Streaming format is just a sequence of messages with Schema first, then  n   DictionaryBatches , then  m   RecordBatches .","headings":[{"value":"RecordBatchReader","depth":1},{"value":"Usage","depth":2},{"value":"Methods","depth":2},{"value":"readAll() : AsyncIterable<RecordBatchReader>","depth":3},{"value":"from(data : *) : RecordBatchFileReader | RecordBatchStreamReader","depth":3},{"value":"toNodeStream()","depth":3},{"value":"pipe()","depth":3},{"value":"toDOMStream() or","depth":3},{"value":"pipeTo(writable)/pipeThrough(transform)","depth":3},{"value":"throughNode","depth":3},{"value":"throughDOM","depth":3},{"value":"Remarks","depth":2}],"slug":"arrowjs/docs/api-reference/record-batch-reader","title":"RecordBatchReader"},{"excerpt":"Row A  Row  is an Object that retrieves each value at a certain index across a collection of child Vectors. Rows are returned from the  get()  function of the nested  StructVector  and  MapVector , as well as  RecordBatch  and  Table . A  Row  defines read-only accessors for the indices and (if applicable) names of the child Vectors. For example, given a  StructVector  with the following schema: Row  implements the Iterator interface, enumerating each value in order of the child vectors list. Notes: If the Row's parent type is a  Struct ,  Object.getOwnPropertyNames(row)  returns the child vector indices. If the Row's parent type is a  Map ,  Object.getOwnPropertyNames(row)  returns the child vector field names, as defined by the  children  Fields list of the  Map  instance. Methods key: string : T keyof T kParent :  MapVector<T>  |  StructVector<T> kRowIndex : number kLength : number (readonly) Symbol.iterator :  IterableIterator<T[keyof T][\"TValue\"]> get(key: K): T K Returns the value at the supplied  key , where  key  is either the integer index of the set of child vectors, or the name of a child Vector toJSON(): any toString(): any","headings":[{"value":"Row","depth":1},{"value":"Methods","depth":2},{"value":"key: string: Tkeyof T","depth":3},{"value":"kParent: MapVector<T> | StructVector<T>","depth":3},{"value":"kRowIndex: number","depth":3},{"value":"kLength: number (readonly)","depth":3},{"value":"Symbol.iterator: IterableIterator<T[keyof T][\"TValue\"]>","depth":3},{"value":"get(key: K): TK","depth":3},{"value":"toJSON(): any","depth":3},{"value":"toString(): any","depth":3}],"slug":"arrowjs/docs/api-reference/row","title":"Row"},{"excerpt":"Schema Sequence of arrow::Field objects describing the columns of a record batch or table data structure Accessors fields : Field[]  ( readonly) Return the list of fields (columns) in the schema. metadata (readonly) The custom key-value metadata, if any. metadata may be null. dictionaries (readonly) TBD - List of dictionaries (each dictionary is associated with a column that is dictionary encoded). dictionaryFields (readonly) TBD - List of fields Methods constructor(fields: Field[], metadata?: Object, dictionaries?: Object, dictionaryFields?: Object) Creates a new schema instance. select(columnNames) : Schema Returns a new  Schema  with the Fields indicated by the column names.","headings":[{"value":"Schema","depth":1},{"value":"Accessors","depth":2},{"value":"fields : Field[] (readonly)","depth":3},{"value":"metadata (readonly)","depth":3},{"value":"dictionaries (readonly)","depth":3},{"value":"dictionaryFields (readonly)","depth":3},{"value":"Methods","depth":2},{"value":"constructor(fields: Field[], metadata?: Object, dictionaries?: Object, dictionaryFields?: Object)","depth":3},{"value":"select(columnNames) : Schema","depth":3}],"slug":"arrowjs/docs/api-reference/schema","title":"Schema"},{"excerpt":"RecordBatch Overview A Record Batch in Apache Arrow is a collection of equal-length array instances. Usage A record batch can be created from this list of arrays using  RecordBatch.from : Inheritance RecordBatch  extends  StructVector  extends  BaseVector Members schema : Schema (readonly) Returns the schema of the data in the record batch numCols : Number (readonly) Returns number of fields/columns in the schema (shorthand for  this.schema.fields.length ). Static Methods RecordBatch.from(vectors: Array, names: String[] = []) : RecordBatch Creates a  RecordBatch , see  RecordBatch.new() . RecordBatch.new(vectors: Array, names: String[] = []) : RecordBatch Creates new a record batch. Schema is auto inferred, using names or index positions if  names  are not supplied. Methods constructor(schema: Schema, numRows: Number, childData: (Data | Vector)[]) Create a new  RecordBatch  instance with  numRows  rows of child data. numRows  -  childData  -  constructor(schema: Schema, data: Data, children?: Vector[]) Create a new  RecordBatch  instance with  numRows  rows of child data. constructor(...args: any[]) clone(data: Data, children?: Array) : RecordBatch Returns a newly allocated copy of this  RecordBatch concat(...others: Vector[]) : Table Concatenates a number of  Vector  instances. select(...columnNames: K[]) : RecordBatch Return a new  RecordBatch  with a subset of columns.","headings":[{"value":"RecordBatch","depth":1},{"value":"Overview","depth":2},{"value":"Usage","depth":2},{"value":"Inheritance","depth":2},{"value":"Members","depth":2},{"value":"schema : Schema (readonly)","depth":3},{"value":"numCols : Number (readonly)","depth":3},{"value":"Static Methods","depth":2},{"value":"RecordBatch.from(vectors: Array, names: String[] = []) : RecordBatch","depth":3},{"value":"RecordBatch.new(vectors: Array, names: String[] = []) : RecordBatch","depth":3},{"value":"Methods","depth":2},{"value":"constructor(schema: Schema, numRows: Number, childData: (Data | Vector)[])","depth":3},{"value":"constructor(schema: Schema, data: Data, children?: Vector[])","depth":3},{"value":"constructor(...args: any[])","depth":3},{"value":"clone(data: Data, children?: Array) : RecordBatch","depth":3},{"value":"concat(...others: Vector[]) : Table","depth":3},{"value":"select(...columnNames: K[]) : RecordBatch","depth":3}],"slug":"arrowjs/docs/api-reference/record-batch","title":"RecordBatch"},{"excerpt":"Types Objects representing types.","headings":[{"value":"Types","depth":1}],"slug":"arrowjs/docs/api-reference/types","title":"Types"},{"excerpt":"StructVector Methods asMap(keysSorted: boolean = false) TBA","headings":[{"value":"StructVector","depth":1},{"value":"Methods","depth":2},{"value":"asMap(keysSorted: boolean = false)","depth":3}],"slug":"arrowjs/docs/api-reference/struct-vector","title":"StructVector"},{"excerpt":"RecordBatchWriter The  RecordBatchWriter  \"serializes\" Arrow Tables (or streams of RecordBatches) to the Arrow File, Stream, or JSON representations for inter-process communication (see also:  Arrow IPC format docs ). The RecordBatchWriter is conceptually a \"transform\" stream that transforms Tables or RecordBatches into binary  Uint8Array  chunks that represent the Arrow IPC messages ( Schema ,  DictionaryBatch ,  RecordBatch , and in the case of the File format,  Footer  messages). These binary chunks are buffered inside the  RecordBatchWriter  instance until they are consumed, typically by piping the RecordBatchWriter instance to a Writable Stream (like a file or socket), enumerating the chunks via async-iteration, or by calling  toUint8Array()  to create a single contiguous buffer of the concatenated results once the desired Tables or RecordBatches have been written. RecordBatchWriter conforms to the  AsyncIterableIterator  protocol in all environments, and supports two additional stream primitives based on the environment (nodejs or browsers) available at runtime. In nodejs, the  RecordBatchWriter  can be converted to a  ReadableStream , piped to a  WritableStream , and has a static method that returns a  TransformStream  suitable in chained  pipe  calls. browser environments that support the  DOM/WhatWG Streams Standard , corresponding methods exist to convert  RecordBatchWriters  to the DOM  ReadableStream ,  WritableStream , and  TransformStream  variants. Note : The Arrow JSON representation is not suitable as an IPC mechanism in real-world scenarios. It is used inside the Arrow project as a human-readable debugging tool and for validating interoperability between each language's separate implementation of the Arrow library. Member Fields closed: Promise (readonly) A Promise which resolves when this  RecordBatchWriter  is closed. Static Methods RecordBatchWriter.throughNode(options?: Object): DuplexStream Creates a Node.js  TransformStream  that transforms an input  ReadableStream  of Tables or RecordBatches into a stream of  Uint8Array  Arrow Message chunks. options.autoDestroy : boolean - (default:  true ) Indicates whether the RecordBatchWriter should close after writing the first logical stream of RecordBatches (batches which all share the same Schema), or should continue and reset each time it encounters a new Schema. options.*  - Any Node Duplex stream options can be supplied Returns: A Node.js Duplex stream Example: RecordBatchWriter.throughDOM(writableStrategy? : Object, readableStrategy? : Object) : Object Creates a DOM/WhatWG  ReadableStream / WritableStream  pair that together transforms an input  ReadableStream  of Tables or RecordBatches into a stream of  Uint8Array  Arrow Message chunks. options.autoDestroy : boolean - (default:  true ) Indicates whether the RecordBatchWriter should close after writing the first logical stream of RecordBatches (batches which all share the same Schema), or should continue and reset each time it encounters a new Schema. writableStrategy.* = - Any options for QueuingStrategy\\<RecordBatch > readableStrategy.highWaterMark ? : Number readableStrategy.size ?: Number Returns: an object with the following fields: writable : WritableStream\\<Table | RecordBatch > readable : ReadableStream\\<Uint8Array > Methods constructor(options? : Object) options.autoDestroy : boolean - toString(sync: Boolean): string |  Promise<string> toUint8Array(sync: Boolean): Uint8Array |  Promise<Uint8Array> writeAll(input: Table |  Iterable<RecordBatch> ): this writeAll(input:  AsyncIterable<RecordBatch> ):  Promise<this> writeAll(input:  PromiseLike<AsyncIterable<RecordBatch> >):  Promise<this> writeAll(input: PromiseLike<Table |  Iterable<RecordBatch> >):  Promise<this> Symbol.asyncIterator :  AsyncByteQueue<Uint8Array> Returns An async iterator that produces Uint8Arrays. toDOMStream(options?: Object):  ReadableStream<Uint8Array> Returns a new DOM/WhatWG stream that can be used to read the Uint8Array chunks produced by the RecordBatchWriter options  - passed through to the DOM ReadableStream constructor, any DOM ReadableStream options. toNodeStream(options?: Object): Readable options  - passed through to the Node ReadableStream constructor, any Node ReadableStream options. close() : void Close the RecordBatchWriter. After close is called, no more chunks can be written. abort(reason?: any) : void finish() : this reset(sink?:  WritableSink<ArrayBufferViewInput> , schema?: Schema | null): this Change the sink write(payload?: Table | RecordBatch |  Iterable<Table>  |  Iterable<RecordBatch>  | null): void Writes a  RecordBatch  or all the RecordBatches from a  Table . Remarks Just like the  RecordBatchReader , a  RecordBatchWriter  is a factory base class that returns an instance of the subclass appropriate to the situation:  RecordBatchStreamWriter ,  RecordBatchFileWriter ,  RecordBatchJSONWriter","headings":[{"value":"RecordBatchWriter","depth":2},{"value":"Member Fields","depth":2},{"value":"Static Methods","depth":2},{"value":"RecordBatchWriter.throughNode(options?: Object): DuplexStream","depth":3},{"value":"RecordBatchWriter.throughDOM(writableStrategy? : Object, readableStrategy? : Object) : Object","depth":3},{"value":"Methods","depth":2},{"value":"toString(sync: Boolean): string | Promise<string>","depth":3},{"value":"toUint8Array(sync: Boolean): Uint8Array | Promise<Uint8Array>","depth":3},{"value":"writeAll(input: Table | Iterable<RecordBatch>): this","depth":3},{"value":"writeAll(input: AsyncIterable<RecordBatch>): Promise<this>","depth":3},{"value":"writeAll(input: PromiseLike<AsyncIterable<RecordBatch>>): Promise<this>","depth":3},{"value":"writeAll(input: PromiseLike<Table | Iterable<RecordBatch>>): Promise<this>","depth":3},{"value":"toDOMStream(options?: Object): ReadableStream<Uint8Array>","depth":3},{"value":"toNodeStream(options?: Object): Readable","depth":3},{"value":"close() : void","depth":3},{"value":"abort(reason?: any) : void","depth":3},{"value":"finish() : this","depth":3},{"value":"reset(sink?: WritableSink<ArrayBufferViewInput>, schema?: Schema | null): this","depth":3},{"value":"write(payload?: Table | RecordBatch | Iterable<Table> | Iterable<RecordBatch> | null): void","depth":3},{"value":"Remarks","depth":2}],"slug":"arrowjs/docs/api-reference/record-batch-writer","title":"RecordBatchWriter"},{"excerpt":"Table Logical table as sequence of chunked arrays Overview The JavaScript  Table  class is not part of the Apache Arrow specification as such, but is rather a tool to allow you to work with multiple record batches and array pieces as a single logical dataset. As a relevant example, we may receive multiple small record batches in a socket stream, then need to concatenate them into contiguous memory for use in NumPy or pandas. The Table object makes this efficient without requiring additional memory copying. A Tables columns are instances of  Column , which is a container for one or more arrays of the same type. Usage Table.new()  accepts an  Object  of  Columns  or  Vectors , where the keys will be used as the field names for the  Schema : It also accepts a a list of Vectors with an optional list of names or\nFields for the resulting Schema. If the list is omitted or a name is\nmissing, the numeric index of each Vector will be used as the name: If the supplied arguments are  Column  instances,  Table.new  will infer the  Schema  from the  Column s: If the supplied Vector or Column lengths are unequal,  Table.new  will\nextend the lengths of the shorter Columns, allocating additional bytes\nto represent the additional null slots. The memory required to allocate\nthese additional bitmaps can be computed as: For example, an additional null bitmap for one million null values would require  125,000  bytes ( ((1e6 + 63) & ~63) >> 3 ), or approx.  0.11MiB Inheritance Table  extends Chunked Static Methods Table.empty() : Table Creates an empty table Table.from() : Table Creates an empty table Table.from(source: RecordBatchReader): Table Table.from(source:  Promise<RecordBatchReader> ):  Promise<Table> Table.from(source?: any) : Table Table.fromAsync(source: import('./ipc/reader').FromArgs):  Promise<Table> Table.fromVectors(vectors: any[], names?: String[]) : Table Table.fromStruct(struct: Vector) : Table Table.new(columns: Object) Table.new(...columns) Table.new(vectors: Vector[], names: String[]) Type safe constructors. Functionally equivalent to calling  new Table()  with the same arguments, however if using Typescript using the  new  method instead will ensure that types inferred from the arguments \"flow through\" into the return Table type. Members schema (readonly) The  Schema  of this table. length : Number (readonly) The number of rows in this table. TBD: this does not consider filters chunks : RecordBatch[]  ( readonly) The list of chunks in this table. numCols : Number (readonly) The number of columns in this table. Methods constructor(batches: RecordBatch[]) The schema will be inferred from the record batches. constructor(...batches: RecordBatch[]) The schema will be inferred from the record batches. constructor(schema: Schema, batches: RecordBatch[]) constructor(schema: Schema, ...batches: RecordBatch[]) constructor(...args: any[]) Create a new  Table  from a collection of  Columns  or  Vectors , with an optional list of names or  Fields . TBD clone(chunks?:) Returns a new copy of this table. getColumnAt(index: number): Column | null Gets a column by index. getColumn(name: String): Column | null Gets a column by name getColumnIndex(name: String) : Number | null Returns the index of the column with name  name . getChildAt(index: number): Column | null TBD serialize(encoding = 'binary', stream = true) : Uint8Array Returns a  Uint8Array  that contains an encoding of all the data in the table. Note: Passing the returned data back into  Table.from()  creates a \"deep clone\" of the table. count(): number TBD - Returns the number of elements. select(...columnNames: string[]) : Table Returns a new Table with the specified subset of columns, in the specified order. countBy(name : Col | String) : Table Returns a new Table that contains two columns ( values  and  counts ).","headings":[{"value":"Table","depth":1},{"value":"Overview","depth":2},{"value":"Usage","depth":2},{"value":"Inheritance","depth":2},{"value":"Static Methods","depth":2},{"value":"Table.empty() : Table","depth":3},{"value":"Table.from() : Table","depth":3},{"value":"Table.from(source: RecordBatchReader): Table","depth":3},{"value":"Table.from(source: Promise<RecordBatchReader>): Promise<Table>","depth":3},{"value":"Table.from(source?: any) : Table","depth":3},{"value":"Table.fromAsync(source: import('./ipc/reader').FromArgs): Promise<Table>","depth":3},{"value":"Table.fromVectors(vectors: any[], names?: String[]) : Table","depth":3},{"value":"Table.fromStruct(struct: Vector) : Table","depth":3},{"value":"Table.new(columns: Object)","depth":3},{"value":"Table.new(...columns)","depth":3},{"value":"Table.new(vectors: Vector[], names: String[])","depth":3},{"value":"Members","depth":2},{"value":"schema (readonly)","depth":3},{"value":"length : Number (readonly)","depth":3},{"value":"chunks : RecordBatch[] (readonly)","depth":3},{"value":"numCols : Number (readonly)","depth":3},{"value":"Methods","depth":2},{"value":"constructor(batches: RecordBatch[])","depth":3},{"value":"constructor(...batches: RecordBatch[])","depth":3},{"value":"constructor(schema: Schema, batches: RecordBatch[])","depth":3},{"value":"constructor(schema: Schema, ...batches: RecordBatch[])","depth":3},{"value":"constructor(...args: any[])","depth":3},{"value":"clone(chunks?:)","depth":3},{"value":"getColumnAt(index: number): Column | null","depth":3},{"value":"getColumn(name: String): Column | null","depth":3},{"value":"getColumnIndex(name: String) : Number | null","depth":3},{"value":"getChildAt(index: number): Column | null","depth":3},{"value":"serialize(encoding = 'binary', stream = true) : Uint8Array","depth":3},{"value":"count(): number","depth":3},{"value":"select(...columnNames: string[]) : Table","depth":3},{"value":"countBy(name : Col | String) : Table","depth":3}],"slug":"arrowjs/docs/api-reference/table","title":"Table"},{"excerpt":"Vector Also referred to as  BaseVector . An abstract base class for vector types. Can support a null map ... TBD Inheritance Fields data:  Data<T>  (readonly) The underlying Data instance for this Vector. numChildren: number (readonly) The number of logical Vector children. Only applicable if the DataType of the Vector is one of the nested types (List, FixedSizeList, Struct, or Map). type : T The DataType that describes the elements in the Vector typeId : T 'typeId' The  typeId  enum value of the  type  instance length : number Number of elements in the  Vector offset : number Offset to the first element in the underlying data. stride : number Stride between successive elements in the the underlying data. The number of elements in the underlying data buffer that constitute a single logical value for the given type. The stride for all DataTypes is 1 unless noted here: For  Decimal  types, the stride is 4. For  Date  types, the stride is 1 if the  unit  is DateUnit.DAY, else 2. For  Int ,  Interval , or  Time  types, the stride is 1 if  bitWidth <= 32 , else 2. For  FixedSizeList  types, the stride is the  listSize  property of the  FixedSizeList  instance. For  FixedSizeBinary  types, the stride is the  byteWidth  property of the  FixedSizeBinary  instance. nullCount : Number Number of  null  values in this  Vector  instance ( null  values require a null map to be present). VectorName : String Returns the name of the Vector ArrayType : TypedArrayConstructor | ArrayConstructor Returns the constructor of the underlying typed array for the values buffer as determined by this Vector's DataType. values : T 'TArray' Returns the underlying data buffer of the Vector, if applicable. typeIds : Int8Array | null Returns the underlying typeIds buffer, if the Vector DataType is Union. nullBitmap : Uint8Array | null Returns the underlying validity bitmap buffer, if applicable. Note: Since the validity bitmap is a Uint8Array of bits, it is  not  sliced when you call  vector.slice() . Instead, the  vector.offset  property is updated on the returned Vector. Therefore, you must factor  vector.offset  into the bit position if you wish to slice or read the null positions manually. See the implementation of  BaseVector.isValid()  for an example of how this is done. valueOffsets : Int32Array | null Returns the underlying valueOffsets buffer, if applicable. Only the List, Utf8, Binary, and DenseUnion DataTypes will have valueOffsets. Methods clone(data:  Data<R> , children):  Vector<R> Returns a clone of the current Vector, using the supplied Data and optional children for the new clone. Does not copy any underlying buffers. concat(...others:  Vector<T>[] ) Returns a  Chunked  vector that concatenates this Vector with the supplied other Vectors. Other Vectors must be the same type as this Vector. slice(begin?: number, end?: number) Returns a zero-copy slice of this Vector. The begin and end arguments are handled the same way as JS'  Array.prototype.slice ; they are clamped between 0 and  vector.length  and wrap around when negative, e.g.  slice(-1, 5)  or  slice(5, -1) isValid(index: number): boolean Returns whether the supplied index is valid in the underlying validity bitmap. getChildAt <R extends DataType = any> (index: number):  Vector<R>  | null Returns the inner Vector child if the DataType is one of the nested types (Map or Struct). toJSON(): any Returns a dense JS Array of the Vector values, with null sentinels in-place.","headings":[{"value":"Vector","depth":1},{"value":"Inheritance","depth":2},{"value":"Fields","depth":2},{"value":"data: Data<T> (readonly)","depth":3},{"value":"numChildren: number (readonly)","depth":3},{"value":"type : T","depth":3},{"value":"typeId : T'typeId'","depth":3},{"value":"length : number","depth":3},{"value":"offset : number","depth":3},{"value":"stride : number","depth":3},{"value":"nullCount : Number","depth":3},{"value":"VectorName : String","depth":3},{"value":"ArrayType : TypedArrayConstructor | ArrayConstructor","depth":3},{"value":"values : T'TArray'","depth":3},{"value":"typeIds : Int8Array | null","depth":3},{"value":"nullBitmap : Uint8Array | null","depth":3},{"value":"valueOffsets : Int32Array | null","depth":3},{"value":"Methods","depth":2},{"value":"clone(data: Data<R>, children): Vector<R>","depth":3},{"value":"concat(...others: Vector<T>[])","depth":3},{"value":"slice(begin?: number, end?: number)","depth":3},{"value":"isValid(index: number): boolean","depth":3},{"value":"getChildAt<R extends DataType = any>(index: number): Vector<R> | null","depth":3},{"value":"toJSON(): any","depth":3}],"slug":"arrowjs/docs/api-reference/vector","title":"Vector"},{"excerpt":"Types and Vectors Overview Usage Constructing new  Vector  instances is done through the static  from()  methods Special Vectors Dictionary Arrays The Dictionary type is a special array type that enables one or more record batches in a file or stream to transmit integer indices referencing a shared dictionary containing the distinct values in the logical array. Later record batches reuse indices in earlier batches and add new ones as needed. A  Dictionary  is similar to a  factor  in R or a pandas, or \"Categorical\" in Python. It is is often used with strings to save memory and improve performance. StructVector Holds nested fields. Bool Vectors Bool Vectors BoolVector Binary Vectors Binary Vectors BinaryVector FloatVectors Float Vectors Backing Comments Float16Vector Uint16Array No native JS 16 bit type, additional methods available Float32Vector Float32Array Holds 32 bit floats Float64Vector Float64Array Holds 64 bit floats Static FloatVector Methods FloatVector.from(data: Uint16Array): Float16Vector; FloatVector.from(data: Float32Array): Float32Vector; FloatVector.from(data: Float64Array): Float64Vector; FloatVector16.from(data: Uint8Array |  Iterable<Number> ): Float16Vector; FloatVector16.from(data: Uint16Array |  Iterable<Number> ): Float16Vector; FloatVector32.from(data: Float32 'TArray'  |  Iterable<Number> ): Float32Vector; FloatVector64.from(data: Float64 'TArray'  |  Iterable<Number> ): Float64Vector; Float16Vector Methods Since JS doesn't have half floats,  Float16Vector  is backed by a  Uint16Array  integer array. To make it practical to work with these arrays in JS, some extra methods are added. toArray() :  Uint16Array Returns a zero-copy view of the underlying  Uint16Array  data. Note: Avoids incurring extra compute or copies if you're calling  toArray()  in order to create a buffer for something like WebGL, but makes it hard to use the returned data as floating point values in JS. toFloat32Array() : Float32Array This method will convert values to 32 bit floats. Allocates a new Array. toFloat64Array() : Float64Array This method will convert values to 64 bit floats. Allocates a new Array. IntVectors Int Vectors Backing Comments Int8Vector Int8Array Int16Vector Int16Array Int32Vector Int32Array Int64Vector Int32Array 64-bit values stored as pairs of  lo, hi  32-bit values for engines without BigInt support, extra methods available Uint8Vector Uint8Array Uint16Vector Uint16Array  Uint32Vector Uint32Array  Uint64Vector Uint32Array 64-bit values stored as pairs of  lo, hi  32-bit values for engines without BigInt support, extra methods available Int64Vector Methods toArray() :  Int32Array Returns a zero-copy view of the underlying pairs of  lo, hi  32-bit values as an  Int32Array . This Array's length is twice the logical length of the  Int64Vector . toBigInt64Array():  BigInt64Array Returns a zero-copy view of the underlying 64-bit integers as a  BigInt64Array . This Array has the samne length as the length of the original  Int64Vector . Note: as of 03/2019,  BigInt64Array  is only available in v8/Chrome. In JS runtimes without support for  BigInt , this method throws an unsupported error. Uint64Vector Methods toArray() :  Uint32Array Returns a zero-copy view of the underlying pairs of  lo, hi  32-bit values as a  Uint32Array . This Array's length is twice the logical length of the  Uint64Vector . toBigUint64Array():  BigUint64Array Returns a zero-copy view of the underlying 64-bit integers as a  BigUint64Array . This Array has the samne length as the length of the original  Uint64Vector . Note: as of 03/2019,  BigUint64Array  is only available in v8/Chrome. In JS runtimes without support for  BigInt , this method throws an unsupported error. Static IntVector Methods IntVector.from(data: Int8Array): Int8Vector; IntVector.from(data: Int16Array): Int16Vector; IntVector.from(data: Int32Array, is64?: boolean): Int32Vector | Int64Vector; IntVector.from(data: Uint8Array): Uint8Vector; IntVector.from(data: Uint16Array): Uint16Vector; IntVector.from(data: Uint32Array, is64?: boolean): Uint32Vector | Uint64Vector; Int8Vector.from(this: typeof Int8Vector,   data: Int8Array   |  Iterable<number> ): Int8Vector; Int16Vector.from(this: typeof Int16Vector,  data: Int16Array  |  Iterable<number> ): Int16Vector; Int32Vector.from(this: typeof Int32Vector,  data: Int32Array  |  Iterable<number> ): Int32Vector; Int64Vector.from(this: typeof Int64Vector,  data: Int32Array  |  Iterable<number> ): Int64Vector; Uint8Vector.from(this: typeof Uint8Vector,  data: Uint8Array  |  Iterable<number> ): Uint8Vector; Uint16Vector.from(this: typeof Uint16Vector, data: Uint16Array |  Iterable<number> ): Uint16Vector; Uint32Vector.from(this: typeof Uint32Vector, data: Uint32Array |  Iterable<number> ): Uint32Vector; Uint64Vector.from(this: typeof Uint64Vector, data: Uint32Array |  Iterable<number> ): Uint64Vector; Date Vectors Date Vectors Backing DateDayVector Int32Array DateMillisecondVector Int32Array TBD - stride: 2?","headings":[{"value":"Types and Vectors","depth":1},{"value":"Overview","depth":2},{"value":"Usage","depth":2},{"value":"Special Vectors","depth":2},{"value":"Dictionary Arrays","depth":3},{"value":"StructVector","depth":3},{"value":"Bool Vectors","depth":3},{"value":"Binary Vectors","depth":3},{"value":"FloatVectors","depth":2},{"value":"Static FloatVector Methods","depth":3},{"value":"FloatVector.from(data: Uint16Array): Float16Vector;","depth":3},{"value":"FloatVector.from(data: Float32Array): Float32Vector;","depth":3},{"value":"FloatVector.from(data: Float64Array): Float64Vector;","depth":3},{"value":"FloatVector16.from(data: Uint8Array | Iterable<Number>): Float16Vector;","depth":3},{"value":"FloatVector16.from(data: Uint16Array | Iterable<Number>): Float16Vector;","depth":3},{"value":"FloatVector32.from(data: Float32'TArray' | Iterable<Number>): Float32Vector;","depth":3},{"value":"FloatVector64.from(data: Float64'TArray' | Iterable<Number>): Float64Vector;","depth":3},{"value":"Float16Vector Methods","depth":2},{"value":"toArray() : Uint16Array","depth":3},{"value":"toFloat32Array() : Float32Array","depth":3},{"value":"toFloat64Array() : Float64Array","depth":3},{"value":"IntVectors","depth":2},{"value":"Int64Vector Methods","depth":2},{"value":"toArray() : Int32Array","depth":3},{"value":"toBigInt64Array(): BigInt64Array","depth":3},{"value":"Uint64Vector Methods","depth":2},{"value":"toArray() : Uint32Array","depth":3},{"value":"toBigUint64Array(): BigUint64Array","depth":3},{"value":"Static IntVector Methods","depth":2},{"value":"IntVector.from(data: Int8Array): Int8Vector;","depth":3},{"value":"IntVector.from(data: Int16Array): Int16Vector;","depth":3},{"value":"IntVector.from(data: Int32Array, is64?: boolean): Int32Vector | Int64Vector;","depth":3},{"value":"IntVector.from(data: Uint8Array): Uint8Vector;","depth":3},{"value":"IntVector.from(data: Uint16Array): Uint16Vector;","depth":3},{"value":"IntVector.from(data: Uint32Array, is64?: boolean): Uint32Vector | Uint64Vector;","depth":3},{"value":"Int8Vector.from(this: typeof Int8Vector,   data: Int8Array   | Iterable<number>): Int8Vector;","depth":3},{"value":"Int16Vector.from(this: typeof Int16Vector,  data: Int16Array  | Iterable<number>): Int16Vector;","depth":3},{"value":"Int32Vector.from(this: typeof Int32Vector,  data: Int32Array  | Iterable<number>): Int32Vector;","depth":3},{"value":"Int64Vector.from(this: typeof Int64Vector,  data: Int32Array  | Iterable<number>): Int64Vector;","depth":3},{"value":"Uint8Vector.from(this: typeof Uint8Vector,  data: Uint8Array  | Iterable<number>): Uint8Vector;","depth":3},{"value":"Uint16Vector.from(this: typeof Uint16Vector, data: Uint16Array | Iterable<number>): Uint16Vector;","depth":3},{"value":"Uint32Vector.from(this: typeof Uint32Vector, data: Uint32Array | Iterable<number>): Uint32Vector;","depth":3},{"value":"Uint64Vector.from(this: typeof Uint64Vector, data: Uint32Array | Iterable<number>): Uint64Vector;","depth":3},{"value":"Date Vectors","depth":2}],"slug":"arrowjs/docs/api-reference/vectors","title":"Types and Vectors"}]}},"staticQueryHashes":["484347790"]}